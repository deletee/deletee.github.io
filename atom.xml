<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>deletee</title>
  
  <subtitle>长安的风何时才能吹到汴梁?</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://deletee.github.io/"/>
  <updated>2018-12-31T16:25:28.803Z</updated>
  <id>https://deletee.github.io/</id>
  
  <author>
    <name>deletee</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>awk的高级应用-多目录输出</title>
    <link href="https://deletee.github.io/2018/12/29/awk-application/"/>
    <id>https://deletee.github.io/2018/12/29/awk-application/</id>
    <published>2018-12-29T07:46:30.000Z</published>
    <updated>2018-12-31T16:25:28.803Z</updated>
    
    <content type="html"><![CDATA[<p>awk是Shell中三剑客之一，功能十分强大，本文分享的通过一份管道输入，实现多目录输出，废话不多说，直接上代码</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment">#***********************************************************************</span></span><br><span class="line"><span class="comment">#脚本功能: 追补历史推送数据</span></span><br><span class="line"><span class="comment">#创建日期: 2016-05-16</span></span><br><span class="line"><span class="comment">#修改纪录：      修改人            修改日期          修改描述</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#----+-----------------+--------------------+--------------------------+</span></span><br><span class="line"><span class="comment">#  1          deletee       2017-10-12            创建</span></span><br><span class="line"><span class="comment">#***********************************************************************</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境变量设置</span></span><br><span class="line"><span class="comment">#-------------------------------------------------------</span></span><br><span class="line"><span class="comment"># JOB_HOME：作业目录</span></span><br><span class="line"><span class="comment"># HDFS_PATH :HDFS目录</span></span><br><span class="line"><span class="comment">#-------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Local</span></span><br><span class="line">JOB_HOME=$(<span class="built_in">cd</span> <span class="string">"<span class="variable">$(dirname "$0")</span>"</span>; <span class="built_in">pwd</span>)</span><br><span class="line">JOB_DATA_PATH=<span class="variable">$&#123;JOB_HOME&#125;</span>/data</span><br><span class="line">PROCESSED_FILE_PATH=<span class="variable">$&#123;JOB_HOME&#125;</span>/pro</span><br><span class="line">SECRETS_CONF_PATH=/home/urs/data/rsync/conf</span><br><span class="line">mkdir -p <span class="variable">$&#123;JOB_HOME&#125;</span>/&#123;data,pro&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># HDFS</span></span><br><span class="line">HADOOP_HOME=/home/<span class="variable">$&#123;USER&#125;</span>/software/hadoop</span><br><span class="line">HDFS_PATH=/hdfs/*/some_log/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------</span></span><br><span class="line"><span class="comment"># 函数名:genTargetFileModel</span></span><br><span class="line"><span class="comment"># 功能:生成目标文件模式,遍历HDFS目录，生成日、时、分</span></span><br><span class="line"><span class="comment"># 参数: $1 日期，格式 yyyy-MM-dd</span></span><br><span class="line"><span class="comment"># 返回:无</span></span><br><span class="line"><span class="comment">#---------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">genTargetFileModel</span></span>() &#123;</span><br><span class="line">      <span class="comment"># 遍历日期</span></span><br><span class="line">      ETL_DT=<span class="variable">$1</span></span><br><span class="line">      CUR_HDFS_PATH=`<span class="built_in">echo</span> <span class="variable">$&#123;HDFS_PATH&#125;</span>|sed <span class="string">"s:*:<span class="variable">$&#123;ETL_DT&#125;</span>:"</span>`</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (( i = 0; i &lt; 24; i++ )); <span class="keyword">do</span></span><br><span class="line">          <span class="keyword">for</span> (( j = 0; j &lt; 12; j++ )); <span class="keyword">do</span></span><br><span class="line">              hour=`<span class="built_in">printf</span> <span class="string">"%02d"</span> <span class="variable">$i</span>`</span><br><span class="line">              minutes=$((j*5))</span><br><span class="line">              minutes=`<span class="built_in">printf</span> <span class="string">"%02d"</span> <span class="variable">$minutes</span>`</span><br><span class="line">              mkdir -p <span class="variable">$&#123;JOB_DATA_PATH&#125;</span>/<span class="variable">$&#123;ETL_DT&#125;</span>/<span class="variable">$&#123;hour&#125;</span>/<span class="variable">$&#123;minutes&#125;</span></span><br><span class="line">          <span class="keyword">done</span></span><br><span class="line">      <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> path <span class="keyword">in</span> `<span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin/hadoop fs -ls <span class="variable">$&#123;CUR_HDFS_PATH&#125;</span>|awk <span class="string">'&#123;print $8&#125;'</span>`;<span class="keyword">do</span></span><br><span class="line">          <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;ETL_DT&#125;</span>:<span class="variable">$&#123;path&#125;</span>"</span> &gt;&gt; <span class="variable">$&#123;PROCESSED_FILE_PATH&#125;</span>/pro_<span class="variable">$&#123;ETL_DT&#125;</span>.txt</span><br><span class="line">          <span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin/hadoop fs -text <span class="variable">$&#123;path&#125;</span> |awk <span class="string">'&#123;</span></span><br><span class="line"><span class="string">              split($0,a," ");</span></span><br><span class="line"><span class="string">              datestr=substr(a[1],11,10);</span></span><br><span class="line"><span class="string">              hourstr=substr(a[2],1,2);</span></span><br><span class="line"><span class="string">              minutestr=substr(a[2],4,2);</span></span><br><span class="line"><span class="string">              minutestr=int(minutestr/5)*5;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">              if(minutestr&lt;10)&#123;</span></span><br><span class="line"><span class="string">                minutestr=("0"minutestr)</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">              gsub("\"","",a[3])</span></span><br><span class="line"><span class="string">              split(a[3],b,",");</span></span><br><span class="line"><span class="string">              gsub("&#123;|&#125;","",b[3])</span></span><br><span class="line"><span class="string">              gsub("fields:HOSTNAME:","",b[3])</span></span><br><span class="line"><span class="string">              gsub("\\\\t","\t",b[1])</span></span><br><span class="line"><span class="string">              gsub(";","\t",b[1])</span></span><br><span class="line"><span class="string">              lens=split(b[1],tA,"\t")</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">              if(lens==12)&#123;</span></span><br><span class="line"><span class="string">                re=(b[1]"\t\t\t\t\t\t\t\t\t\t\t\t"b[3])</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">              if(lens==15)&#123;</span></span><br><span class="line"><span class="string">                re=(b[1]"\t\t\t\t\t\t\t\t\t"b[3])</span></span><br><span class="line"><span class="string">              &#125;</span></span><br><span class="line"><span class="string">              path=(datestr"/"hourstr"/"minutestr"/log_"datestr"_"hourstr"_"minutestr".txt")</span></span><br><span class="line"><span class="string">              print re &gt;&gt; path</span></span><br><span class="line"><span class="string">          &#125;'</span></span><br><span class="line">      <span class="keyword">done</span></span><br><span class="line">      <span class="built_in">unset</span> ETL_DT</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------------</span></span><br><span class="line"><span class="comment"># 函数名:genTargetFileModel</span></span><br><span class="line"><span class="comment"># 功能:生成目标文件模式,遍历HDFS目录，生成日、时、分</span></span><br><span class="line"><span class="comment"># 参数: $1 日期，格式 yyyy-MM-dd</span></span><br><span class="line"><span class="comment"># 返回:无</span></span><br><span class="line"><span class="comment">#---------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">sendByRysnc</span></span>() &#123;</span><br><span class="line">  ETL_DT=<span class="variable">$1</span></span><br><span class="line">  nohup /usr/bin/rsync -avz \</span><br><span class="line">        --bwlimit=30720 \</span><br><span class="line">        --port=873 \</span><br><span class="line">        --progress \</span><br><span class="line">        --password-file=<span class="variable">$&#123;SECRETS_CONF_PATH&#125;</span>/secret.conf <span class="variable">$&#123;JOB_DATA_PATH&#125;</span>/<span class="variable">$&#123;ETL_DT&#125;</span> <span class="variable">$&#123;user&#125;</span>@<span class="variable">$&#123;ip&#125;</span>::<span class="variable">$&#123;module&#125;</span>/<span class="variable">$&#123;path&#125;</span>/ &gt;&gt; rysnc_send_$(date +%Y%m%d).<span class="built_in">log</span> 2&gt;&amp;1 &amp;</span><br><span class="line">  <span class="built_in">unset</span> ETL_DT</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">mainStart</span></span>() &#123;</span><br><span class="line">    ETL_DT=<span class="variable">$1</span></span><br><span class="line">    genTargetFileModel <span class="variable">$&#123;ETL_DT&#125;</span></span><br><span class="line">    sendByRysnc <span class="variable">$&#123;ETL_DT&#125;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mainStart 2017-09-29</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;awk是Shell中三剑客之一，功能十分强大，本文分享的通过一份管道输入，实现多目录输出，废话不多说，直接上代码&lt;/p&gt;
    
    </summary>
    
      <category term="Shell" scheme="https://deletee.github.io/categories/shell/"/>
    
    
      <category term="shell" scheme="https://deletee.github.io/tags/shell/"/>
    
      <category term="awk" scheme="https://deletee.github.io/tags/awk/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库之数据建模</title>
    <link href="https://deletee.github.io/2018/12/29/data-modeling/"/>
    <id>https://deletee.github.io/2018/12/29/data-modeling/</id>
    <published>2018-12-29T07:46:30.000Z</published>
    <updated>2018-12-31T15:33:50.871Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、为什么需要数据建模？"><a href="#1、为什么需要数据建模？" class="headerlink" title="1、为什么需要数据建模？"></a>1、为什么需要数据建模？</h2><p>理解关键字：数据建模就是数据组织和存储的方法<br>数据模型是数据组织和存储方法，正如我们希望图书馆的书分门别类的放置。<br>数据模型强调从业务、数据存取和使用角度合理规划数据</p><p>那么为什么需要数据建模呢？我们可以从建模有哪些好处理解</p><a id="more"></a><p>（1）性能 良好的模型能帮助我们快速查询所需要的数据，减少不必要的数据查询I/O<br>（2）成本 良好的数据模型能够极大地减少不必要的数据存储，也能实现计算结果的复用，极大地降低大数据系统中的存储和计算成本<br>（3）效率 良好的数据模型能够极大地提高用户体验，用户可通过业务沉淀后的模型尽可能减少不必要的查询，从而提高效率<br>（4）质量 良好的数据模型将遵循一定的数据规范（例如：口径约束，字段约束等）从而减少数据计算的失误，保障数据质量</p><h2 id="2、OLTP与OLAP的区别"><a href="#2、OLTP与OLAP的区别" class="headerlink" title="2、OLTP与OLAP的区别"></a>2、OLTP与OLAP的区别</h2><p>理解关键字：</p><p> OLTP 是讲究事务，在OLAP中事务不是所关注的，主要是批量的读写，致力于联机分析<br> 一个是讲究快速响应，一个是讲究大吞吐<br>OLTP通常使用ER模型，采用三范式消除数据中的冗余，而OLAP采用的模型则比较丰富</p><h2 id="3、有哪些建模方法论？"><a href="#3、有哪些建模方法论？" class="headerlink" title="3、有哪些建模方法论？"></a>3、有哪些建模方法论？</h2><p>（1）ER建模<br>（2）维度建模<br>（3）Data vault建模<br>（4）Anchor建模</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、为什么需要数据建模？&quot;&gt;&lt;a href=&quot;#1、为什么需要数据建模？&quot; class=&quot;headerlink&quot; title=&quot;1、为什么需要数据建模？&quot;&gt;&lt;/a&gt;1、为什么需要数据建模？&lt;/h2&gt;&lt;p&gt;理解关键字：数据建模就是数据组织和存储的方法&lt;br&gt;数据模型是数据组织和存储方法，正如我们希望图书馆的书分门别类的放置。&lt;br&gt;数据模型强调从业务、数据存取和使用角度合理规划数据&lt;/p&gt;
&lt;p&gt;那么为什么需要数据建模呢？我们可以从建模有哪些好处理解&lt;/p&gt;
    
    </summary>
    
      <category term="数据仓库" scheme="https://deletee.github.io/categories/edw/"/>
    
    
      <category term="建模" scheme="https://deletee.github.io/tags/%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="数据仓库" scheme="https://deletee.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库之维度建模</title>
    <link href="https://deletee.github.io/2018/12/29/dim-modeling/"/>
    <id>https://deletee.github.io/2018/12/29/dim-modeling/</id>
    <published>2018-12-29T07:46:30.000Z</published>
    <updated>2018-12-31T15:33:46.349Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、什么是维度建模？"><a href="#1、什么是维度建模？" class="headerlink" title="1、什么是维度建模？"></a>1、什么是维度建模？</h3><p><strong>理解关键字：</strong>维度建模的出发点是实现快速的数据分析与决策，维度建模通常面向业务人员、分析人员使用，相对ER建模来说会更加开放，更容易理解</p><p>维度建模是从业务过程中提炼而来，典型维度建表代表星形建模和雪花建模<br><a id="more"></a></p><h3 id="2、维度建模一般过程（摘自《大数据之路》-amp-《离线和实时大数据之战》）"><a href="#2、维度建模一般过程（摘自《大数据之路》-amp-《离线和实时大数据之战》）" class="headerlink" title="2、维度建模一般过程（摘自《大数据之路》 &amp; 《离线和实时大数据之战》）"></a>2、维度建模一般过程（摘自《大数据之路》 &amp; 《离线和实时大数据之战》）</h3><p>Kimball四步建模流程适合上述数据仓库系统建设流程中模型设计环节，重点解决数据粒度、维度设计和事实表设计问题。</p><p>（1）选择业务过程</p><p>​          业务过程描述的是企业的业务活动，例如电商系统用户下单，账号系统用户注册了一个账号</p><p>（2）选择粒度</p><p>​           那么什么是粒度呢？粒度是描述的是活动细节的每一个子项，例如，用户注册一个账号，包含了用户名、时间、IP、终端类型、手机号码，帐号类型等</p><p>（3）确定维度</p><p>​           确定维度之前，需要先明确什么是维度？维度是表示业务活动在不同角度的表现方式，例如：注册账号的活动中，从时间、IP、终端类型等角度进行描述</p><p>​           比如：用户名、注册时间、注册IP、注册终端、注册手机号码、注册账号类型（手机账号、邮箱账号、三方账号）</p><p>（4）确定事实</p><p>​          确定维度之后，事实则变的清晰，比如：我们需要计算某终端、某天的注册量</p><p>那么问题来了？</p><hr><p>1、如何确定粒度？</p><p>​             最细粒度和聚合粒度之争？ 到底是选择最细粒度，还是聚合粒度？</p><p>2、如何确定维度？</p><p>​             标识维度解决的是业务人员如何描述来自业务过程的数据，维度用来表示“谁、什么、何时、何处、为何、如何”的问题。以竞价广告检索流程而言就是客户通过什么渠道、什么样的客户端（OS、IP）、检索了什么样的内容、请求最终有谁受理等。</p><p>3、如何确定事实？</p><p>​             标识事实其实是在确定业务过程的度量指标，指标何来？哪些指标必须保留，那些指标必须删除，待定指标如何处理？<strong>必须综合考虑业务用户需求和现实数据的实际情况</strong>。事实表的设计完全依赖于物理活动，不受可能产生的最终报表的影响，报表只是事实表设计的参考视角。</p><p>​            <strong>指标必需要考虑业务的需求和数据的实际情况</strong></p><h3 id="3、雪花模型-与-星形模型"><a href="#3、雪花模型-与-星形模型" class="headerlink" title="3、雪花模型 与 星形模型"></a>3、雪花模型 与 星形模型</h3><p><strong>理解关键字：时间与空间交换</strong></p><p><strong>（1）雪花模型是充分展开维度，当出现维度层次时，例如：某商品分大类、种类、小类，使用雪花模型建模时，通常将维度表中记录小类的ID，额外设计一张类别的维度表</strong></p><p><strong>雪花模型去除了数据冗余，节省了部分存储，但也带来了一定的不变，使得业务人员分析查询时需要关联多张维度表。</strong></p><p><strong>（2）星系模型则是允许一定的冗余，将维度层次扁平化，通过牺牲一小部分空间换取查询时快捷</strong></p><p>PS:某些情况下可以需要用到雪花模型</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1、什么是维度建模？&quot;&gt;&lt;a href=&quot;#1、什么是维度建模？&quot; class=&quot;headerlink&quot; title=&quot;1、什么是维度建模？&quot;&gt;&lt;/a&gt;1、什么是维度建模？&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;理解关键字：&lt;/strong&gt;维度建模的出发点是实现快速的数据分析与决策，维度建模通常面向业务人员、分析人员使用，相对ER建模来说会更加开放，更容易理解&lt;/p&gt;
&lt;p&gt;维度建模是从业务过程中提炼而来，典型维度建表代表星形建模和雪花建模&lt;br&gt;
    
    </summary>
    
      <category term="数据仓库" scheme="https://deletee.github.io/categories/edw/"/>
    
    
      <category term="建模" scheme="https://deletee.github.io/tags/%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="数据仓库" scheme="https://deletee.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
      <category term="维度" scheme="https://deletee.github.io/tags/%E7%BB%B4%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库之ER建模</title>
    <link href="https://deletee.github.io/2018/12/29/er-modeling/"/>
    <id>https://deletee.github.io/2018/12/29/er-modeling/</id>
    <published>2018-12-29T07:46:30.000Z</published>
    <updated>2018-12-31T15:33:43.573Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、什么是ER建模？"><a href="#1、什么是ER建模？" class="headerlink" title="1、什么是ER建模？"></a>1、什么是ER建模？</h3><p><strong>理解关键字：ER模型又称实体-关系模型，遵循3NF建模，采用ER进行数据仓库建模需要从整个企业的角度理清各业务之间的关系，建模的出发点是基于企业数据的整合，建设EDW需要建模人员对企业整体业务有精深的把控</strong></p><p><strong>例如：Teradata的 FS-LDM模型，将金融业务分为10大主题，通常是对整体行业发展的沉淀，将成熟的模型做适当的调整即可快速落地实施</strong><br><a id="more"></a></p><h3 id="2、ER建模的三个阶段"><a href="#2、ER建模的三个阶段" class="headerlink" title="2、ER建模的三个阶段"></a><strong>2、ER建模的三个阶段</strong></h3><p><strong>理解关键字：ER模型是自上而下的建模方式，由此建模的需要先从企业的整体框架进行高度抽象</strong></p><p><strong>（1）高层模型</strong><br>​         一个高度抽象的模型，描述主要的主题及主体间的关系，用于描述企业的总体情况<br><strong>（2）中层模型</strong><br>​         在高层模型的基础上，细化主题域的实体、关系等数据项<br><strong>（3）物理模型</strong><br>​        在中层模型的基础上，通常会根据平台的特性进行物理属性设计，例如：分表、分区、分区索引等设计<br><strong>理解关键字2：先通过整合整个企业的业务关系，划分相应主题域及对应的主题之间的关系，再细化每个主题域中的内容，最后是根据平台的特点进行物理设计</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1、什么是ER建模？&quot;&gt;&lt;a href=&quot;#1、什么是ER建模？&quot; class=&quot;headerlink&quot; title=&quot;1、什么是ER建模？&quot;&gt;&lt;/a&gt;1、什么是ER建模？&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;理解关键字：ER模型又称实体-关系模型，遵循3NF建模，采用ER进行数据仓库建模需要从整个企业的角度理清各业务之间的关系，建模的出发点是基于企业数据的整合，建设EDW需要建模人员对企业整体业务有精深的把控&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;例如：Teradata的 FS-LDM模型，将金融业务分为10大主题，通常是对整体行业发展的沉淀，将成熟的模型做适当的调整即可快速落地实施&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="数据仓库" scheme="https://deletee.github.io/categories/edw/"/>
    
    
      <category term="建模" scheme="https://deletee.github.io/tags/%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="数据仓库" scheme="https://deletee.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
      <category term="ER" scheme="https://deletee.github.io/tags/ER/"/>
    
  </entry>
  
  <entry>
    <title>Hive SQL优化</title>
    <link href="https://deletee.github.io/2018/12/29/sql-optimization/"/>
    <id>https://deletee.github.io/2018/12/29/sql-optimization/</id>
    <published>2018-12-29T07:46:30.000Z</published>
    <updated>2018-12-31T15:33:35.261Z</updated>
    
    <content type="html"><![CDATA[<h4 id="0x00-group-by-引起的倾斜"><a href="#0x00-group-by-引起的倾斜" class="headerlink" title="0x00 group by 引起的倾斜"></a><strong>0x00 group by 引起的倾斜</strong></h4><h5 id="1-优化措施"><a href="#1-优化措施" class="headerlink" title="1|优化措施"></a>1|优化措施</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr = <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">true</span></span><br></pre></td></tr></table></figure> <a id="more"></a><h5 id="2-原理"><a href="#2-原理" class="headerlink" title="2|原理"></a>2|原理</h5><p>​    作业将生成两个MapReduce Job</p><p>​    第一个MapReduce Job中 Map的输出结果会随机的分布到Reduce中，每个Reduce做部分聚合操作并输出结果，这样处理的结果 相同的group by key会分布到不同的reduce中</p><p>​    第二个MapReduce Job中 再根据 相同group by key分布到reduce中</p><h4 id="0x01-count-distinct-优化"><a href="#0x01-count-distinct-优化" class="headerlink" title="0x01 count distinct 优化"></a><strong>0x01 count distinct 优化</strong></h4><h5 id="1-例子"><a href="#1-例子" class="headerlink" title="1|例子"></a>1|例子</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> ssn) <span class="keyword">from</span> <span class="keyword">table</span></span><br></pre></td></tr></table></figure><h5 id="2-说明"><a href="#2-说明" class="headerlink" title="2|说明"></a>2|说明</h5><p>​    由于如上SQL，Hive会将Map阶段的输出全部分布到一个Reduce中，很容易引起性能问题</p><h5 id="3-正确的操作方法"><a href="#3-正确的操作方法" class="headerlink" title="3|正确的操作方法"></a>3|正确的操作方法</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) </span><br><span class="line"> <span class="keyword">from</span> (</span><br><span class="line">        <span class="keyword">select</span> ssn <span class="keyword">from</span> <span class="keyword">table</span> </span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span> ssn</span><br><span class="line">) t</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 原理</span><br><span class="line">-- 利用group by 先进行数据去重，然后再进行计数，计数是不可避免的，目的是为了大量减少重复计数</span><br></pre></td></tr></table></figure><h4 id="0x02-大表join小表"><a href="#0x02-大表join小表" class="headerlink" title="0x02 大表join小表"></a><strong>0x02 大表join小表</strong></h4><h5 id="1-方法"><a href="#1-方法" class="headerlink" title="1|方法"></a>1|方法</h5><p>​    一般都是采用 map join的方法，默认Hive是开启 mapjoin，其Hive设置参数如下： </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>​    或者是使用Hint的方式进行小表指定</p><h5 id="2-案例"><a href="#2-案例" class="headerlink" title="2|案例"></a>2|案例</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+mapjoin(t1)*/</span></span><br><span class="line">       t0.ssn</span><br><span class="line">       ,t1.name</span><br><span class="line"><span class="keyword">from</span> big_table0 t0</span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span></span><br><span class="line">     small_table1 t1</span><br><span class="line"><span class="keyword">on</span> t0.ssn = t1.ssn;</span><br><span class="line"><span class="comment">-- /*+mapjoin(t1)*/ 即 map join hint，如果需要mapjoin多个表，则格式为 /*+mapjoin(t1,t2,t3)*/</span></span><br></pre></td></tr></table></figure><h4 id="0x03-大表join大表-待补充"><a href="#0x03-大表join大表-待补充" class="headerlink" title="0x03 大表join大表(待补充)"></a><strong>0x03 大表join大表(待补充)</strong></h4><p>​    1|问题场景</p><p>​    2|解决方法1</p><p>​    3|解决方法2</p><h4 id="0x04-优化案例"><a href="#0x04-优化案例" class="headerlink" title="0x04 优化案例"></a>0x04 优化案例</h4><h5 id="1-业务背景："><a href="#1-业务背景：" class="headerlink" title="1|业务背景："></a>1|业务背景：</h5><p>​       筛选出 当日 注册产品、注册IP为维度下超过30个帐号的 帐号清单</p><h5 id="2-常规做法"><a href="#2-常规做法" class="headerlink" title="2|常规做法"></a>2|常规做法</h5><p>​      先筛选出满足 注册产品、注册IP 数量超过30个的 注册产品及注册IP的维度清单，然后再关联源表进行筛选出帐号</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp.tmp_algorithm1 <span class="keyword">as</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  t1.ssn,</span><br><span class="line">  <span class="number">1</span> <span class="keyword">AS</span> rubbish_rate</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      dt,</span><br><span class="line">      ssn,</span><br><span class="line">      reg_ip,</span><br><span class="line">      reg_product</span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      dw.dw_user_static_reg_info_dd</span><br><span class="line">    <span class="keyword">WHERE</span> dt = <span class="string">'2018-11-22'</span></span><br><span class="line">          <span class="keyword">AND</span> (</span><br><span class="line">            <span class="keyword">split</span>(ssn, <span class="string">'@'</span>) [<span class="number">1</span>] <span class="keyword">IN</span> (<span class="string">'126.com'</span>, <span class="string">'yeah.net'</span>, <span class="string">'vip.126.com'</span>, <span class="string">'vip.163.com'</span>)</span><br><span class="line">            <span class="keyword">OR</span></span><br><span class="line">            <span class="keyword">split</span>(ssn, <span class="string">'@'</span>) [<span class="number">1</span>] <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">          )</span><br><span class="line">  ) t1</span><br><span class="line">  <span class="keyword">JOIN</span></span><br><span class="line">  (</span><br><span class="line">         <span class="keyword">SELECT</span></span><br><span class="line">           dt,</span><br><span class="line">           reg_ip,</span><br><span class="line">           reg_product,</span><br><span class="line">           <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line">         <span class="keyword">FROM</span></span><br><span class="line">           dw.dw_user_static_reg_info_dd</span><br><span class="line">         <span class="keyword">WHERE</span> dt = <span class="string">'2018-11-22'</span></span><br><span class="line">               <span class="keyword">AND</span> (</span><br><span class="line">                 <span class="keyword">split</span>(ssn, <span class="string">'@'</span>) [<span class="number">1</span>] <span class="keyword">IN</span> (<span class="string">'126.com'</span>, <span class="string">'yeah.net'</span>, <span class="string">'vip.126.com'</span>, <span class="string">'vip.163.com'</span>)</span><br><span class="line">                 <span class="keyword">OR</span></span><br><span class="line">                 <span class="keyword">split</span>(ssn, <span class="string">'@'</span>) [<span class="number">1</span>] <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">               )</span><br><span class="line">         <span class="keyword">GROUP</span> <span class="keyword">BY</span> dt,reg_ip, reg_product</span><br><span class="line">         <span class="keyword">HAVING</span> cnt &gt;= <span class="number">30</span></span><br><span class="line">       ) t2</span><br><span class="line">    <span class="keyword">ON</span> t1.dt=t2.dt <span class="keyword">and</span> t1.reg_ip = t2.reg_ip <span class="keyword">AND</span> t1.reg_product = t2.reg_product</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t1.ssn</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h5 id="3-优化做法"><a href="#3-优化做法" class="headerlink" title="3|优化做法"></a>3|优化做法</h5><p>​     使用窗口函数进行维度内计数，如下通过窗口函数计算在  维度 注册产品、注册IP下的数量，通过子查询进行筛选出目标帐号清单</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp.tmp_algorithm2 <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> ssn,<span class="number">1</span> <span class="keyword">as</span> rubbish_rate</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">       ssn</span><br><span class="line">      ,reg_ip</span><br><span class="line">      ,reg_product</span><br><span class="line">      ,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> reg_ip,reg_product) <span class="keyword">as</span> cnt</span><br><span class="line"> <span class="keyword">from</span> dw.dw_user_static_reg_info_dd</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'2018-11-22'</span></span><br><span class="line"> <span class="keyword">and</span> ( </span><br><span class="line">        <span class="keyword">split</span>(ssn, <span class="string">'@'</span>) [<span class="number">1</span>] <span class="keyword">in</span> (<span class="string">'126.com'</span>, <span class="string">'yeah.net'</span>, <span class="string">'vip.126.com'</span>, <span class="string">'vip.163.com'</span>) </span><br><span class="line">      <span class="keyword">or</span> <span class="keyword">split</span>(ssn, <span class="string">'@'</span>) [<span class="number">1</span>] <span class="keyword">is</span> <span class="literal">null</span></span><br><span class="line">  )</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> cnt &gt;= <span class="number">30</span>;</span><br></pre></td></tr></table></figure><h5 id="4-优化思路"><a href="#4-优化思路" class="headerlink" title="4|优化思路"></a>4|优化思路</h5><p>​     由于HQL 是转换为MapReduce的形态进行数据计算，常规做法中会启用大量的MapReduce，并重复的抽取了源表数据，浪费了计算，优化做法是实现一份数据提取，复杂内存计算，从而减少使用计算资源</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;0x00-group-by-引起的倾斜&quot;&gt;&lt;a href=&quot;#0x00-group-by-引起的倾斜&quot; class=&quot;headerlink&quot; title=&quot;0x00 group by 引起的倾斜&quot;&gt;&lt;/a&gt;&lt;strong&gt;0x00 group by 引起的倾斜&lt;/strong&gt;&lt;/h4&gt;&lt;h5 id=&quot;1-优化措施&quot;&gt;&lt;a href=&quot;#1-优化措施&quot; class=&quot;headerlink&quot; title=&quot;1|优化措施&quot;&gt;&lt;/a&gt;1|优化措施&lt;/h5&gt;&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;set&lt;/span&gt; hive.map.aggr = &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;set&lt;/span&gt; hive.groupby.skewindata = &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="SQL" scheme="https://deletee.github.io/categories/sql/"/>
    
    
      <category term="Hive" scheme="https://deletee.github.io/tags/Hive/"/>
    
      <category term="SQL" scheme="https://deletee.github.io/tags/SQL/"/>
    
      <category term="优化" scheme="https://deletee.github.io/tags/%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>维度设计</title>
    <link href="https://deletee.github.io/2018/12/29/dim-design/"/>
    <id>https://deletee.github.io/2018/12/29/dim-design/</id>
    <published>2018-12-29T07:46:30.000Z</published>
    <updated>2018-12-31T15:33:48.742Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>都说维度设计是维度建模的灵魂，在维度建模中，我们将度量成为「事实」，将环境描述为「维度」</p><p>但是如何设计一个好的维度模型，我们需要遵循一定的技巧和方法。</p><h3 id="如何标识维度？"><a href="#如何标识维度？" class="headerlink" title="如何标识维度？"></a>如何标识维度？</h3><a id="more"></a><p>通常我们都会问「谁、何时、何地、为何、如何」来描述一个事件过程，同样维度可以理解为一个事件或者业务过程的角度，比如商品购买</p><p>购买人、购买事件、购买产品、购买使用的设备、购买使用的IP等均为维度范畴</p><h3 id="围绕业务流程来构建维度"><a href="#围绕业务流程来构建维度" class="headerlink" title="围绕业务流程来构建维度"></a>围绕业务流程来构建维度</h3><p>考虑到需求总是变换多端，但是有一点不变：需求总是围绕着业务流程，那么当我们设计维度的时候，就需要充分生成丰富的维度及属性</p><h3 id="维度的基本设计方法？"><a href="#维度的基本设计方法？" class="headerlink" title="维度的基本设计方法？"></a>维度的基本设计方法？</h3><p>（1）根据业务流程确定主维度</p><p>​         即在业务过程中找出关键维度作为主维度，在企业级数据仓库中，必需维度的统一口径，以商品为例，有且仅有一个维度定义</p><p>（2）确定相关维度</p><p>（3）确定维度属性</p><ul><li>尽可能生成丰富的维度属性</li><li>尽可能多地给出包括一些富有意义的文字性描述<br>​         维度建模中较多维度以ID形态存在，例如：商品ID，但是单一的ID在业务描述非常欠缺，通常ID的含义描述也随ID作为维度设计中的一部分</li><li>区分数值型属性和事实<br>​       在一些业务数据中，一些数值型字段是作为维度还是作为度量（事实）</li><li>尽量沉淀出通用的维度属性<br>​        有些维度属性值获取是经过了复杂的逻辑处理，或者是通过解析某个字段得到，或是多张表关联得到，当设计出负责的维度时，若不保留原生维度，将难以覆盖业务的变化<br>​        并且属性值格式做到统一，比如某模型A时间格式为 yyyy-MM-dd HH:mm:ss，而模型B中的时间格式 则是 Unix Timestamp，同样也将造成口径不一致</li></ul><p><strong>理解关键字：维度设计需要严格遵循开发规范，对字段命名、含义、口径、属性值格式的规范，需严格遵循。</strong></p><h3 id="常见维度设计方法"><a href="#常见维度设计方法" class="headerlink" title="常见维度设计方法"></a>常见维度设计方法</h3><hr><p><strong>（1）缓慢变化或快速变化</strong></p><p>​         维度缓慢变化</p><ul><li>修改维度属性值      不保留历史操作，适用于无需关心历史维度值的情形。</li><li>插入一行                 保留历史操作，由于多了新记录，使得维度关联时，会产生冗余记录，增加额外学习成本</li><li>插入一列                 预留一列作为上一次维度值的保留，适用于只需要查看上一次的维度值情形</li><li>快照维度                 即将每天的变更存储为一个快照，在维度表较小时，可以忽略存储</li><li>极限存储                 通过拉链表的形式，记录维度的缓慢变化，但是注意，维度表中有频繁变化的字段。</li></ul><p>​         维度快速变化</p><p>​         <strong>此类问题又称微型维度，</strong>主要是为了解决快变超大维度（rapidly changing monster dimension）,顾名思义，在某些维度表中，大量维度进行缓慢变化甚至没有变化，但是有少量的维度频繁的发生变化，此种情形下，若使用缓慢变化的方式进行处理的话，将消耗大量资源。由此解决办法是：</p><p>​         将维度属性值频率变化比较高的字段提取出来，建立一个单独的维度表，只需维度这一张快速变化的维度表即可。</p><p><strong>（2）维度的层次结构</strong></p><p>​           <strong>也是通常所说的 规范化和反规范化</strong></p><p>​          在维度设计的过程中，我们经常遇到维度的层次问题，比如：商品通常会有一级分类、二级分类、三级分类此种情形，对于这样的维度处理方式，通常有两种做法：</p><ul><li>星系模型   将维度层次问题扁平化，将层次的维度项进行扁平处理，例如有如下维度：sku_id，category_lv1, category_lv2, category_lv3 …</li><li>雪花模型   将层级维度提炼出来单独作为一张新的维度表</li></ul><p>上文已经阐述过，两种模型的利弊，在新互联网的大数据平台基础上，存储也变的非常廉价，通常是选用星系模型解决维度层次的问题</p><p>（3）维度一致性</p><p>In data warehousing, a conformed dimension is a dimension that has the same meaning to every fact with which it relates. Conformed dimensions allow facts and measures to be categorized and described in the same way across multiple facts and/or data marts, ensuring consistent reporting across the enterprise.</p><p>​         维度一致性是如此解释的，在数据仓库中，一致的维度是与其相关的每个事实具有相同含义的维度。 一致的维度允许在多个事实和/或数据集市中以相同的方式对事实和度量进行分类和描述，从而确保整个企业的一致报告。</p><p>​         通常在数据仓库开发过程中如何保证维度的一致性呢？</p><ul><li><p>维度的定义需要遵循命名、取值等规范</p></li><li><p>维度需通过元数据进行管理  采用维度建模，意味着建仓过程是自下而上的，各个数据集市各自开发，那么如何保证企业级数据仓库维度一致性，则需要统一的元数据进行管理</p></li></ul><p>维度建模的数据仓库中，有一个概念叫Conformed Dimension，中文一般翻译为“一致性维度”。一致性维度是Kimball的多维体系结构（MD）中的三个关键性概念之一，另两个是总线架构（Bus Architecture）和一致性事实（Conformed Fact）。</p><p>在多维体系结构中，没有物理上的数据仓库，由物理上的数据集市组合成逻辑上的数据仓库。而且数据集市的建立是可以逐步完成的，最终组合在一起，成为一个数据仓库。如果分步建立数据集市的过程出现了问题，数据集市就会变成孤立的集市，不能组合成数据仓库，而一致性维度的提出正式为了解决这个问题。</p><p>一致性维度的范围是总线架构中的维度，即可能会在多个数据集市中都存在的维度，这个范围的选取需要架构师来决定。一致性维度的内容和普通维度并没有本质上区别，都是经过数据清洗和整合后的结果。</p><p>一致性维度建立的地点是多维体系结构的后台（Back Room），即数据准备区。在多维体系结构的数据仓库项目组内需要有专门的维度设计师，他的职责就是建立维度和维护维度的一致性。在后台建立好的维度同步复制到各个数据集市。这样所有数据集市的这部分维度都是完全相同的。建立新的数据集市时，需要在后台进行一致性维度处理，根据情况来决定是否新增和修改一致性维度，然后同步复制到各个数据集市。这是不同数据集市维度保持一致的要点。</p><p>在同一个集市内，一致性维度的意思是两个维度如果有关系，要么就是完全一样的，要么就是一个维度在数学意义上是另一个维度的子集。例如，如果建立月维度话，月维度的各种描述必须与日期维度中的完全一致，最常用的做法就是在日期维度上建立视图生成月维度。这样月维度就可以是日期维度的子集，在后续钻取等操作时可以保持一致。如果维度表中的数据量较大，出于效率的考虑，应该建立物化视图或者实际的物理表。</p><p>这样，维度保持一致后，事实就可以保存在各个数据集市中。虽然在物理上是独立的，但在逻辑上由一致性维度使所有的数据集市是联系在一起，随时可以进行交叉探察等操作，也就组成了数据仓库。</p><p>（4）维度整合和拆分</p><p>​         因为数据仓库的数据源自各个系统，有从移动端，web端，PC端，每个段的数据结构差异较大，即便是同一端，可能因为业务拆分，字段的属性值也不完全一致。</p><p>​         例如：A系统 某字段值用 1/0表示 token是否有效，B系统则使用F/T表示是否有效</p><p>​         那么，在此种情形下，需要进行维度整合。维度整合需要遵循如下规范：</p><ul><li>命名规范统一</li><li>字段类型统一</li><li>属性值编码和含义统一</li></ul><p>​      另外有一种场景，因为系统差异太大，而无法进行维度整合，此时需要进行维度的拆分</p><p>​      例如：加油站主要的商品是则是油，加油站内同样也有零售店，售卖一些日常百货，油 有 92，95，98等维度属性，日常百货的通常 涉及的 进销存，单价等</p><p>​                 两种商品在维度差异过大，通常的做法，加油站的主营业务是售卖油品，从而建立主要的商品维度表，另外建立一个零售商品维度记录表</p><p>​      <strong>理解关键词：各个业务差异独特性较大的业务各自建立独立的两个维度表</strong></p><p>（5）杂项维度的处理方法</p><ul><li><strong>退化维度</strong><br>​       所谓退化维度（degenerate dimension），是指在实施表中那些看起来像是一个事实表的一个维度关键字，但实际上没有对应的维度字段。退化维度一般都是事务的编号，如购物小票编号，发票编号</li><li><strong>行为维度</strong><br>​       行为维度是基于过去维度成员的行为进行分组或者过滤事实的办法。行为维度即将事实转化为维度，以确保获得更多的分析能力<br>​       例如：购买次数超过30次，30次至100次，超过100次 作为维度值</li><li><strong>角色维度</strong><br>​       角色维度通常是一个业务活动有多个角色参与，例如：办理银行业务，有客户经理，审批人等参与两个及以上角色，而这些角色均属于员工维度表，对于此种情形，没有必要根据角色建立多个维度表，而是可以通过建立视图的方式达到目的。</li><li><strong>多值维度</strong><br>​       <strong>多值维度一般会出现多对多的关系中，例如：购买房产，会有夫妻两人共同持有，一次下单，多个子订单</strong></li></ul><p>​       有如下三种方法：</p><p>​                1）降低事实表的粒度      每一个事实都标注最小粒度，例如：前台业务与商业智能关注交易子订单，每一个子订单一个事实，只会有一个商品与之对应，很多时候，事实表的粒度是不能降低的，当强行降低之后，那么订单事实则发生改变，对于统计每日订单量计算，则复杂度变高，增加学习和计算成本</p><p>​                2）采用多字段   例如：在买房合同中，标注第一买受人，第二买受人，可顺位增加冗余的字段位</p><p>​               3）使用常用的桥接表      例如：订单中，确定父订单为粒度，建立父订单与子订单的桥接表，另建立子订单与商品的维度关系表，桥接表包含事实表关联的分组KEY，以及作为买受方的维度表外键ID，有多个买受方，则有在相同的KEY下有多条记录，桥接表需要更多的计算，也可能会造成双重计算，例如，买受人1籍贯为山东，买受人2籍贯为浙江，那么当分别统计外地山东的购房数，和浙江的购房数，则产生了多重计算。双重计算不一定是错误，对于一些业务需求是合理的，但对于另一些业务需求，则需要规避。</p><p>​         <strong>理解关键字：</strong>当前大数据平台支持复杂的数据结构，将多个买受人可以作为数据组结构存入一个字段中，使用key:value的形式，当然此种方法需增加一定的计算成本，属于非规范化操作。</p><ul><li><strong>杂项维度（junk dimension）</strong><br>​      <strong>杂项维度就是一种包含的数据具有很少可能值的维度</strong><br>​      <strong>当定义好各种维度后，会发现一些小范围维度取离散值或者标志位的字段，但是这样的维度又很难退化存储在事实表中，可能会造成事实表过大，若果单独建立维度表进行关联，通过外键关联，会出现维度过多的情况，如果将这些字段删除，则业务方不同意。</strong><br> 这时，我们通常的解决方案就是建立杂项维度，将这些字段建立到一个维度表中，在事实表中只需保存一个外键。<strong>几个字段的不同取值组成一条记录，生成代理键，存入维度表</strong>，并将该代理键保存入相应的事实表字段。建议不要直接使用所有的组合生成完整的杂项维度表，在抽取时遇到新的组合时生成相应记录即可。杂项维度的ETL过程比一般的维度略为复杂。</li></ul><p><strong>几个字段的不同取值组成一条记录，生成代理键，存入维度表： 即将几个杂项维度的组合建立杂项维度表，生成一个代理键，植入事实表中</strong>            </p><ul><li><strong>微型维度</strong></li></ul><p><strong>理解关键字2：</strong></p><p>​         <strong>第一：避免维度过度增长     某些维度值变化过高，如果维度表使用了拉链极限存储，那边过度增长或者变化的维度，将使得极限存储效率差</strong></p><p>​         <strong>第二：避免耦合度过高        例如：卖家的主营项目，加工逻辑异常复杂，如果融合进现有的卖家维表中，那么过多的业务耦合将会导致卖家维度难以维护，应适当做维度拆分</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;都说维度设计是维度建模的灵魂，在维度建模中，我们将度量成为「事实」，将环境描述为「维度」&lt;/p&gt;
&lt;p&gt;但是如何设计一个好的维度模型，我们需要遵循一定的技巧和方法。&lt;/p&gt;
&lt;h3 id=&quot;如何标识维度？&quot;&gt;&lt;a href=&quot;#如何标识维度？&quot; class=&quot;headerlink&quot; title=&quot;如何标识维度？&quot;&gt;&lt;/a&gt;如何标识维度？&lt;/h3&gt;
    
    </summary>
    
      <category term="数据仓库" scheme="https://deletee.github.io/categories/edw/"/>
    
    
      <category term="建模" scheme="https://deletee.github.io/tags/%E5%BB%BA%E6%A8%A1/"/>
    
      <category term="数据仓库" scheme="https://deletee.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>利用fuse_dfs快速建立hdfs云存储</title>
    <link href="https://deletee.github.io/2018/08/16/fuse_hdfs/"/>
    <id>https://deletee.github.io/2018/08/16/fuse_hdfs/</id>
    <published>2018-08-16T07:46:30.000Z</published>
    <updated>2018-12-31T15:33:38.181Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h3><p>将HDFS挂载到本地，允许用户像访问本地文件系统一样访问远程文件系统，用户可像读写本地文件一样读写HDFS上的文件，大大简化了HDFS使用，方法较为常用两种方法是：利用fuse_dfs或者利用NFSv3，因NFSv3需对Hadoop集群端进行操作，修改代价太大，本文介绍以fuse_dfs，只需在客户端进行配置即可挂载hdfs。<br><a id="more"></a><br>用户空间文件系统（Filesystem in Userspace，简称FUSE）是一个面向类Unix计算机操作系统的软件接口，它使无特权的用户能够无需编辑内核代码而创建自己的文件系统。目前Linux通过内核模块对此进行支持。一些文件系统如ZFS、glusterfs和lustre使用FUSE实现。Linux用于支持用户空间文件系统的内核模块名叫FUSE，FUSE一词有时特指Linux下的用户空间文件系统。通过使用Hadoop的Fuse-DFS分类模块，任意一个Hadoop文件系统(不过一般为HDFS)都可以作为一个标准文件系统进行挂载。<br><br><br>Hadooop源码中自带了fuse-dfs模块，利用fuse_dfs将hdfs当一个网盘挂载到本地。</p><h3 id="二、应用场景"><a href="#二、应用场景" class="headerlink" title="二、应用场景"></a>二、应用场景</h3><p>同步：实现用户对个人计算指定目录下的特定类型的文件自动与HDFS同步，实时监测用户文件变化情况。不需要用户干预就能及时自动备份文件。当用户新建、修改或删除文件（目录）时自动更新同步，保障用户重要文件的安全。当用户移动办公时，可直接从HDFS访问，省略了移动存储介质的拷贝过程，提高用户工作效率。更换新计算机时，用户登录后原计算机中的个人文件将自动从HDFS存储中同步到新计算机中。例如：适用数据仓库需要ETL进行Hadoop文件进行ftp或者rsync推送的场景。</p><p>网盘：用户可将不常用的文件存储在HDFS存储中，从而节约本地存储空间，提高文件的物理安全性，也便于资源共享。</p><p>共享：用户可以把网盘或同步盘的任意文件（目录）分享给群组或其他用户。只需要在云存储中存储一份文件，被分享的人不需要下载存储即可在线浏览，实现资源共享。若源文件被修改，所有分享人立即会得到更新文件。</p><h3 id="三、部署实现"><a href="#三、部署实现" class="headerlink" title="三、部署实现"></a>三、部署实现</h3><p>目前网络上关于利用fuse挂载hdfs较多基于较早版本的hadoop(0.x，1.x) ，并大多都基于无kerberos认证，本文以北京亦庄Hadoop 2.5.2为例，介绍通过fuse挂载hdfs到本地的方法。设定源码目录为$HADOOP_SRC_HOME​，那么Hadoop 2.x 的 fuse_dfs 源码位于<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/</span></span><br></pre></td></tr></table></figure></p><p>那么怎么编译fuse_dfs？fuse_dfs是Hadoop自带模块，编译Hadoop时，增加编译参数require.fuse=true，即编译命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar</span><br></pre></td></tr></table></figure></p><h4 id="编译环境"><a href="#编译环境" class="headerlink" title="编译环境"></a>编译环境</h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Linux: 3.16.0-0.bpo.4-amd64 (Debian7)</span><br><span class="line">JDK: 1.7.0_80 64bit</span><br><span class="line">Apache Hadoop: 2.5.2 </span><br><span class="line">Ant: 1.8.2</span><br><span class="line">GCC: 4.7.2 (系统默认)</span><br><span class="line">Protocbuf 2.5.0</span><br><span class="line">Maven 3.0.4</span><br><span class="line">Cmake 2.8.9</span><br><span class="line">Make 3.8.1</span><br></pre></td></tr></table></figure><h4 id="安装fuse等依赖"><a href="#安装fuse等依赖" class="headerlink" title="安装fuse等依赖"></a>安装fuse等依赖</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get -y install maven build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev libfuse-dev</span><br></pre></td></tr></table></figure><h4 id="编译fuse-dfs-Hadoop"><a href="#编译fuse-dfs-Hadoop" class="headerlink" title="编译fuse_dfs(Hadoop)"></a>编译fuse_dfs(Hadoop)</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载</span></span><br><span class="line">mkdir package &amp;&amp; cd package</span><br><span class="line">wget http://archive.apache.org/dist/hadoop/core/hadoop-2.5.2/hadoop-2.5.2-src.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">解压</span></span><br><span class="line">tar -zxvf hadoop-2.5.2-src.tar.gz</span><br><span class="line">ln -s hadoop-2.5.2-src hadoop-src</span><br><span class="line">echo "export HADOOP_SRC_HOME=$HOME/package/hadoop-src/" &gt;&gt; .bashrc &amp;&amp; source .bashrc</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 编译</span></span><br><span class="line">cd hadoop-2.5.2-src</span><br><span class="line">mvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar</span><br></pre></td></tr></table></figure><h6 id="PS-编译Hadoop之前需提前安装好fuse，否则fuse-dfs无法编译成功"><a href="#PS-编译Hadoop之前需提前安装好fuse，否则fuse-dfs无法编译成功" class="headerlink" title="PS: 编译Hadoop之前需提前安装好fuse，否则fuse_dfs无法编译成功"></a>PS: 编译Hadoop之前需提前安装好fuse，否则fuse_dfs无法编译成功</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- [1] 编译后的fuse_dfs位于：</span><br><span class="line"><span class="meta">  $</span><span class="bash">HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs/fuse_dfs</span></span><br><span class="line">- [2] 完整的Hadoop包则位于：</span><br><span class="line"><span class="meta">  $</span><span class="bash">HADOOP_SRC_HOME/hadoop-dist/target/hadoop-2.5.2.tar.gz</span></span><br><span class="line">- [3] 一个用于挂载hdfs的脚本：</span><br><span class="line"><span class="meta">  $</span><span class="bash">HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh</span></span><br></pre></td></tr></table></figure><h4 id="部署Hadoop（略）"><a href="#部署Hadoop（略）" class="headerlink" title="部署Hadoop（略）"></a>部署Hadoop（略）</h4><h4 id="配置fuse-dfs-wrapper-sh"><a href="#配置fuse-dfs-wrapper-sh" class="headerlink" title="配置fuse_dfs_wrapper.sh"></a>配置fuse_dfs_wrapper.sh</h4><p>HDFS挂载主要依赖源码的两个模块，一个是hadoop-hdfs-project，另一个是hadoop-client，可将依赖的jar包单独提炼出来。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env bash</span></span><br><span class="line">export HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/local/share/hadoop&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HADOOP_PREFIX=$HOME/package/hadoop-src</span><br><span class="line">export JAVA_HOME=$HOME/opt/jdk</span><br><span class="line"></span><br><span class="line">export FUSEDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/main/native/fuse-dfs"</span><br><span class="line">export LIBHDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/target/usr/local/lib"</span><br><span class="line"></span><br><span class="line">if [ "$OS_ARCH" = "" ]; then</span><br><span class="line">    export OS_ARCH=amd64</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$JAVA_HOME" = "" ]; then</span><br><span class="line">    export  JAVA_HOME=/usr/local/java</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$LD_LIBRARY_PATH" = "" ]; then</span><br><span class="line">export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/usr/local/lib</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">JARS=`find "$HADOOP_PREFIX/hadoop-hdfs-project" -name "*.jar" | xargs`</span><br><span class="line">for jar in $JARS; do</span><br><span class="line">  CLASSPATH=$jar:$CLASSPATH</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">JARS=`find "$HADOOP_PREFIX/hadoop-client" -name "*.jar" | xargs`</span><br><span class="line">for jar in $JARS; do</span><br><span class="line">  CLASSPATH=$jar:$CLASSPATH</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">export CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATH</span><br><span class="line">export PATH=$FUSEDFS_PATH:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=$LIBHDFS_PATH:$JAVA_HOME/jre/lib/$OS_ARCH/server:$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line">fuse_dfs $@</span><br></pre></td></tr></table></figure><p>利用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> sudo bash ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://$&#123;nn&#125;:$&#123;port&#125; /mnt/hdfs</span><br><span class="line">INFO /home/hzxijingjing/package/hadoop-2.5.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c:164 Adding FUSE arg /mnt/hdfs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 挂载信息</span></span><br><span class="line"></span><br><span class="line"> df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">rootfs           50G  8.2G   39G  18% /</span><br><span class="line">udev             10M     0   10M   0% /dev</span><br><span class="line">tmpfs           1.6G  216K  1.6G   1% /run</span><br><span class="line">/dev/vda1        50G  8.2G   39G  18% /</span><br><span class="line">tmpfs           5.0M  4.0K  5.0M   1% /run/lock</span><br><span class="line">tmpfs           3.2G     0  3.2G   0% /run/shm</span><br><span class="line">/dev/vdb1        99G   75M   94G   1% /data</span><br><span class="line">fuse_dfs         49G     0   49G   0% /mnt/hdfs  # HDFS挂载成功</span><br></pre></td></tr></table></figure><p>​ 至此为止，已实现无简单认证的Hadoop挂载，然而对于配置了kerberos认证的Hadoop，实施挂载会出现如下情况：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /mnt &amp;&amp; ls -lh</span><br><span class="line">ls: cannot access hdfs: Transport endpoint is not connected</span><br><span class="line">d????????? ? ?      ?          ?            ? hdfs</span><br></pre></td></tr></table></figure></p><p>后置参数 -d，进入debug模式， 查看运行信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /bin/sh ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://$&#123;nn&#125;:$&#123;port&#125; /mnt/dfs -d</span><br></pre></td></tr></table></figure><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">............ # 省略一些无用日志 </span><br><span class="line">FUSE <span class="keyword">library</span> version: <span class="number">2</span><span class="variable">.9</span><span class="variable">.0</span></span><br><span class="line">nullpath_ok: <span class="number">0</span></span><br><span class="line">nopath: <span class="number">0</span></span><br><span class="line">utime_omit_ok: <span class="number">0</span></span><br><span class="line"><span class="keyword">unique</span>: <span class="number">1</span>, opcode: INIT (<span class="number">26</span>), nodeid: <span class="number">0</span>, insize: <span class="number">56</span>, pid: <span class="number">0</span></span><br><span class="line">INIT: <span class="number">7</span><span class="variable">.23</span></span><br><span class="line">flags=<span class="number">0</span>x0003fffb</span><br><span class="line">max_readahead=<span class="number">0</span>x00020000</span><br><span class="line">INFO /home/hzxijingjing/<span class="keyword">package</span>/hadoop-<span class="number">2</span><span class="variable">.5</span><span class="variable">.2</span>-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init<span class="variable">.c</span>:<span class="number">98</span> Mounting <span class="keyword">with</span> options: [ <span class="keyword">protected</span>=(NULL), nn_uri=hdfs:<span class="comment">//hadoop1232.hz.163.org:8020, nn_port=0, debug=0, read_only=0, initchecks=0, no_permissions=0, usetrash=1, entry_timeout=60, attribute_timeout=60, rdbuffer_size=10485760, direct_io=0 ]</span></span><br><span class="line">............ # 省略一些无用日志                                                                                                                                                                                                                                                                                          </span><br><span class="line">fuseConnectInit: initialized <span class="keyword">with</span> timer period <span class="number">5</span>, expiry period <span class="number">300</span></span><br><span class="line">   INIT: <span class="number">7</span><span class="variable">.18</span></span><br><span class="line">   flags=<span class="number">0</span>x00000039</span><br><span class="line">   max_readahead=<span class="number">0</span>x00020000</span><br><span class="line">   max_write=<span class="number">0</span>x00020000</span><br><span class="line">   max_background=<span class="number">0</span></span><br><span class="line">   congestion_threshold=<span class="number">0</span></span><br><span class="line">   <span class="keyword">unique</span>: <span class="number">1</span>, success, outsize: <span class="number">40</span></span><br><span class="line"><span class="keyword">unique</span>: <span class="number">2</span>, opcode: STATFS (<span class="number">17</span>), nodeid: <span class="number">1</span>, insize: <span class="number">40</span>, pid: <span class="number">13769</span></span><br><span class="line">statfs /</span><br><span class="line">   <span class="keyword">unique</span>: <span class="number">2</span>, success, outsize: <span class="number">96</span></span><br><span class="line"><span class="keyword">unique</span>: <span class="number">3</span>, opcode: GETATTR (<span class="number">3</span>), nodeid: <span class="number">1</span>, insize: <span class="number">56</span>, pid: <span class="number">13815</span></span><br><span class="line">getattr /</span><br><span class="line">fuseNewConnect: failed to find Kerberos ticket cache file '/tmp/krb5cc_0'.  Did you remember to kinit <span class="keyword">for</span> UID <span class="number">0</span>?</span><br><span class="line">fuseConnect(usrname=root): fuseNewConnect failed <span class="keyword">with</span> error code -<span class="number">13</span></span><br><span class="line">fuseConnectAsThreadUid: failed to open a libhdfs connection!  error -<span class="number">13</span>.</span><br><span class="line">   <span class="keyword">unique</span>: <span class="number">3</span>, error: -<span class="number">5</span> (Input/<span class="keyword">output</span> error), outsize: <span class="number">16</span></span><br><span class="line"><span class="keyword">unique</span>: <span class="number">4</span>, opcode: STATFS (<span class="number">17</span>), nodeid: <span class="number">1</span>, insize: <span class="number">40</span>, pid: <span class="number">13826</span></span><br></pre></td></tr></table></figure><p>从上可以看出，大概是无法通过kerberos的cache文件/tmp/krb5cc_0认证，早就意识到基于kerberos的hadoop是块难啃的骨头，用过kerberos认证的同学都知道，kerberos认证依赖本地Linux命令 kinit 实现认证，认证之后通常会有一个有效期，即在一段时间内，无需再次认证。<br><br><br>然而利用fuse_dfs挂载kerberos认证的HDFS，无论是度娘还是谷爸在这方面资料也是极少，无奈只能看下fuse_dfs的源码。</p><h4 id="fuse-dfs实现"><a href="#fuse-dfs实现" class="headerlink" title="fuse-dfs实现"></a>fuse-dfs实现</h4><p>​ fuse_dfs是由C语言开发，代码目录位于$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/（上文有提到）<br>​ 如下是fuse_dfs连接hdfs及操作相关源码文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./fuse-dfs/fuse_connect.c</span><br><span class="line">./libhdfs/hdfs.c</span><br></pre></td></tr></table></figure><h5 id="vim-fuse-connect-c"><a href="#vim-fuse-connect-c" class="headerlink" title="vim fuse_connect.c"></a>vim fuse_connect.c</h5><p>利用libhdfs建立连接<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**函数主体中一段代码表示如何通过libhdfs连接*/</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a new libhdfs connection.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param usrname       Username to use for the new connection</span></span><br><span class="line"><span class="comment"> * @param ctx           FUSE context to use for the new connection</span></span><br><span class="line"><span class="comment"> * @param out           (out param) the new libhdfs connection</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @return              0 on success; error code otherwise</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">fuseNewConnect</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *usrname, struct fuse_context *ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">        struct hdfsConn **out)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="comment">/** </span></span></span><br><span class="line"><span class="function"><span class="comment"> * Find out what type of authentication the system administrator</span></span></span><br><span class="line"><span class="function"><span class="comment"> * has configured.</span></span></span><br><span class="line"><span class="function"><span class="comment"> *</span></span></span><br><span class="line"><span class="function"><span class="comment"> * @return     the type of authentication, or AUTH_CONF_UNKNOWN on error.</span></span></span><br><span class="line"><span class="function"><span class="comment"> */</span></span></span><br><span class="line"><span class="function">  </span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">//获取Hadoop认证方式  </span></span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">enum</span> authConf <span class="title">discoverAuthConf</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> ret;</span><br><span class="line">  <span class="keyword">char</span> *val = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">enum</span> authConf authConf;</span><br><span class="line"></span><br><span class="line">  ret = hdfsConfGetStr(HADOOP_SECURITY_AUTHENTICATION, &amp;val);</span><br><span class="line">  <span class="keyword">if</span> (ret)</span><br><span class="line">    authConf = AUTH_CONF_UNKNOWN;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (!val)</span><br><span class="line">    authConf = AUTH_CONF_OTHER;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(val, <span class="string">"kerberos"</span>))</span><br><span class="line">    authConf = AUTH_CONF_KERBEROS;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    authConf = AUTH_CONF_OTHER;</span><br><span class="line">  <span class="built_in">free</span>(val);</span><br><span class="line">  <span class="keyword">return</span> authConf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取到Hadoop的认证方式之后，首先找kerberos认证后的cache文件</span></span><br><span class="line"><span class="keyword">if</span> (gHdfsAuthConf == AUTH_CONF_KERBEROS) &#123;</span><br><span class="line">    findKerbTicketCachePath(ctx, kpath, <span class="keyword">sizeof</span>(kpath));</span><br><span class="line">    <span class="keyword">if</span> (stat(kpath, &amp;st) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"fuseNewConnect: failed to find Kerberos ticket cache "</span></span><br><span class="line">        <span class="string">"file '%s'.  Did you remember to kinit for UID %d?\n"</span>,</span><br><span class="line">        kpath, ctx-&gt;uid);</span><br><span class="line">      ret = -EACCES;</span><br><span class="line">      <span class="keyword">goto</span> error;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找kerberos认证后的cache文件，从发现cache文件位于：/tmp/krb5cc_%d</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Find the Kerberos ticket cache path.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This function finds the Kerberos ticket cache path from the thread ID and</span></span><br><span class="line"><span class="comment"> * user ID of the process making the request.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Normally, the ticket cache path is in a well-known location in /tmp.</span></span><br><span class="line"><span class="comment"> * However, it's possible that the calling process could set the KRB5CCNAME</span></span><br><span class="line"><span class="comment"> * environment variable, indicating that its Kerberos ticket cache is at a</span></span><br><span class="line"><span class="comment"> * non-default location.  We try to handle this possibility by reading the</span></span><br><span class="line"><span class="comment"> * process' environment here.  This will be allowed if we have root</span></span><br><span class="line"><span class="comment"> * capabilities, or if our UID is the same as the remote process' UID.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note that we don't check to see if the cache file actually exists or not.</span></span><br><span class="line"><span class="comment"> * We're just trying to find out where it would be if it did exist. </span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param path          (out param) the path to the ticket cache file</span></span><br><span class="line"><span class="comment"> * @param pathLen       length of the path buffer</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">findKerbTicketCachePath</span><span class="params">(struct fuse_context *ctx,</span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">char</span> *path, <span class="keyword">size_t</span> pathLen)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  FILE *fp = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> * <span class="keyword">const</span> KRB5CCNAME = <span class="string">"\0KRB5CCNAME="</span>;</span><br><span class="line">  <span class="keyword">int</span> c = <span class="string">'\0'</span>, pathIdx = <span class="number">0</span>, keyIdx = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">size_t</span> KRB5CCNAME_LEN = <span class="built_in">strlen</span>(KRB5CCNAME + <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// /proc/&lt;tid&gt;/environ contains the remote process' environment.  It is</span></span><br><span class="line">  <span class="comment">// exposed to us as a series of KEY=VALUE pairs, separated by NULL bytes.</span></span><br><span class="line">  <span class="built_in">snprintf</span>(path, pathLen, <span class="string">"/proc/%d/environ"</span>, ctx-&gt;pid);</span><br><span class="line">  fp = fopen(path, <span class="string">"r"</span>);</span><br><span class="line">  <span class="keyword">if</span> (!fp)</span><br><span class="line">    <span class="keyword">goto</span> done;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (c == EOF)</span><br><span class="line">      <span class="keyword">goto</span> done;</span><br><span class="line">    <span class="keyword">if</span> (keyIdx == KRB5CCNAME_LEN) &#123;</span><br><span class="line">      <span class="keyword">if</span> (pathIdx &gt;= pathLen - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">goto</span> done;</span><br><span class="line">      <span class="keyword">if</span> (c == <span class="string">'\0'</span>)</span><br><span class="line">        <span class="keyword">goto</span> done;</span><br><span class="line">      path[pathIdx++] = c;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (KRB5CCNAME[keyIdx++] != c) &#123;</span><br><span class="line">      keyIdx = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    c = fgetc(fp);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">done:</span><br><span class="line">  <span class="keyword">if</span> (fp)</span><br><span class="line">    fclose(fp);</span><br><span class="line">  <span class="keyword">if</span> (pathIdx == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">snprintf</span>(path, pathLen, <span class="string">"/tmp/krb5cc_%d"</span>, ctx-&gt;uid); <span class="comment">// cache文件的目录</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    path[pathIdx] = <span class="string">'\0'</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>​ 由上代码可以看出，函数findKerbTicketCachePath从上下文ctx中获取uid从而获取<strong>/tmp/krb5cc_${uid}</strong>，那么如何挂载hdfs时，识别到kerberos的cache文件呢？</p><h5 id="vim-hdfs-c"><a href="#vim-hdfs-c" class="headerlink" title="vim hdfs.c"></a>vim hdfs.c</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">struct hdfsBuilder &#123;</span><br><span class="line">    int forceNewInstance;</span><br><span class="line">    const char *nn;</span><br><span class="line">    tPort port;</span><br><span class="line">    const char *kerbTicketCachePath;</span><br><span class="line">    const char *userName;</span><br><span class="line">&#125;;</span><br><span class="line">  </span><br><span class="line">#define KERBEROS_TICKET_CACHE_PATH &quot;hadoop.security.kerberos.ticket.cache.path&quot;</span><br><span class="line"></span><br><span class="line">if (bld-&gt;kerbTicketCachePath) &#123;</span><br><span class="line">            jthr = hadoopConfSetStr(env, jConfiguration,</span><br><span class="line">                KERBEROS_TICKET_CACHE_PATH, bld-&gt;kerbTicketCachePath);</span><br><span class="line">            if (jthr) &#123;</span><br><span class="line">                ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,</span><br><span class="line">                    &quot;hdfsBuilderConnect(%s)&quot;,</span><br><span class="line">                    hdfsBuilderToStr(bld, buf, sizeof(buf)));</span><br><span class="line">                goto done;</span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终定位到：KERBEROS_TICKET_CACHE_PATH “hadoop.security.kerberos.ticket.cache.path” </p><p>在core-site.xml添加如下属性：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.kerberos.ticket.cache.path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/krb5cc_$&#123;uid&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Path to the Kerberos ticket cache. <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>${uid}是什么？uid=7765</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kinit urs<span class="variable">.keytab</span> urs</span><br><span class="line">Ticket cache: FILE:/tmp/krb5cc_7765</span><br><span class="line">Default principal: urs@HADOOP<span class="variable">.HZ</span><span class="variable">.NETEASE</span><span class="variable">.COM</span></span><br><span class="line"></span><br><span class="line">Valid starting    Expires           Service principal</span><br><span class="line"><span class="number">10</span>/<span class="number">05</span>/<span class="number">2018</span> <span class="number">10</span>:<span class="number">52</span>  <span class="number">10</span>/<span class="number">05</span>/<span class="number">2018</span> <span class="number">20</span>:<span class="number">52</span>  krbtgt/HADOOP<span class="variable">.HZ</span><span class="variable">.NETEASE</span><span class="variable">.COM</span>@HADOOP<span class="variable">.HZ</span><span class="variable">.NETEASE</span><span class="variable">.COM</span></span><br><span class="line">  renew <span class="keyword">until</span> <span class="number">11</span>/<span class="number">05</span>/<span class="number">2018</span> <span class="number">10</span>:<span class="number">52</span></span><br></pre></td></tr></table></figure><p>最后重新挂载，操作手法同上。<br><br><br> <img src="http://nos.netease.com/knowledge/4bb30049-1f3b-40b2-b065-c57605ab708f" style="height:100px;width:900px"><br> <img src="http://nos.netease.com/knowledge/29f7fedd-7a00-4c51-bc38-a2615ce3d49c" style="height:350px;width:530px"><br><br><br><strong>大功告成!</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;一、介绍&quot;&gt;&lt;a href=&quot;#一、介绍&quot; class=&quot;headerlink&quot; title=&quot;一、介绍&quot;&gt;&lt;/a&gt;一、介绍&lt;/h3&gt;&lt;p&gt;将HDFS挂载到本地，允许用户像访问本地文件系统一样访问远程文件系统，用户可像读写本地文件一样读写HDFS上的文件，大大简化了HDFS使用，方法较为常用两种方法是：利用fuse_dfs或者利用NFSv3，因NFSv3需对Hadoop集群端进行操作，修改代价太大，本文介绍以fuse_dfs，只需在客户端进行配置即可挂载hdfs。&lt;br&gt;
    
    </summary>
    
      <category term="HDFS" scheme="https://deletee.github.io/categories/HDFS/"/>
    
    
      <category term="fuse" scheme="https://deletee.github.io/tags/fuse/"/>
    
      <category term="hdfs" scheme="https://deletee.github.io/tags/hdfs/"/>
    
  </entry>
  
</feed>
