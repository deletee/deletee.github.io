{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"all-categories","text":"","link":"/all-categories/index.html"},{"title":"whoami","text":"","link":"/whoami/index.html"}],"posts":[{"title":"数据仓库之数据建模","text":"1、为什么需要数据建模？理解关键字：数据建模就是数据组织和存储的方法数据模型是数据组织和存储方法，正如我们希望图书馆的书分门别类的放置。数据模型强调从业务、数据存取和使用角度合理规划数据 那么为什么需要数据建模呢？我们可以从建模有哪些好处理解（1）性能 良好的模型能帮助我们快速查询所需要的数据，减少不必要的数据查询I/O（2）成本 良好的数据模型能够极大地减少不必要的数据存储，也能实现计算结果的复用，极大地降低大数据系统中的存储和计算成本（3）效率 良好的数据模型能够极大地提高用户体验，用户可通过业务沉淀后的模型尽可能减少不必要的查询，从而提高效率（4）质量 良好的数据模型将遵循一定的数据规范（例如：口径约束，字段约束等）从而减少数据计算的失误，保障数据质量 2、OLTP与OLAP的区别理解关键字： OLTP 是讲究事务，在OLAP中事务不是所关注的，主要是批量的读写，致力于联机分析 一个是讲究快速响应，一个是讲究大吞吐OLTP通常使用ER模型，采用三范式消除数据中的冗余，而OLAP采用的模型则比较丰富 3、有哪些建模方法论？（1）ER建模（2）维度建模（3）Data vault建模（4）Anchor建模","link":"/2018/12/29/data-modeling/"},{"title":"利用fuse_dfs快速建立hdfs云存储","text":"一、介绍将HDFS挂载到本地，允许用户像访问本地文件系统一样访问远程文件系统，用户可像读写本地文件一样读写HDFS上的文件，大大简化了HDFS使用，方法较为常用两种方法是：利用fuse_dfs或者利用NFSv3，因NFSv3需对Hadoop集群端进行操作，修改代价太大，本文介绍以fuse_dfs，只需在客户端进行配置即可挂载hdfs。用户空间文件系统（Filesystem in Userspace，简称FUSE）是一个面向类Unix计算机操作系统的软件接口，它使无特权的用户能够无需编辑内核代码而创建自己的文件系统。目前Linux通过内核模块对此进行支持。一些文件系统如ZFS、glusterfs和lustre使用FUSE实现。Linux用于支持用户空间文件系统的内核模块名叫FUSE，FUSE一词有时特指Linux下的用户空间文件系统。通过使用Hadoop的Fuse-DFS分类模块，任意一个Hadoop文件系统(不过一般为HDFS)都可以作为一个标准文件系统进行挂载。Hadooop源码中自带了fuse-dfs模块，利用fuse_dfs将hdfs当一个网盘挂载到本地。 二、应用场景同步：实现用户对个人计算指定目录下的特定类型的文件自动与HDFS同步，实时监测用户文件变化情况。不需要用户干预就能及时自动备份文件。当用户新建、修改或删除文件（目录）时自动更新同步，保障用户重要文件的安全。当用户移动办公时，可直接从HDFS访问，省略了移动存储介质的拷贝过程，提高用户工作效率。更换新计算机时，用户登录后原计算机中的个人文件将自动从HDFS存储中同步到新计算机中。例如：适用数据仓库需要ETL进行Hadoop文件进行ftp或者rsync推送的场景。 网盘：用户可将不常用的文件存储在HDFS存储中，从而节约本地存储空间，提高文件的物理安全性，也便于资源共享。 共享：用户可以把网盘或同步盘的任意文件（目录）分享给群组或其他用户。只需要在云存储中存储一份文件，被分享的人不需要下载存储即可在线浏览，实现资源共享。若源文件被修改，所有分享人立即会得到更新文件。 三、部署实现目前网络上关于利用fuse挂载hdfs较多基于较早版本的hadoop(0.x，1.x) ，并大多都基于无kerberos认证，本文以北京亦庄Hadoop 2.5.2为例，介绍通过fuse挂载hdfs到本地的方法。设定源码目录为$HADOOP_SRC_HOME​，那么Hadoop 2.x 的 fuse_dfs 源码位于1$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/ 那么怎么编译fuse_dfs？fuse_dfs是Hadoop自带模块，编译Hadoop时，增加编译参数require.fuse=true，即编译命令如下1mvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar 编译环境123456789Linux: 3.16.0-0.bpo.4-amd64 (Debian7)JDK: 1.7.0_80 64bitApache Hadoop: 2.5.2 Ant: 1.8.2GCC: 4.7.2 (系统默认)Protocbuf 2.5.0Maven 3.0.4Cmake 2.8.9Make 3.8.1 安装fuse等依赖1sudo apt-get -y install maven build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev libfuse-dev 编译fuse_dfs(Hadoop)123456789101112# 下载mkdir package &amp;&amp; cd packagewget http://archive.apache.org/dist/hadoop/core/hadoop-2.5.2/hadoop-2.5.2-src.tar.gz#解压tar -zxvf hadoop-2.5.2-src.tar.gzln -s hadoop-2.5.2-src hadoop-srcecho \"export HADOOP_SRC_HOME=$HOME/package/hadoop-src/\" &gt;&gt; .bashrc &amp;&amp; source .bashrc# 编译cd hadoop-2.5.2-srcmvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar PS: 编译Hadoop之前需提前安装好fuse，否则fuse_dfs无法编译成功123456- [1] 编译后的fuse_dfs位于： $HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs/fuse_dfs- [2] 完整的Hadoop包则位于： $HADOOP_SRC_HOME/hadoop-dist/target/hadoop-2.5.2.tar.gz- [3] 一个用于挂载hdfs的脚本： $HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh 部署Hadoop（略）配置fuse_dfs_wrapper.shHDFS挂载主要依赖源码的两个模块，一个是hadoop-hdfs-project，另一个是hadoop-client，可将依赖的jar包单独提炼出来。 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env bashexport HADOOP_HOME=${HADOOP_HOME:-/usr/local/share/hadoop}export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoopexport HADOOP_PREFIX=$HOME/package/hadoop-srcexport JAVA_HOME=$HOME/opt/jdkexport FUSEDFS_PATH=\"$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/main/native/fuse-dfs\"export LIBHDFS_PATH=\"$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/target/usr/local/lib\"if [ \"$OS_ARCH\" = \"\" ]; then export OS_ARCH=amd64fiif [ \"$JAVA_HOME\" = \"\" ]; then export JAVA_HOME=/usr/local/javafiif [ \"$LD_LIBRARY_PATH\" = \"\" ]; thenexport LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/usr/local/libfiJARS=`find \"$HADOOP_PREFIX/hadoop-hdfs-project\" -name \"*.jar\" | xargs`for jar in $JARS; do CLASSPATH=$jar:$CLASSPATHdoneJARS=`find \"$HADOOP_PREFIX/hadoop-client\" -name \"*.jar\" | xargs`for jar in $JARS; do CLASSPATH=$jar:$CLASSPATHdoneexport CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATHexport PATH=$FUSEDFS_PATH:$PATHexport LD_LIBRARY_PATH=$LIBHDFS_PATH:$JAVA_HOME/jre/lib/$OS_ARCH/server:$LD_LIBRARY_PATHfuse_dfs $@ 利用命令 1234567891011121314 sudo bash ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://${nn}:${port} /mnt/hdfsINFO /home/hzxijingjing/package/hadoop-2.5.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c:164 Adding FUSE arg /mnt/hdfs# 挂载信息 df -hFilesystem Size Used Avail Use% Mounted onrootfs 50G 8.2G 39G 18% /udev 10M 0 10M 0% /devtmpfs 1.6G 216K 1.6G 1% /run/dev/vda1 50G 8.2G 39G 18% /tmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 3.2G 0 3.2G 0% /run/shm/dev/vdb1 99G 75M 94G 1% /datafuse_dfs 49G 0 49G 0% /mnt/hdfs # HDFS挂载成功 ​ 至此为止，已实现无简单认证的Hadoop挂载，然而对于配置了kerberos认证的Hadoop，实施挂载会出现如下情况：123cd /mnt &amp;&amp; ls -lhls: cannot access hdfs: Transport endpoint is not connectedd????????? ? ? ? ? ? hdfs 后置参数 -d，进入debug模式， 查看运行信息 1sudo /bin/sh ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://${nn}:${port} /mnt/dfs -d 1234567891011121314151617181920212223242526272829............ # 省略一些无用日志 FUSE library version: 2.9.0nullpath_ok: 0nopath: 0utime_omit_ok: 0unique: 1, opcode: INIT (26), nodeid: 0, insize: 56, pid: 0INIT: 7.23flags=0x0003fffbmax_readahead=0x00020000INFO /home/hzxijingjing/package/hadoop-2.5.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c:98 Mounting with options: [ protected=(NULL), nn_uri=hdfs://hadoop1232.hz.163.org:8020, nn_port=0, debug=0, read_only=0, initchecks=0, no_permissions=0, usetrash=1, entry_timeout=60, attribute_timeout=60, rdbuffer_size=10485760, direct_io=0 ]............ # 省略一些无用日志 fuseConnectInit: initialized with timer period 5, expiry period 300 INIT: 7.18 flags=0x00000039 max_readahead=0x00020000 max_write=0x00020000 max_background=0 congestion_threshold=0 unique: 1, success, outsize: 40unique: 2, opcode: STATFS (17), nodeid: 1, insize: 40, pid: 13769statfs / unique: 2, success, outsize: 96unique: 3, opcode: GETATTR (3), nodeid: 1, insize: 56, pid: 13815getattr /fuseNewConnect: failed to find Kerberos ticket cache file '/tmp/krb5cc_0'. Did you remember to kinit for UID 0?fuseConnect(usrname=root): fuseNewConnect failed with error code -13fuseConnectAsThreadUid: failed to open a libhdfs connection! error -13. unique: 3, error: -5 (Input/output error), outsize: 16unique: 4, opcode: STATFS (17), nodeid: 1, insize: 40, pid: 13826 从上可以看出，大概是无法通过kerberos的cache文件/tmp/krb5cc_0认证，早就意识到基于kerberos的hadoop是块难啃的骨头，用过kerberos认证的同学都知道，kerberos认证依赖本地Linux命令 kinit 实现认证，认证之后通常会有一个有效期，即在一段时间内，无需再次认证。然而利用fuse_dfs挂载kerberos认证的HDFS，无论是度娘还是谷爸在这方面资料也是极少，无奈只能看下fuse_dfs的源码。 fuse-dfs实现​ fuse_dfs是由C语言开发，代码目录位于$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/（上文有提到）​ 如下是fuse_dfs连接hdfs及操作相关源码文件 12./fuse-dfs/fuse_connect.c./libhdfs/hdfs.c vim fuse_connect.c利用libhdfs建立连接123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111 /**函数主体中一段代码表示如何通过libhdfs连接*//** * Create a new libhdfs connection. * * @param usrname Username to use for the new connection * @param ctx FUSE context to use for the new connection * @param out (out param) the new libhdfs connection * * @return 0 on success; error code otherwise */static int fuseNewConnect(const char *usrname, struct fuse_context *ctx, struct hdfsConn **out) /** * Find out what type of authentication the system administrator * has configured. * * @return the type of authentication, or AUTH_CONF_UNKNOWN on error. */ //获取Hadoop认证方式 static enum authConf discoverAuthConf(void){ int ret; char *val = NULL; enum authConf authConf; ret = hdfsConfGetStr(HADOOP_SECURITY_AUTHENTICATION, &amp;val); if (ret) authConf = AUTH_CONF_UNKNOWN; else if (!val) authConf = AUTH_CONF_OTHER; else if (!strcmp(val, \"kerberos\")) authConf = AUTH_CONF_KERBEROS; else authConf = AUTH_CONF_OTHER; free(val); return authConf;}// 获取到Hadoop的认证方式之后，首先找kerberos认证后的cache文件if (gHdfsAuthConf == AUTH_CONF_KERBEROS) { findKerbTicketCachePath(ctx, kpath, sizeof(kpath)); if (stat(kpath, &amp;st) &lt; 0) { fprintf(stderr, \"fuseNewConnect: failed to find Kerberos ticket cache \" \"file '%s'. Did you remember to kinit for UID %d?\\n\", kpath, ctx-&gt;uid); ret = -EACCES; goto error; }// 查找kerberos认证后的cache文件，从发现cache文件位于：/tmp/krb5cc_%d/** * Find the Kerberos ticket cache path. * * This function finds the Kerberos ticket cache path from the thread ID and * user ID of the process making the request. * * Normally, the ticket cache path is in a well-known location in /tmp. * However, it's possible that the calling process could set the KRB5CCNAME * environment variable, indicating that its Kerberos ticket cache is at a * non-default location. We try to handle this possibility by reading the * process' environment here. This will be allowed if we have root * capabilities, or if our UID is the same as the remote process' UID. * * Note that we don't check to see if the cache file actually exists or not. * We're just trying to find out where it would be if it did exist. * * @param path (out param) the path to the ticket cache file * @param pathLen length of the path buffer */static void findKerbTicketCachePath(struct fuse_context *ctx, char *path, size_t pathLen){ FILE *fp = NULL; static const char * const KRB5CCNAME = \"\\0KRB5CCNAME=\"; int c = '\\0', pathIdx = 0, keyIdx = 0; size_t KRB5CCNAME_LEN = strlen(KRB5CCNAME + 1) + 1; // /proc/&lt;tid&gt;/environ contains the remote process' environment. It is // exposed to us as a series of KEY=VALUE pairs, separated by NULL bytes. snprintf(path, pathLen, \"/proc/%d/environ\", ctx-&gt;pid); fp = fopen(path, \"r\"); if (!fp) goto done; while (1) { if (c == EOF) goto done; if (keyIdx == KRB5CCNAME_LEN) { if (pathIdx &gt;= pathLen - 1) goto done; if (c == '\\0') goto done; path[pathIdx++] = c; } else if (KRB5CCNAME[keyIdx++] != c) { keyIdx = 0; } c = fgetc(fp); }done: if (fp) fclose(fp); if (pathIdx == 0) { snprintf(path, pathLen, \"/tmp/krb5cc_%d\", ctx-&gt;uid); // cache文件的目录 } else { path[pathIdx] = '\\0'; }} ​ 由上代码可以看出，函数findKerbTicketCachePath从上下文ctx中获取uid从而获取/tmp/krb5cc_${uid}，那么如何挂载hdfs时，识别到kerberos的cache文件呢？ vim hdfs.c123456789101112131415161718192021struct hdfsBuilder { int forceNewInstance; const char *nn; tPort port; const char *kerbTicketCachePath; const char *userName;}; #define KERBEROS_TICKET_CACHE_PATH &quot;hadoop.security.kerberos.ticket.cache.path&quot;if (bld-&gt;kerbTicketCachePath) { jthr = hadoopConfSetStr(env, jConfiguration, KERBEROS_TICKET_CACHE_PATH, bld-&gt;kerbTicketCachePath); if (jthr) { ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL, &quot;hdfsBuilderConnect(%s)&quot;, hdfsBuilderToStr(bld, buf, sizeof(buf))); goto done; }} 最终定位到：KERBEROS_TICKET_CACHE_PATH “hadoop.security.kerberos.ticket.cache.path” 在core-site.xml添加如下属性： 12345&lt;property&gt; &lt;name&gt;hadoop.security.kerberos.ticket.cache.path&lt;/name&gt; &lt;value&gt;/tmp/krb5cc_${uid}&lt;/value&gt; &lt;description&gt;Path to the Kerberos ticket cache. &lt;/description&gt;&lt;/property&gt; ${uid}是什么？uid=7765 1234567kinit urs.keytab ursTicket cache: FILE:/tmp/krb5cc_7765Default principal: urs@HADOOP.HZ.NETEASE.COMValid starting Expires Service principal10/05/2018 10:52 10/05/2018 20:52 krbtgt/HADOOP.HZ.NETEASE.COM@HADOOP.HZ.NETEASE.COM renew until 11/05/2018 10:52 最后重新挂载，操作手法同上。 大功告成!","link":"/2018/08/16/fuse_hdfs/"},{"title":"如何使用MarkDown","text":"如何使用MarkDown 我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文（LaTeX 公式支持） 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1. 制作一份待办事宜 Todo 列表 支持以 PDF 格式导出文稿 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 新增 Todo 列表功能 修复 LaTex 公式渲染问题 新增 LaTex 公式编号功能 2. 书写一个质能守恒公式[^LaTeX]$$E=mc^2$$ 3. 高亮一段代码[^code]12345678&gt; @requires_authorization&gt; class SomeClass:&gt; pass&gt; &gt; if __name__ == '__main__':&gt; # A comment&gt; print 'hello world'&gt; 4. 高效绘制 流程图 123456789&gt; st=&gt;start: Start&gt; op=&gt;operation: Your Operation&gt; cond=&gt;condition: Yes or No?&gt; e=&gt;end&gt; &gt; st-&gt;op-&gt;cond&gt; cond(yes)-&gt;e&gt; cond(no)-&gt;op&gt; 5. 高效绘制 序列图 1234&gt; Alice-&gt;Bob: Hello Bob, how are you?&gt; Note right of Bob: Bob thinks&gt; Bob--&gt;Alice: I am good thanks!&gt; 6. 高效绘制 甘特图 1234567891011121314&gt; title 项目开发流程&gt; section 项目确定&gt; 需求分析 :a1, 2016-06-22, 3d&gt; 可行性报告 :after a1, 5d&gt; 概念验证 : 5d&gt; section 项目实施&gt; 概要设计 :2016-07-05 , 5d&gt; 详细设计 :2016-07-08, 10d&gt; 编码 :2016-07-15, 10d&gt; 测试 :2016-07-22, 5d&gt; section 发布验收&gt; 发布: 2d&gt; 验收: 3d&gt; 7. 绘制表格 项目 价格 数量 计算机 \\$1600 5 手机 \\$12 12 管线 \\$1 234 8. 更详细语法说明 想要查看更详细的语法说明，可以参考我们准备的 [Cmd Markdown 简明语法手册][1]，进阶用户可以参考 [Cmd Markdown 高阶语法手册][2] 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 什么是 Cmd Markdown 您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1. 实时同步预览 我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 2. 编辑工具栏 也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 3. 编辑模式 完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 4. 实时的云端文稿 为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 5. 离线模式 在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 6. 管理工具栏 为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 8. 阅读模式 在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 9. 标签、分类和搜索 在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 10. 文稿发布和分享 在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ 作者 [@ghosert][3] 2016 年 07月 07日 [^1]: 支持 LaTeX 编辑显示支持，例如：$\\sum_{i=1}^n a_i=0$， 访问 [MathJax][4] 参考更多使用方法。 [^2]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。","link":"/2018/12/31/how-to-use-md/"}],"tags":[{"name":"建模","slug":"建模","link":"/tags/建模/"}],"categories":[]}