<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据仓库-50问]]></title>
    <url>%2F2019%2F11%2F23%2Fedw-50FAQ%2F</url>
    <content type="text"><![CDATA[​ 很长一段时间没有写博客了，由于一些想法暗暗发芽，一直以来工作上的事情太多，也将很多学习内容搁置，今天新增一篇博客，用于]]></content>
      <categories>
        <category>3x 职场篇</category>
        <category>3x2 面试宝典</category>
      </categories>
      <tags>
        <tag>职场</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F28%2F%E5%B8%AD%E8%80%81%E5%B8%88%E7%9A%84Mac%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[席老师的Mac使用技巧标签（空格分隔）： Mac 软件 技巧 系统篇 教程/ 系统偏好设置 -&gt; 触摸板 推荐/ 轻点代替点按, 三指取词 App 篇]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F24%2Faiops%2F</url>
    <content type="text"><![CDATA[从自动化到智能化，OP&amp;URS 在AIOps探索与实战一、运维面临问题与挑战​ 眼下，随着信息化、数字化的深入发展，技术飞速迭代，应用服务也不断升级，企业面临的运维压力也越来越大，传统运维受到了前所未有的挑战。 ​ （1）运维内容：传统的互联网运维的内容仅是关注软硬件、网络、应用系统及基础设备的运维，而当前将面临数十万台主机、容器，复杂的网络环境，以及复杂的部署环境：私有云、公有云、跨IDC混合部署 ​ （2）运维工具：传统的互联网运维尽管也利用了工具实现了部分工作的自动化，但主要依赖人力，工作量较大，并效率低下，业务快速增长，技术飞速迭代，意味着工具也要顺势升级。 ​ （3）运维模式：7*24小时服务模式，PE\SA\DBA 成为了“救火式”英雄，成千上万的监控指标，一旦故障出现，SA、PE、DBA、开发童鞋齐上阵，被故障牵着走，被动性强且风险高。 ​ ​ OP&amp;URS作为网易运维服务团队，面对新的挑战，这就要求我们打造信息化、数字化的综合管理体系，为企业带来全方位IT运维服务的同时，又可提供定制化、专业化、全链路、无死角的运维解决方案。在大数据时代下，借助机器学习、数据仓库、大数据平台等大数据技术手段，将运维产生的数据进行分析、处理，得出最佳运维策略，以期实现对故障的事先干预，将风险降低到最低，从而降低运维成本，提升运维效率，最终实现运维智能化。 二、AIOps 现状、定位以及我们的理解​ AIOps即智能运维，是 Gartner 在2016年提出的概念，真正火起来是在这两年，这个概念提出后，各大厂都已经先后利用AIOps理念培养智能运维人才梯队，建设智能运维平台、打造智能运维体系。腾讯在2018年推出织云平台（Metis已开源）、百度Noah智能运维平台面向商用，Gartner预测到2020年，将近50%的企业将会在他们的业务和 IT 运维方面采用 AIOps 。高效运维发起人萧田国在《AIOps实施之路》中指出了AIOps在效率提升、质量保障、成本优化提出了系列可应用方向以及实施AIOps需要具备的能力。 ​ OP&amp;URS 于2018年加入智能运维大营，拥抱变化，以实现全链路、无死角智能运维体系为目标，旨在利用AI的能力解决运维行业的问题（1）解决重复造轮子的问题（2）解决运维效率仍然低下的问题（3）运维的数据没有得到合理应用的问题。 ​ ​ 此图为故障管理全场景图，该图从服务部署、故障发生、故障发现、故障止损、根因诊断、故障恢复、故障关闭，完整的阐述应用监控的故障管理生命周期。 ​ OP&amp;URS 实施中的智能运维产品形态 ​ 故障预警：通过算法计算KPI曲线变化趋势，故障前发出故障预警；​ 故障告警：能对周期性变化指标进行预测和异常检测，且有告警分级；​ 告警合并：支持按照合适的维度对告警进行合并，展现概况信息；​ 根因分析：智能对故障根因进行分析，给出最可能的原因，辅助人做决策；​ 故障自愈：可以根据故障原因选择合适的故障自愈策略并执行，自动解决故障。 ​ OP&amp;URS 运维产品生态 三、OP&amp;URS 在AIOps中实战场景-智能监控​ 笔者自接触智能运维以来，也是三千烦劳丝，如何让运维“智能”起来？如何让AIOps结合网易现有运维体系实施落地? 又如何推进AIOps发展? 种种挑战考验着我们的团队。经过1年来不断探索、研究、试错，我们首先在监控方面突破，下面介绍如何从0到1建设AIOps应用-智能监控系统心路历程。 3.1 运维监控现状​ 随着互联网，特别是移动互联网的高速发展，web服务已经深入到社会的各个领域，人们使用互联网搜索，购物，付款，娱乐等等。因此，保障web服务的稳定已经变的越来越重要。运维人员通过监控各种各样的关键性能指标（KPI）来判断服务、系统是否稳定，因为KPI如果发生异常，往往意味着与其相关的应用发生了问题。这些KPI可能包括：基础KPI及服务KPI，服务KPI是指能够反映Web服务的规模、质量的性能指标，例如，网页响应时间，网页访问量，连接错误数量等。基础KPI是指能够反映机器（服务器、路由器、交换机）健康状态的性能指标，例如，磁盘使用率，CPU使用率，内存使用率，磁盘IO，网卡吞吐率等。 ​ 这些KPI数据表现为时序序列，即一条指标曲线（后文统一称KPI曲线）。由此问题转化为对曲线的异常判断，KPI曲线可以简单分类下面三种类型： ​ 周期型： ​ 随机型： ​ 平稳型： ​ 在OP&amp;URS基于基础设施管理的CMDB系统：哨兵系统，通过哨兵 Agent将数据实时/采样的传输至哨兵服务端进行可视化及报警监控。监控系统主要采用规则判定的报警方式，设定上限、下限阈值，触发规则则发出报警，随着业务集成越来越多，体量也越来越大，规则报警也到了其瓶颈，主要有以下痛点： ​ （1）需要频繁调整阈值 ​ case1:随着业务变化，已有的阈值适用性变差，当业务发生变动时，报警规则也需要及时调整 case2:夜间与白天范围不一样，工作日与周末不一样，统一的阈值适用性较差 ​ （2）覆盖范围有限 ​ 传统的方式，需要针对指标的每一项进行设定报警规则，比如在DubboProviderCollector，每个方法对应的调用集群的量不一，需要独立配置报警规则，那么配置将会相当耗时且繁琐，并且很多Dubbo服务接口都是随业务随时新增或下线，很容易被忽视。 ​ （3）无效报警过多 ​ 阈值规则报警的方式，往往会出现这样的情况，当阈值设定的太高，异常很难被发现，当阈值设定的太低，则会造成大量报警，造成报警风暴，真正有用的报警消息淹没在风暴中。 3.2 算法引入通用异常检测流程： ​ ​ 针对上述痛点，我们思考如何利用算法来突破传统的阈值报警局限性。于是调研了业界使用的各种异常检测算法，较为常见的算法包括逻辑回归、关联关系挖掘、聚类、决策树、随机森林、支持向量机、蒙特卡洛树搜索、隐式马尔科夫、多示例学习、迁移学习、卷积神经网络等 ,数学算法类：k-sigma，Grubbs，Turkey，MeanPercent，Value，AR，MR，ARIMA。 ​ 曾想使用一种或一类算法来解决所有KPI曲线的预测，而碰到业务情况远比我们想象要复杂，例如：首先面临各种不同曲线表现特征不一，同一类型的算法很难做到召回率整体提升；其二，在同样类型dubbo调用异常 KPI曲线波动情况，在URS是可以接受的，但是在其他产品可能是不能接受的异常，可能在URS在意的指标，在其他产品无需在意；第三，尽管想做到一个模型泛华兼容所有场景，但是所需特征工程工作量巨大，特征也很多。 ​ 有人说采用投票的方法，用一大堆算法同时预测，对于结果进行投票，少数服从多数，对于这种方式也是存在一定的缺陷，本身每个算法适用性不一样，那么势必在影响投票结果。在OP&amp;URS采用的是分类算法，即在不同的场景下采用一类算法进行预测，以减少误判率，下面调研和使用了上述部分算法： ​ 机器学习类： 算法 优点 缺点 rnn 适合序列变化 存在梯度消失现象 lstm 解决RNN梯度消息问题 单指标、单模型 dnn 单模型覆盖所有场景，泛华能力比rf更优 特征工程复杂，需要大量标注数据 xgboost 考虑训练数据为稀疏值，分布式并行 特征工程复杂，需要大量标注数据 rf 支持高维度，不用做特征选择、模型泛化能力强 某些噪音较大的分类或回归问题上会过拟合 ​ 特征工程是机器学习中一块重要的环节，针对单一KPI表现的数据形态将逐一转换为数据特征，如下将数据特征归类如下5个方面 ​ 1、统计特征 ：描述样本内相关的数学表现，例如：方差、均值、中位数、斜率、偏度、峰度等重要指标 ​ 2、拟合特征 ：获取曲线的动态特征，根据曲线平稳或不平稳，采用不同模型获取预测值与实际值的差 ​ 3、周期特征：利用滑动窗口，傅里叶转换，获取曲线中可能存在的季节性、周期性特征 ​ 4、分类特征：基于曲线变换、小波变换、主成分分析等方式 获取曲线分类特征 ​ 5、业务特征：KPI具有业务集群效应，工作日邮箱访问量，周末游戏访问量等业务特征 ​ PS：由于篇幅有限，这里就不枚举所有特征，有兴趣的同学可以私下沟通交流。 ​ 数学算法类： ​ （1）恒定阈值类算法​ 恒定阈值的含义是表示均值基本恒定，标准差与均值比约等于0（即KPI曲线近似一条直线） ​ （2）突升突降类算法 ​ 突变的含义是发生了均值漂移 ​ 空间转换:𝑟(𝑡)= ​ （3）同比算法​ 适用于周期性数据表现，每天同时刻的数据分布相似​ 参数估计:求正态分布的均值、方差 3.3 功能设计（1）KPI 管理 标注打标：提供标注打标的功能，标记/取消标记为正负样本，标记后样本自动转存样本库 样本管理：提供样本管理功能，检索、图示、编辑、删除等功能 异常查询：经API检测后的时间序列（仅异常）入库存储，提供管理功能，分页查询、检索、放缩等 （2）模型管理​ 模型管理：提供模型管理功能​ 模型训练：支持自定义模型训练 （通过PE或开发标注的异常KPI，正常KPI训练符合自己业务的模型，或使用开放的通用模型） （3）KPI异常检测 基于数学统计算法集成 基于机器学习算法集成 （4）多维度报警聚合 按产品维度合并 按应用维度合并 按集群维度合并 按单机维度合并 按业务类型合并 按报警接收人维度合并 （5）反馈系统 用户标记、报警关闭 3.4 架构设计​ 我们将整个智能监控系统分为7个核心功能模块，每个模块承担异常检测流程中相应功能，整体架构上做到模块之间相互独立，通过数据流信号、RPC进行模块通信。 序号 模块名称 1 配置管理中心 配置库、模型库、标注库 相关配置与管理（含UI） 2 流式计算中心 KPI预处理、KPI聚合、KPI分发 3 数据存储&amp;读取中心 用于读写KPI时序数据 4 消息中心 数据→ 消息 用于触发模型计算、插值计算 5 算法中心 异常检测、模型训练 6 报警处理中心 用于解析异常KPI及发送报警内容至哨兵报警中心 7 反馈系统 用于反馈报警是否有效，反馈于模型提升 ​ 系统架构如下： ​ 数据架构如下： 3.5 难题​ 前面介绍了OP&amp;URS 智能监控系统的在智能监控系统中整体功能及架构设计，下面简单聊聊我们碰到一些问题以及解决思路： （1）实时计算问题 ​ 问题描述：1期采用的是Spark Streaming实时计算框架，通过直连Kafka获取数据源，但是在实际生产中，由于数据源是采样收集（有1分钟采样，有2分钟采样），发现每1分钟会有流量尖刺现象，随着KPI数量增多，尖刺更明显。 ​ 分析：经分析，发现Spark Streaming 并非是实时的数据抽取Kafka的数据。然而在Spark Streaming在实时计算中不能支持实时读取数据，而是在窗口结束时一次性抽取，造成了一次抽取大量数据，造成网络波动异常，流量尖刺。 ​ 解决方案：由于Spark流式计算本质上是微批处理，尽管使用各种手段（限制抽取数据大小、减少窗口大小），仍然指标不治本，由此尝试找Spark的替代方案，我们调研了Flink，经测试Flink能够完美的解决实时抽取的问题，并在数据延时方面处理有一项不到的效果，显著提高了数据准确度。 （2）性能问题 ​ 问题描述：系统在1期时，为了实时计算历史特征（周期性，同比等），算法计算的输入是360个数据点（当前时间区间的120个点，前一天同区间的120个点，7天前同区间的120个点），历史数据存储在HBase（作为TSDB存在）中，但是随着KPI数量增长（当前已有10万级），三个时间段分布在不同的HBase Key（Key 设计：keyId + DayHour， Column设计：minute）中，意味着每次计算需要查询3次HBase，当KPI到达20万级时，需要查询60万次，由于前期采用的还是Spark Steaming模式计算，QPS超过20w，HBase出现明显延迟。 ​ 分析：由于算法所需数据点甚大，实时计算抽取历史数据成为瓶颈，团队的攻城狮们自然想到的是减少算法数据需求，最好是用实时的1个数据点进行计算，但对于算法来说是完全不可能实现的，其一，一个点的输入完全构建不了任何特征，算法无从获知曲线形态，不知道同环比等，只会退化到传统的阈值计算；其二，1期所有特征工程基于360个计算，计算点过度减少相当于1期的算法工作全部推翻重做。经过大量讨论论证，得出如下解决方案： ​ 解决方案： ​ （1）算法输入精简，由360个数据点更改为当前时间区间的60个点，部分实时特征转为离线计算特征，通过内存加载，无需实时进行计算，既保留当前曲线的数据特征，又能减少实时计算模块的需求。 ​ （2）去HBase改为Redis，将60个缓存至Redis中，采用pipeline方法，进行头尾操作（新数据点插入头部，旧的数据点从尾部剔除） ​ AIOps前行的路上荆棘丛生，类似的难题还有很多，很多方法也不一定是最优方案，好在大家愿意尝试，愿意推翻重来。 四、总结与展望​ OP&amp;URS 智能监控系统经历了2期的开发，当前智能监控系统2.0已上线，算法召回率在70%左右，报警覆盖度100%，报警配置成本节省90%，当前传媒、考拉、URS、云阅读、网易金融等产品先后接入智能监控报警（排名不分先后），也多次及时的探测到异常事故及时止损，为客户减少损失。 ​ 当前系统也存在很多不足： ​ （1）算法召回率不高，仍需要专家经验与算法的结合 ​ （2）当前智能监控系统只能基于单KPI计算，不满足多KPI计算 ​ （3）依赖数据源的丰富，缺乏全链路监控 ​ 智能监控是AIOps中冰山一角，除了解决当前不足以外，我们还有很多工作要做，例如根因分析，场景监控，故障诊断、故障自愈等等，AIOps之路任重而道远，期待广大志同道合的operator加入AIOps阵营，一起努力。 ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flink学习笔记02->Flink配置文件详解]]></title>
    <url>%2F2019%2F08%2F11%2Fflink-learning-notes-02%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前文已经简要的介绍了flink的如何在Mac下运行一个Maven程序，本文主要聊下在Flink conf目录下的各配置的文件以及文件中各种配置的含义，更多更详细的配置信息请点击：传送门 ，下面进入正题… 0x1 配置目录&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文中已经定义了FLINK_HOME=/usr/local/Cellar/apache-flink/1.6.0/ 123456789101112131415λ deletee [~] → cd $FLINK_HOME/libexec/confλ deletee [1.6.0/libexec/conf] ls -lhtotal 112-rw-r--r-- 1 deletee admin 9.6K Aug 6 2018 flink-conf.yaml-rw-r--r-- 1 deletee admin 2.1K Aug 6 2018 log4j-cli.properties-rw-r--r-- 1 deletee admin 1.8K Aug 6 2018 log4j-console.properties-rw-r--r-- 1 deletee admin 1.7K Aug 6 2018 log4j-yarn-session.properties-rw-r--r-- 1 deletee admin 1.9K Aug 6 2018 log4j.properties-rw-r--r-- 1 deletee admin 2.2K Aug 6 2018 logback-console.xml-rw-r--r-- 1 deletee admin 1.5K Aug 6 2018 logback-yarn.xml-rw-r--r-- 1 deletee admin 2.3K Aug 6 2018 logback.xml-rw-r--r-- 1 deletee admin 15B Aug 6 2018 masters-rw-r--r-- 1 deletee admin 10B Aug 6 2018 slaves-rw-r--r-- 1 deletee admin 3.2K Aug 6 2018 sql-client-defaults.yaml-rw-r--r-- 1 deletee admin 1.4K Aug 6 2018 zoo.cfg &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;配置目录中有flink-conf.yaml（flink的核心配置文件），log4j*.properties 系列log4j文件以及logback*.xml系列日志配置文件，masters、slave用于flink分布式配置，sql-client-defaults.yaml 是Flink SQL 客户端配置，zoo.cfg 是 flink所依赖的 zookeeper的配置文件 0x2 flink-conf.yaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;flink-conf文件格式为yaml即flink的配置需要遵循yaml的文件格式，下面主要从基础配合，内存管理，内存与性能debug等进行展开。 — 基础配置 123456789101112131415161718192021222324env.java.home: # java安装路径，如果没有指定则默认使用系统的$JAVA_HOME环境变量。建议设置此值，因为之前我曾经在standalone模式中启动flink集群，报找不到JAVA_HOME的错误。config.sh中（Please specify JAVA_HOME. Either in Flink config ./conf/flink-conf.yaml or as system-wide JAVA_HOME.）env.java.opts: # 定制JVM选项，在Flink启动脚本中执行。需要单独执行JobManager和TaskManager的选项。env.java.opts.jobmanager: # 执行jobManager的JVM选项。在Yarn Client环境下此参数无效。env.java.opts.taskmanager:# 执行taskManager的JVM选项。在Yarn Client环境下此参数无效。jobmanager.rpc.address:# Jobmanager的IP地址，即master地址。默认是localhost，此参数在HA环境下或者Yarn下无效，仅在local和无HA的standalone集群中有效。jobmanager.rpc.port: # JobMamanger的端口，默认是6123。jobmanager.heap.mb: # JobManager的堆大小（单位是MB）。当长时间运行operator非常多的程序时，需要增加此值。具体设置多少只能通过测试不断调整。taskmanager.heap.mb:# 每一个TaskManager的堆大小（单位是MB），由于每个taskmanager要运行operator的各种函数（Map、Reduce、CoGroup等，包含sorting、hashing、caching），因此这个值应该尽可能的大。如果集群仅仅跑Flink的程序，建议此值等于机器的内存大小减去1、2G，剩余的1、2GB用于操作系统。如果是Yarn模式，这个值通过指定tm参数来分配给container，同样要减去操作系统可以容忍的大小（1、2GB）。taskmanager.numberOfTaskSlots:# 每个TaskManager的并行度。一个slot对应一个core，默认值是1.一个并行度对应一个线程。总的内存大小要且分给不同的线程使用。parallelism.default:# 每个operator的默认并行度。默认是1.如果程序中对operator设置了setParallelism，或者提交程序时指定了-p参数，则会覆盖此参数。如果只有一个Job运行时，此值可以设置为taskManager的数量 * 每个taskManager的slots数量。即NumTaskManagers * NumSlotsPerTaskManager 。fs.default-scheme:# 设置默认的文件系统模式。默认值是file:///即本地文件系统根目录。如果指定了hdfs://localhost:9000/，则程序中指定的文件/user/USERNAME/in.txt，即指向了hdfs://localhost:9000/user/USERNAME/in.txt。这个值仅仅当没有其他schema被指定时生效。一般hadoop中core-site.xml中都会配置fs.default.name。fs.hdfs.hadoopconf:# HDFS的配置路径。例如：/home/flink/hadoop/hadoop-2.6.0/etc/hadoop。如果配置了这个值，用户程序中就可以简写hdfs路径如：hdfs:///path/to/files。而不用写成：hdfs://address:port/path/to/files这种格式。配置此参数后，Flink就可以找到此路径下的core-site.xml和hdfs-site.xml了。建议配置此参数。 — 内存管理 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于批处理程序，Flink使用了自我的内存管理，默认使用70%的taskmanager.heap.mb的内存。这样Flink批处理程序不会出现OOM问题，因为Flink自己知道有多少内存可以使用，当内存不够时，就使用磁盘空间。而且这样有些operation可以直接访问数据，而不需要序列化数据到java对象。Flink自我管理的内存可以加速程序的执行。如果需要的话，自我管理的内存也可以在JVM堆外被分配，这也有助于提升性能。 1234567891011taskmanager.memory.size:# 相对于jobmanager.heap.mb，使用多少内存用于内存管理器进行内存的自我管理。如果没有指定，则默认值是-1，代表参考参数taskmanager.memory.fraction。设置了自我内存管理，Flink在批处理中就可以在堆内或堆外进行排序、hash、cache等操作。taskmanager.memory.fraction:# 当taskmanager.memory.size没有设置时（或-1），此参数才生效。意思是使用taskmanager.heap.mb的百分比用于自我内存管理。默认值是0.7，代表使用70%的taskmanager的内存。剩余的30%用于UDF的堆以及用于taskmanager间通信数据的内存。taskmanager.memory.off-heap:# 默认是false。如果设置为true，则taskmanager在JVM堆外分配内存。这对于大段内存的使用很有效。taskmanager.memory.segment-size:# 内存段的大小，默认单位是32KB。taskmanager.memory.preallocate:# 默认是false。指定Flink当启动时，是否一次性分配所有管理的内存。如果taskmanager.memory.off-heap设置true，则建议此值也设置为true。 — Kerberos（Flink安全配置）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Flink支持Hadoop生态的kerberos认证服务，例如HDFS，Yarn，或者是HBase，值得注意的是，Flink仅支持Hadoop2.6.1及以上版本，其他的版本可能在Flink作业中失败。 12345678# 指示是否从 Kerberos ticket 缓存中读取security.kerberos.login.use-ticket-cache: true# 包含用户凭据的 Kerberos 密钥表文件的绝对路径security.kerberos.login.keytab: /path/to/kerberos/keytab# 与 keytab 关联的 Kerberos 主体名称security.kerberos.login.principal: flink-user# 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）security.kerberos.login.contexts: Client,KafkaClient —Zookeeper安全配置 1234# 覆盖以下配置以提供自定义 ZK 服务名称zookeeper.sasl.service-name: zookeeper# 该配置必须匹配 "security.kerberos.login.contexts" 中的列表（含有一个）zookeeper.sasl.login-context-name: Client —HistoryServer 12345678910111213# 你可以通过 bin/historyserver.sh (start|stop) 命令启动和关闭 HistoryServer# 将已完成的作业上传到的目录jobmanager.archive.fs.dir: hdfs:///completed-jobs/# 基于 Web 的 HistoryServer 的地址historyserver.web.address: 0.0.0.0# 基于 Web 的 HistoryServer 的端口号historyserver.web.port: 8082# 以逗号分隔的目录列表，用于监视已完成的作业historyserver.archive.fs.dir: hdfs:///completed-jobs/# 刷新受监控目录的时间间隔（以毫秒为单位）historyserver.archive.fs.refresh-interval: 10000 —其他 1234567891011121314151617181920212223242526272829303132333435363738394041424344askmanager.tmp.dirs: # taskmanager的临时目录，默认是系统的tmp路径。可以指定多个路径，通过：隔开，多路径的设置导致多个线程去执行IO操作。taskmanager.log.path:# taskmanager的日志文件路径，默认在$FLINK_HOME/log jobmanager.web.address: # Jobmanager的web接口地址，默认是anyLocalAddress()，即所有地址的请求都会被处理，指向jobmanager的IP地址jobmanager.web.port: # Jobmanager的web接口，默认是8081jobmanager.web.tmpdir: # 存放web接口静态文件的路径，上传的jar文件也会存储在此路径（这里指通过webUI启动job时上传的jar文件）。默认是系统tmp路径。jobmanager.web.upload.dir: # 通过web UI上传的jar文件的路径，如果没有指定，则指向jobmanager.web.tmpdir的路径。fs.overwrite-files: # 当输出文件到文件系统时，是否覆盖已经存在的文件。默认是falsefs.output.always-create-directory: # 文件输出时是否单独创建一个子目录。默认是false，即直接将文件输出到指向的目录下taskmanager.network.numberOfBuffers: # taskmanager数据在网络传输的大小，默认是2048个块，每个块大小是32KB。这个值官方建议的值为：#slots-per-TM^2 * #TMs * 4。我之前按照这个建议配置时，系统出错说这个值太小。个人建议这个值可以稍微设置大些。state.backend: # 当检查点被激活时，保存的有状态的检查点的目录。默认是存在jobmanager的内存中。也支持保存到hdfs或者rocksdb中。默认是jobmanager，如果是hdfs，则制定filesystem。例如hdfs://namenode-host:port/flink-checkpoints；如果是RocksDB，则制定RocksDB的路径。state.backend.fs.checkpointdir: # 存储检查点的具体路径，必须是Flink可访问的路径。例如：hdfs://namenode-host:port/flink-checkpoints。state.backend.rocksdb.checkpointdir: # 存储RocksDB的检查点路径，默认是taskmanager.tmp.dirsstate.checkpoints.dir: # checkpoint数据的目录。recovery.zookeeper.storageDir: # 定义Jobmanager的的元数据信息，zookeeper只是保存一个指向此目录的指针，在HA环境下用于Jobmanager的恢复。例如设置为: hdfs:///flink/recovery。blob.storage.directory: # 指定taskmanager中Blob文件（jar）的路径。blob.server.port: # taskmanager中blob服务器的端口号，默认是0，即操作系统选定一个可用的端口。可以指定一个范围 (“50100-50200”)，避免相同机器中运行多个Jobmanager的情况时使用。restart-strategy: # 默认是"none"，即不设置任何重启策略。如果设置为"fixed-delay"，则代表固定延迟策略；如果指定"failure-rate"，则代表失败的时间比例。restart-strategy.fixed-delay.attempts: # fixed-delay模式下的恢复尝试次数，默认是1次。restart-strategy.fixed-delay.delay: # fixed-delay模式下，两次重启尝试之间的延迟时间。默认为akka.ask.timeout.restart-strategy.failure-rate.max-failures-per-interval: # failure-rate模式下，失败重启的最大次数，默认是1次。restart-strategy.failure-rate.failure-rate-interval: # failure-rate下，失败时间间隔的度量。restart-strategy.failure-rate.delay: # failure-rate模式下，两次重启之间的延迟时间，默认是akka.ask.timeout. 0x3 slaves/master1234567# master host:portλ deletee [1.6.0/libexec/conf] cat masterslocalhost:8081#slave hostλ deletee [1.6.0/libexec/conf] cat slaves localhost 0x4 zoo.cfg123456789101112131415161718# 每个 tick 的毫秒数tickTime=2000# 初始同步阶段可以采用的 tick 数initLimit=10# 在发送请求和获取确认之间可以传递的 tick 数syncLimit=5# 存储快照的目录# dataDir=/tmp/zookeeper# 客户端将连接的端口clientPort=2181# ZooKeeper quorum peersserver.1=localhost:2888:3888# server.2=host:peer-port:leader-port 0x5 日志配置&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Flink在不同平台运行的日志文件 1234567log4j-cli.propertieslog4j-console.propertieslog4j-yarn-session.propertieslog4j.propertieslogback-console.xmllogback-yarn.xmllogback.xml 0x6 sql-client-defaults.yaml12345678910111213141516171819202122232425execution: # 'batch' or 'streaming' execution type: streaming # allow 'event-time' or only 'processing-time' in sources time-characteristic: event-time # interval in ms for emitting periodic watermarks periodic-watermarks-interval: 200 # 'changelog' or 'table' presentation of results result-mode: changelog # parallelism of the program parallelism: 1 # maximum parallelism max-parallelism: 128 # minimum idle state retention in ms min-idle-state-retention: 0 # maximum idle state retention in ms max-idle-state-retention: 0 deployment: # general cluster communication timeout in ms response-timeout: 5000 # (optional) address from cluster to gateway gateway-address: "" # (optional) port from cluster to gateway gateway-port: 0 Flink Sql Client 可以了解更多关于 Flink sql的配置及使用]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x2 技术平台</category>
        <category>1x23 Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink学习笔记01->Mac下部署flink并构建简单程序]]></title>
    <url>%2F2019%2F08%2F11%2Fflink-learning-notes-01%2F</url>
    <content type="text"><![CDATA[说在前面的话&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着Flink大数据平台快速发展及日益成熟，公司也逐步利用Flink来解决业务问题，之前也学过Scala，Spark，由于本人在实际工作中应用场景较少，基本上又忘了。本次利用一个月的时间，每天更新一篇自学笔记用以记录及知识分享。 准备工作12jdk-1.8flink-1.6.0 Mac 下安装 flink1brew install apache-flink 若是网络不好的话，可能会下载非常慢（brew下载未完成的文件后缀为.incomplete）或是失败，可以把brew待下载的包通过其他渠道先下载到cache路径下12deletee [~] → brew --cache/Users/deletee/Library/Caches/Homebrew 具体操作思路： 点击下载 flink安装包 123456789101112131： # brew install apache-flink以上命令会提示开始下载一个文件，但进度很慢。半个小时过去了，才到30%。复制上面的下载链接，使用其它下载工具下载。例如使用迅雷，可以在5分钟左右下载完成，也可以尝试配合vpn下载。加速下载的办法大家都是各有绝招，根据自己的环境选择最快的一种下载方式即可。2：找到brew下载文件的目录，将刚才下载的文件移动到目录 # cd `brew --cache` //进入brew的下载目录 # rm apache-flink-1.6.0.tar.gz.incomplete //删除刚才下载一半的文件 # mv apache-flink-1.6.0.tar.gz ./ //将下载好的压缩包放到brew下载目录3：继续执行：brw install apache-flink 安装之后，可以通过命令进行验证12deletee [~] → flink -vVersion: 1.6.0, Commit ID: ff472b4 那么安装好的flink在什么位置呢？/usr/local/Cellar/apache-flink/1.6.0/我们配置下FLINK_HOME, vim ~/.bashrc 1export FLINK_HOME=/usr/local/Cellar/apache-flink/1.6.0/ 启动Flink一键启动 12345→ cd $FLINK_HOME/libexec [ac229f9]deletee [apache-flink/1.6.0/libexec]$: bin/start-cluster.sh [ac229f9]Starting cluster.Starting standalonesession daemon on host localhost.Starting taskexecutor daemon on host localhost. 是的，可以通过$FLINK_HOME/libexec 路径下 bin目录下 启动脚本来启动 flink 此时可以通过浏览器访问: http://localhost:8081 访问flink web界面 构建Flink Maven 工程 (flink-java)简要说明 ​ Demo是接收套接字输入流进行分词计数。 工程信息 123&lt;groupId&gt;org.hz.deletee&lt;/groupId&gt;&lt;artifactId&gt;flink-learning&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://maven.apache.org/POM/4.0.0" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.hz.deletee&lt;/groupId&gt; &lt;artifactId&gt;flink-learning&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;flink.version&gt;1.6.0&lt;/flink.version&gt; &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt; &lt;scala.version&gt;2.11.12&lt;/scala.version&gt; &lt;maven.build.timestamp.format&gt;yyyyMMddHHmmss&lt;/maven.build.timestamp.format&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Add connector dependencies here. They must be in the default scope (compile). --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Add logging framework, to produce console output when running in the IDE. --&gt; &lt;!-- These dependencies are excluded from the application JAR by default. --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package org.hz.detelee.x01;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;public class SocketTextStreamWordCount &#123; public static void main(String[] args) throws Exception &#123; //参数检查 if (args.length != 2) &#123; System.err.println("USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"); return; &#125; String hostname = args[0]; Integer port = Integer.parseInt(args[1]); // set up the streaming execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据 DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port); //计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new LineSplitter()) .keyBy(0) .sum(1); sum.print(); env.execute("Java WordCount from SocketTextStream Example"); &#125; public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; String[] tokens = s.toLowerCase().split("\\W+"); for (String token: tokens) &#123; if (token.length() &gt; 0) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 编译 1mvn clean package -Dmaven.skpi.test=true 执行开启数据入监听 12# 开启输入流 端口监听nc -kl 9000 运行程序1flink run -c org.hz.detelee.x01.SocketTextStreamWordCount flink-learning-1.0-SNAPSHOT.jar 127.0.0.1 9000 查看结果日志12cd $FLINK_HOME/libexec/log/tail -f flink-deletee-taskexecutor-0-localhost.out]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x2 技术平台</category>
        <category>1x23 Flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git创建与合并分支]]></title>
    <url>%2F2019%2F04%2F11%2Fgit-repository%2F</url>
    <content type="text"><![CDATA[什么是版本库呢？版本库又名仓库，英文名repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。 所以，创建一个版本库非常简单，首先，选择一个合适的地方，创建一个空目录： 1234$ mkdir learngit$ cd learngit$ pwd/Users/michael/learngit pwd命令用于显示当前目录。在我的Mac上，这个仓库位于/Users/michael/learngit。 如果你使用Windows系统，为了避免遇到各种莫名其妙的问题，请确保目录名（包括父目录）不包含中文。 第二步，通过git init命令把这个目录变成Git可以管理的仓库： 12$ git initInitialized empty Git repository in /Users/michael/learngit/.git/ 瞬间Git就把仓库建好了，而且告诉你是一个空的仓库（empty Git repository），细心的读者可以发现当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 如果你没有看到.git目录，那是因为这个目录默认是隐藏的，用ls -ah命令就可以看见。 也不一定必须在空目录下创建Git仓库，选择一个已经有东西的目录也是可以的。不过，不建议你使用自己正在开发的公司项目来学习Git，否则造成的一切后果概不负责。 把文件添加到版本库首先这里再明确一下，所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git也不例外。版本控制系统可以告诉你每次的改动，比如在第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”。而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道，也没法知道。 不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的，前面我们举的例子只是为了演示，如果要真正使用版本控制系统，就要以纯文本方式编写文件。 因为文本是有编码的，比如中文有常用的GBK编码，日文有Shift_JIS编码，如果没有历史遗留问题，强烈建议使用标准的UTF-8编码，所有语言使用同一种编码，既没有冲突，又被所有平台所支持。 使用Windows的童鞋要特别注意： 千万不要使用Windows自带的记事本编辑任何文本文件。原因是Microsoft开发记事本的团队使用了一个非常弱智的行为来保存UTF-8编码的文件，他们自作聪明地在每个文件开头添加了0xefbbbf（十六进制）的字符，你会遇到很多不可思议的问题，比如，网页第一行可能会显示一个“?”，明明正确的程序一编译就报语法错误，等等，都是由记事本的弱智行为带来的。建议你下载Notepad++代替记事本，不但功能强大，而且免费！记得把Notepad++的默认编码设置为UTF-8 without BOM即可： 言归正传，现在我们编写一个readme.txt文件，内容如下： 12Git is a version control system.Git is free software. 一定要放到learngit目录下（子目录也行），因为这是一个Git仓库，放到其他地方Git再厉害也找不到这个文件。 和把大象放到冰箱需要3步相比，把一个文件放到Git仓库只需要两步。 第一步，用命令git add告诉Git，把文件添加到仓库： 1$ git add readme.txt 执行上面的命令，没有任何显示，这就对了，Unix的哲学是“没有消息就是好消息”，说明添加成功。 第二步，用命令git commit告诉Git，把文件提交到仓库： 1234$ git commit -m &quot;wrote a readme file&quot;[master (root-commit) eaadf4e] wrote a readme file 1 file changed, 2 insertions(+) create mode 100644 readme.txt 简单解释一下git commit命令，-m后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。 嫌麻烦不想输入-m &quot;xxx&quot;行不行？确实有办法可以这么干，但是强烈不建议你这么干，因为输入说明对自己对别人阅读都很重要。实在不想输入说明的童鞋请自行Google，我不告诉你这个参数。 git commit命令执行成功后会告诉你，1 file changed：1个文件被改动（我们新添加的readme.txt文件）；2 insertions：插入了两行内容（readme.txt有两行内容）。 为什么Git添加文件需要add，commit一共两步呢？因为commit可以一次提交很多文件，所以你可以多次add不同的文件，比如： 123$ git add file1.txt$ git add file2.txt file3.txt$ git commit -m &quot;add 3 files.&quot; 小结现在总结一下今天学的两点内容： 初始化一个Git仓库，使用git init命令。 添加文件到Git仓库，分两步： 使用命令git add &lt;file&gt;，注意，可反复多次使用，添加多个文件； 使用命令git commit -m &lt;message&gt;，完成。]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x02 Git管理</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git简介]]></title>
    <url>%2F2019%2F04%2F11%2Fgit-brief%2F</url>
    <content type="text"><![CDATA[Git是什么？ Git是目前世界上最先进的分布式版本控制系统（没有之一）。 Git有什么特点？简单来说就是：高端大气上档次！ 那什么是版本控制系统？ 如果你用Microsoft Word写过长篇大论，那你一定有这样的经历： 想删除一个段落，又怕将来想恢复找不回来怎么办？有办法，先把当前文件“另存为……”一个新的Word文件，再接着改，改到一定程度，再“另存为……”一个新文件，这样一直改下去，最后你的Word文档变成了这样： 过了一周，你想找回被删除的文字，但是已经记不清删除前保存在哪个文件里了，只好一个一个文件去找，真麻烦。 看着一堆乱七八糟的文件，想保留最新的一个，然后把其他的删掉，又怕哪天会用上，还不敢删，真郁闷。 更要命的是，有些部分需要你的财务同事帮助填写，于是你把文件Copy到U盘里给她（也可能通过Email发送一份给她），然后，你继续修改Word文件。一天后，同事再把Word文件传给你，此时，你必须想想，发给她之后到你收到她的文件期间，你作了哪些改动，得把你的改动和她的部分合并，真困难。 于是你想，如果有一个软件，不但能自动帮我记录每次文件的改动，还可以让同事协作编辑，这样就不用自己管理一堆类似的文件了，也不需要把文件传来传去。如果想查看某次改动，只需要在软件里瞄一眼就可以，岂不是很方便？ 这个软件用起来就应该像这个样子，能记录每次文件的改动： 版本 文件名 用户 说明 日期 1 service.doc 张三 删除了软件服务条款5 7/12 10:38 2 service.doc 张三 增加了License人数限制 7/12 18:09 3 service.doc 李四 财务部门调整了合同金额 7/13 9:51 4 service.doc 张三 延长了免费升级周期 7/14 15:17 这样，你就结束了手动管理多个“版本”的史前时代，进入到版本控制的20世纪。 Git的诞生 很多人都知道，Linus在1991年创建了开源的Linux，从此，Linux系统不断发展，已经成为最大的服务器系统软件了。 Linus虽然创建了Linux，但Linux的壮大是靠全世界热心的志愿者参与的，这么多人在世界各地为Linux编写代码，那Linux的代码是如何管理的呢？ 事实是，在2002年以前，世界各地的志愿者把源代码文件通过diff的方式发给Linus，然后由Linus本人通过手工方式合并代码！ 你也许会想，为什么Linus不把Linux代码放到版本控制系统里呢？不是有CVS、SVN这些免费的版本控制系统吗？因为Linus坚定地反对CVS和SVN，这些集中式的版本控制系统不但速度慢，而且必须联网才能使用。有一些商用的版本控制系统，虽然比CVS、SVN好用，但那是付费的，和Linux的开源精神不符。 不过，到了2002年，Linux系统已经发展了十年了，代码库之大让Linus很难继续通过手工方式管理了，社区的弟兄们也对这种方式表达了强烈不满，于是Linus选择了一个商业的版本控制系统BitKeeper，BitKeeper的东家BitMover公司出于人道主义精神，授权Linux社区免费使用这个版本控制系统。 安定团结的大好局面在2005年就被打破了，原因是Linux社区牛人聚集，不免沾染了一些梁山好汉的江湖习气。开发Samba的Andrew试图破解BitKeeper的协议（这么干的其实也不只他一个），被BitMover公司发现了（监控工作做得不错！），于是BitMover公司怒了，要收回Linux社区的免费使用权。 Linus可以向BitMover公司道个歉，保证以后严格管教弟兄们，嗯，这是不可能的。实际情况是这样的： Linus花了两周时间自己用C写了一个分布式版本控制系统，这就是Git！一个月之内，Linux系统的源码已经由Git管理了！牛是怎么定义的呢？大家可以体会一下。 Git迅速成为最流行的分布式版本控制系统，尤其是2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至GitHub，包括jQuery，PHP，Ruby等等。 历史就是这么偶然，如果不是当年BitMover公司威胁Linux社区，可能现在我们就没有免费而超级好用的Git了。]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x02 Git管理</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]职场晋升进阶宝典：都是知识点]]></title>
    <url>%2F2019%2F01%2F26%2Fpromotion-reply%2F</url>
    <content type="text"><![CDATA[时间眨眼到了5月，阿里人最近都在忙什么呢？橙子去公司内网看了看，发现“晋升”已经成了热搜词。 晋升，可不是你想象中简单的升职加薪哦。 它既是认可，也是期望。是对过去一段时间内，个人的能力成长和业务产出的认可；也是对今后的责任、付出提出了更高的要求和期望。听说面对压力，不少同学还是挺紧张的，这种关键时候，贴心的橙子自然会想方设法助大家一臂之力咯~你看，我千方百计找到了几大绝招哦！话说这绝招从哪儿学来的？ 《晋升答辩的7个小技巧》是玉顺（玉顺，飞猪（阿里集团旗下旅行品牌）产品总监，曾就职于美国微软Bing、腾讯。）在2016年写的，同步发表在某社交平台，当时就被评为自媒体热文。今年这个时候恰逢晋升季，他就拿出来跟小伙伴们分享，本文略有删减 。 01 展示你的核心能力。(Show your core competency, through projects) 好多同学的答辩PPT都是按照时间排序，把自己做过的大大小小的项目罗列一遍。这样做最大的问题是目的不清楚，答辩本身不是评估项目，而是评估你这个人，评估你这个人的能力。 我首先要问你的一个问题就是，“你想要展示哪些核心能力？” 在答辩的准备上，你一定不能沉没到项目的细节里面，要跳出项目，仔细考虑自己的核心能力有哪些提升，如何把自己的核心能力表现出来。 02 展示你的领导力。(Show leadership in small wins) 比如，你如果想在任何一个岗位上有出色的成绩，单靠完成工作是不够的，你需要展现“领导力”：主动承担、积极推动。这能让面试官相信，你可以承担更大更重要的责任。 有同学认为领导力是需要title才能发挥出来的，不，你不一定是Leader的title，各个通道的同学在平常的工作中，都可以展现领导力。 比如开发同学并没有等着产品经理来催进度、协调资源，而是主动和产品、前端、iOS、安卓的负责同学沟通，推动项目的进展和问题的解决，目标是在细分领域做到业界第一，这就是领导力。 03 深入思考。(Think twice on “why”) 我发现在面试中，有一类同学容易脱颖而出，他们往往在做过的项目中有自己较深入的思考。 比如同样一个项目，一个技术同学可能只记得写了多少行代码，完成了什么功能；另一个同学可以清晰地分析架构设计、可扩展性、容错，以及不同实现框架的对比，展现出了极强的思考能力。 这类深入思考的同学，比单纯只是执行的同学，能够承担更复杂和更重要的项目。 例如，每个产品创新，都需要产品经理站在整个行业生态的角度去考虑和布局，在做具体功能之前想清楚价值链的各个环节和价值洼地，想清楚了之后再来设计具体的场景和功能。 “Keep your feet on the ground, head above the cloud” ，这是我很喜欢一句话，也是鼓励大家多多思考，站在更高的角度和格局上去思考和提高。 04 强调数据度量。 (Emphasize more on metrics, less on idea) 在你的项目中，你如何采集数据、建立度量、分析数据，是表现你科学思考的重要手段。 有一些同学从心里会有点瞧不起做数据的工作，觉得更喜欢做产品功能开发，但是，数据分析和决策能力是一种非常重要的能力。现代的互联网开发讲究的是Growth hacking，它的核心是持续的数据驱动的产品设计和开发流程。Big data, A/B testing都是越来越普及的工具。 有时候有的同学找到我，请我做一个两难的决策，我问了几个问题往往就发现，他对数据的分析和采集不够；反过来，很多时候，这些数据出来了，不需要问我就已经可以找到清晰合理的方案了。 推荐一本书叫做“How To Measure Anything” ，可以帮助建立一套以数据驱动的决策体系，不管你是什么岗位，读一下都是很有好处的。 05 知行合一。(Walk the talk, talk the walk) 虽然每年的晋升机会是有限的，但职业能力的提升是一个持续的过程。每天进步一点点，3个月之后就会大不一样。 这里面的关键是，光知道是不够的，要去做，不断地做，不断地思考。做投资有一个重要的概念是复利，提高自己也是一样的，不断地积累，哪怕每次只有一点点，这个积累的力量是惊人的。 这里的英文标题我用的是Walk the talk, talk the walk. 它有两层意思，Walk the talk, 是实践你所知道的，talk the walk是讲你实践过的。平时你需要walk the talk,晋升评审的时候talk the walk，一切就变得很容易了。 06 陈述要简洁。(KISS: Keep it Simple, Stupid) 我看很多同学讲PPT有一个主要的问题，就是无关的内容太多。回到第一点，罗列内容和项目是没有用的，关键是你是否展示了你的核心能力。 网络接口设计中有一个重要的原则是KISS: Keep it simple, stupid. 你在讲PPT的时候也需要非常简洁， 只突出你的核心能力，整个PPT都是围绕你的核心能力组织和展开，你讲的每一句话都是围绕这一个目标。 07 练习，练习，练习。(Practice, practice, practice) 再好的演讲者也需要不断练习。乔布斯每次苹果发布会都要练习几十次，精益求精。对我们这些平时不太有机会演讲的产品和技术同学们，更加需要加倍的练习。 Stanford大学的教授Andrew Ng，是个美籍华人，刚加入Stanford的时候，因为不善言谈，讲课排名垫底，后来靠不断的有意识的练习，变成了最受欢迎的老师之一；他的机器学习课程也是Coursera上的最受欢迎的课程之一。我上过他的机器学习，讲解生动有趣，也让我坚定地相信，每一个同学都可以成为一个出色的演讲者，只是需要“练习，练习，练习”。 在阿里有句老话：成长是自己的事儿。 如果每一天都在重复着昨天，没有思考、总结和提升，那么你的人生就像荡秋千，只是在重复摇摆而已。 蹲下来，是为了跳得更高。 如果对照着“聪明、乐观、皮实、自省”这“四好青年”标准，你样样都达标很优秀了，那么晋升就是一个水到渠成的结果。 没有比脚更长的路，没有比人更高的山，为了更好的自己，你我共勉。]]></content>
      <categories>
        <category>3x 职场篇</category>
        <category>3x1 职场宝典</category>
      </categories>
      <tags>
        <tag>职场</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年，自己做了什么？]]></title>
    <url>%2F2019%2F01%2F26%2F2018-summary%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回想2018年，大部分日常工作是日志收集、数据统计、数据分析等工作，太过零碎有不能好好的展开，所以还是蛮纠结的。所以打算从可以提炼的日常以及唯一一个值得说道的项目：数据仓库机房迁移作为2018年完成的总结。 0x1 数据仓库机房迁移0x2 数据开发的日常未完待续]]></content>
      <categories>
        <category>3x 职场篇</category>
        <category>3x0 总结与规划</category>
        <category>3x00 2018年总结</category>
      </categories>
      <tags>
        <tag>工作总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起来学Scala(4)-循环]]></title>
    <url>%2F2019%2F01%2F22%2Fscala-programing-loop%2F</url>
    <content type="text"><![CDATA[&ensp;&ensp;&ensp;&ensp;&ensp;今天学习的内容是循环，循环语句允许我们多次执行一个语句或语句组，下面是大多数编程语言中循环语句的流程图：&ensp;&ensp;&ensp;&ensp;&ensp;Scala中的循环与Java中循环表现不一样，主要体现在写法及卫语句支持。下面看一段伪代码： 1234567for(line &lt;- source.getLines)&#123; for (char &lt;- line)&#123; if char.isLetter&#123; // your code &#125; &#125;&#125; Scala可以这样简写… 12345for&#123;line &lt;- source.getLines)&#123; char &lt;- line if char.isLetter // your code&#125; Scala可使得循环的写法非常简洁。 0x1 循环类型0x10 for循环表达式: 123for(x &lt;- Range )&#123; statement(x);&#125; 举个例子「🌰」例1： 1234567891011scala&gt; var r = Range(1,4,1)r: scala.collection.immutable.Range = Range(1, 2 ,3)scala&gt; for( x&lt;- r)&#123; | println(x) | &#125;123scala&gt; 遍历一个Map，例2 1234val names = Map("fname" -&gt; "Robert","lname"-&gt;"Goren")for((k,v)&lt;-names)&#123; println(s"key:$k,value:$v")&#125; 多种循环： 123for(x &lt;- Range; y&lt;- Range )&#123; statement(x,y);&#125; 0x11 while循环表达式如下: 123while(condition)&#123; statement(x);&#125; 例子： 123456789101112scala&gt; var i = 0i: Int = 0scala&gt; while(i&lt;10)&#123; | println(i) | i += 1 | &#125;0123scala&gt; 多重循环条件: 1234while(condition1)&#123; while(condition2) statement(x);&#125; 0x12 do…while循环while 与 do…while循环类似 表达式如下： 123do&#123; statement(x);&#125;while(condition) 0x2 循环中的卫语句&ensp;&ensp;&ensp;&ensp;&ensp;循环中的卫语句即在for循环中嵌入if语句，我们在业务开发中，在遍历过程中需要根据if 条件进行不同逻辑处理，比如过滤掉某些项。通常的做法是： 123456for(i &lt;- 1 to 10)&#123; if (i % 2 == 0) &#123; println(i) &#125;&#125; 在Scala中，我们可以这样写: 123for(i &lt;- 1 to 10 if i % 2 == 0)&#123; println(i)&#125; 也可以这样写 1234for&#123; i &lt;- 1 to 10 if i % 2 == 0&#125; println(i) 表达式语法： 123456for&#123; loop condition1 condition2 ...&#125; statement(); 0x3 yield语法&ensp;&ensp;&ensp;&ensp;&ensp;通过yield语法生成新的集合（PS：通常原集合是什么类型，生成的集合就是什么类型），假设有一个数组，数组值都是小写的字符串，将其进行首字母大写处理： 123456789val arr = Array("delete","scala","zt")val arr_yield = for (i&lt;-arr) yield i.capitalizescala&gt; arr_yield.foreach(println)DeleteScalaZtscala&gt; 0x4 break &amp; continue&ensp;&ensp;&ensp;&ensp;&ensp;与Java和其他语言不通，在Scala中break 一种方法，而不是一个关键字，在Scala中需用用到 breakable及break两个方法，由break抛出异常，breakable进行异常捕获，由此实现break &amp; continue功能，下面看一个例子： 例1：break实现 12345breakable&#123; for(i&lt;- 1 to 10)&#123; if i % 2 == 0 break # 将跳出循环 实现break &#125;&#125; 例2：continue实现 12345for(i&lt;- 1 to 10)&#123; breakable&#123; if i % 2 == 0 break # 跳出当前，继续下一遍历 &#125;&#125; 在scala.util.control.breaks类中，有非常清晰的定义： 123456789101112131415161718192021 /** * A block from which one can exit with a `break`. The `break` may be * executed further down in the call stack provided that it is called on the * exact same instance of `Breaks`. */ def breakable(op: =&gt; Unit) &#123; try &#123; op &#125; catch &#123; case ex: BreakControl =&gt; if (ex ne breakException) throw ex &#125; &#125;/** * Break from dynamically closest enclosing breakable block using this exact * `Breaks` instance. * * @note This might be different than the statically closest enclosing block! */ def break(): Nothing = &#123; throw breakException &#125; 由break函数抛出异常，由breakable捕获异常 0xF 总结&ensp;&ensp;&ensp;&ensp;&ensp;循环结构与其他语言大同小异，Scala中更加简洁的表达方式是其一大亮点，学习Scala可能需要多适应Scala语言的表达方式才能理解，当然是用常规的方式进行表达也不会有问题。]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Hive窗口函数]]></title>
    <url>%2F2019%2F01%2F20%2Fhow-to-use-window-func%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;窗口函数常常用于我们业务中的复杂计算，本文介绍Hive中几个常用的窗口函数，并用案例讲述如何使用。那么我们常用的窗口函数有哪些？ 函数名 备注 row_number 分组内根据排列条件从1开始的排序，没有相同的序号 rank 分组内根据排列条件从1开始的排序，排名相等会在名次中留下空位，例如：1，2，2，4 dense_rank 分组内根据排列条件从1开始的排序，排名相等会在名次中不会留下空位，例如：1，2，2，3 lag 分组内根据排列条件取前第n个值 lead 分组内根据排列条件取后第n个值，与 lag相反 first_value 分组内根据排列条件去分组第一个值 last_value 分组内根据排列条件去分组最后一个值 min 分组内取最小值 max 分组内取最大值 sum 分组内取分组值的和 … … PS: 所有的窗口函数都将是生成的一个新列，并不会对原有数据列造成影响 下面将一一这些函数的使用。 0x0 row_numberrow_number 可以说是最常用的一个窗口函数。 举个栗子「🌰」：我们有一组数据，是一个班级一学期4次数学月考的成绩表 score 学生ID(sid) 第几次月考（exam_time） 成绩(score) s001 1 83 s001 2 87 s001 3 84 s001 4 89 s002 1 90 s002 2 75 s002 3 89 s002 4 82 那么Question：请问每个学生4次月考成绩中最好的一次是哪一次月考？ 此时，max的手法是不能够使用了，如果解决上述问题？实际上是将根据每个学生进行分组，即每个学生的4次月考为一组，那么计算这个分组成绩中的最大值即可 12345678910111213select sid ,exam_time ,scorefrom ( select sid ,exam_time ,score ,row_number() over(partition by sid order by score desc) as r_idx from score)t where t.r_idx = 1 说明： 我们称 row_number 为窗口函数，over为从句，窗口函数与over配合使用 1、使用PARTITION BY语句，使用一个或者多个原始数据类型的列进行分组2、使用ORDER BY语句，使用一个或者多个数据类型的排序列 反过来over 中若不指定 partition by 的数据列，那么整个表作为一个默认分组，同理不指定 order by 那么默认当前顺序，大家先只需了解这两点，熟练之后可以学习第三点。 3、使用窗口规范，窗口规范支持以下格式： (ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING 详细参考文档：Hive官方参考文档]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x2 技术平台</category>
        <category>1x21 Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ftp上传目录的方法]]></title>
    <url>%2F2019%2F01%2F16%2Fftp-upload-folder%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;众所周知，使用ftp命令是不能够直接上传文件夹的，然后工作中有很多需要上传目录的场景，通常大家想到的就是先创建文件夹，然后再通过遍历的方式上传文件，其中比较繁琐的是，ftp里不支持，多层级目录直接创建，1deletee:$ mkdir -p $&#123;dir&#125;/$&#123;subdir&#125; 这样的方式，只能逐级创建，将造成很多工作量。下面将分享下如何快速有效的实现上传ftp文件夹 0x0 生成创建多级文件夹命令1234567891011export mkdir_cmds=`find $&#123;upload_folder&#125; -type d -printf '%P\n' \ |awk '&#123; \ split($0,a,"/"); \ for (i in a) &#123; \ cmds ="mkdir "; \ for (j=1;j&lt;=i;j++) &#123; \ cmds = cmds"/"a[j] \ &#125; print cmds \ &#125; \ &#125;' \ |sort|uniq` 效果如下： 1234567891011121314151617181920212223242526272829deletee@pig_house:~/logon_log/bin$ ls -lhtotal 48K-rwxr-xr-x 1 delete pig_house 324 Jan 9 14:59 clearftpfile.sh-rwxr-xr-x 1 delete pig_house 2.2K Jan 11 12:19 contact_send.sh-rwxr-xr-x 1 delete pig_house 5.7K Jan 9 14:59 handle_file_repair.sh-rwxr-xr-x 1 delete pig_house 7.9K Jan 10 10:29 handle_file.shdrwxr-xr-x 3 delete pig_house 4.0K Jan 14 15:59 logon-rwxr-xr-x 1 delete pig_house 2.2K Jan 11 12:18 mail_send.sh-rwxr-xr-x 1 delete pig_house 2.2K Jan 11 12:18 news_send.shdrwxr-xr-x 3 delete pig_house 4.0K Jan 11 12:19 reg-rwxr-xr-x 1 delete pig_house 1.4K Jan 9 14:59 repair_login.sh-rwxr-xr-x 1 delete pig_house 2.8K Jan 16 14:50 send_news.shdeletee@pig_house:~/logon_log/bin$ upload_folder="bin"deletee@pig_house:~/logon_log/bin$ find $&#123;upload_folder&#125; -type d -printf '%P\n' \&gt; |awk '&#123; \&gt; split($0,a,"/"); \&gt; for (i in a) &#123; \&gt; cmds ="mkdir /$&#123;upload_folder&#125;"; \&gt; for (j=1;j&lt;=i;j++) &#123; \&gt; cmds = cmds"/"a[j] \&gt; &#125; print cmds \&gt; &#125; \&gt; &#125;' \&gt; |sort|uniqmkdir /bin/logonmkdir /bin/logon/bakmkdir /bin/regmkdir /bin/reg/bak 0x1 生成put文件命令&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原理与创建文件夹命令类似1export put_cmds=`find $&#123;upload_folder&#125; -type f -printf 'put %p $&#123;ftp_target_folder&#125;/%P\n'` 123456789deletee@pig_house:upload_path='~/logon_log/bin'deletee@pig_house:upload_folder='bin'deletee@pig_house:ftp_target_path='/ftp'deletee@pig_house:find $&#123;upload_path&#125; -type f -printf 'put %p $&#123;ftp_target_path&#125;/%P\n'put ./bin/logon/bak/police-logon.jar.bak /ftp/bin/logon/bak/police-logon.jar.bakput ./bin/logon/bak/run.sh /ftp/bin/logon/bak/run.shput ./bin/logon/bak/stop.sh /ftp/bin/logon/bak/stop.sh... 0x2 一个栗子123456789101112131415161718192021222324252627282930313233#!/bin/bashupload_path=$1 # 待上传目录ftp_target_path=$2 # ftp目标目录upload_folder=`basename $&#123;upload_path&#125;`export mkdir_cmds=`find $&#123;upload_path&#125; -type d -printf '%P\n' \ |awk '&#123; \ split($0,a,"/"); \ for (i in a) &#123; \ cmds ="mkdir $&#123;upload_folder&#125;"; \ for (j=1;j&lt;=i;j++) &#123; \ cmds = cmds"/"a[j] \ &#125; print cmds \ &#125; \ &#125;' \ |sort|uniq`export put_cmds=`find $&#123;upload_path&#125; -type f -printf 'put %p $&#123;ftp_target_folder&#125;/%P\n'`ftp_host=ftp_port=ftp_username=ftp_password=ftp -i -n &lt;&lt; EOFopen $ftp_host $ftp_portuser $ftp_username $ftp_passwordbi$&#123;mkdir_cmds&#125;$&#123;put_cmds&#125;byeEOF 0XF 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用生成ftp命令的方式，避免多层级及不定层级目录的创建ftp目录的繁琐。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大家觉得怎么样？或是有更好的方法！]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x00 一起来学Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起来学Scala(3)-字符串]]></title>
    <url>%2F2019%2F01%2F10%2Fscala-programing-string%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文中已经介绍了Scala语言中的基本类型，本文着重介绍下字符串的用法，字符串在我们日常开发中经常用到，字符串在Scala中的类型String，那么Scala中的String和Java中的String有什么关系呢？ 1234scala&gt; "Scala".getClass.getNameres0: String = java.lang.Stringscala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本以为Scala是怎样实现了String，上述代码中告诉我们Scala中的String竟就是Java中的String。我们接下更深层次的学习String。 0x0 基本用法0x00 长度 .length1234567scala&gt; val str = "Scala"str: String = Scalascala&gt; str.lengthres3: Int = 5scala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过 .length的方式可获取变量str的长度 0x01 遍历第一种方式: 通过遍历字符串的每一个字符 12345678scala&gt; for(c &lt;- str) &#123; println(c) &#125;Scalascala&gt; 第二种方式: 通过调用.foreach方法，入参 println函数实现 12345678scala&gt; str.foreach(println)Scalascala&gt; 第三种方式: 通过调用.map方法，入参 println函数实现 12345678scala&gt; str.map(println)Scalascala&gt; 0x02 过滤 .filter1234scala&gt; str.filter(_ != 'a')res10: String = Sclscala&gt; 0x03 Bytes数组 .getBytes1234scala&gt; str.getBytesres11: Array[Byte] = Array(83, 99, 97, 108, 97)scala&gt; 0x0F 小结 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;String有很多的方法，例如 .drop(n) 删除前面n个字符,.take(n)取前n个字符，.captitalize 将字符串转为大写(与 .toUpperCase 功能等同)等等。 0x1 字符串相等性&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如何比较两个字符串是否相等 12345678910scala&gt; val str1 = "Scala"str1: String = Scalascala&gt; val str2 = "S" + "cala"str2: String = Scalascala&gt; str1 == str2res12: Boolean = truescala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如上，两个字符串内容是相等的,通过== 可判断两个字符串内容是否相等，这与Java中使用equal的方法比较两个对象不同。在Java中需要先判断调用的对象是否为null，即在AnyRef类使用时，Scala也会判断调用是否是null，其他情况下是不需要判断变量是否为null。实际上在Scala开发过程中，不推荐定义null， 0x2 创建多行字符串&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一点与Python类似，可以通过三个双引号来定义，例如： 1234567scala&gt; val multi_line_str = """Hello, | I love Scala"""multi_line_str: String =Hello, I love Scalascala&gt; 显示第二行是一连串的空格，我们可以使用管道的方式，与Python类似 123456789scala&gt; val multi_line_str = """Hello, | I love Scala | """multi_line_str: String ="Hello,I love Scala"scala&gt; 或者 123456789scala&gt; val multi_line_str = """Hello, | I love Scala | """.stripMarginmulti_line_str: String ="Hello,I love Scala"scala&gt; 如果不喜欢使用管道 |可以使用其他符号表示，例如: 123456789scala&gt; val multi_line_str = """Hello, @ I love Scala @ """.stripMargin('@')multi_line_str: String ="Hello,I love Scala"scala&gt; 0x3 变量代换&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 类似于Python一样，使用变量的值代换进另一个字符串 0x30 s是一个方法1234567scala&gt; val name = "deleee"name: String = deleeescala&gt; val setence = s"My name is $name"setence: String = My name is deleeescala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;整形变量可以做计算，例如： 1234567scala&gt; var age = 2age: Int = 2scala&gt; val setence1 = s"My age is $&#123;age + 1&#125;"setence1: String = My age is 3scala&gt; 0x31 字符串差值(printf)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 即我们常在C语言或者Python中的数值格式化，需要在字符串前使用f标识 1234scala&gt; val setence2 = f"My age is $&#123;age + 1&#125;%.2f"setence2: String = My age is 3.00scala&gt; %.2f表示将数值进行小数点后2位精度 格式化符号 描述 %c 字符 %d 整数 与 %i 相同 %e 指数浮点型 %f 浮点型 %i 整数 %o 八进制 %s 字符串 %u 无符号整形 %x 十六进制 %% 或者 \% 输出一个百分号 0x4 正则表达式0x40 创建正则表达式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在字符串末尾使用.r即表示创建的为一个Regex对象，随后可以用.findFirstIn匹配第一个，.findAllIn来匹配所有的。 例如: 1234567891011121314scala&gt; var numPatten = "[0-9]+".rnumPatten: scala.util.matching.Regex = [0-9]+scala&gt; val str = "123 is not 321"str: String = 123 is not 321scala&gt; val match1 = numPatten.findFirstIn(str)match1: Option[String] = Some(123)scala&gt; numPatten.findAllIn(str).foreach(println)123321scala&gt; 或者： 1234567scala&gt; import scala.util.matching.Regeximport scala.util.matching.Regexscala&gt; val pattern = new Regex("(S|s)cala")pattern: scala.util.matching.Regex = (S|s)calascala&gt; PS:关注下返回类型 （1）.findFirstIn 返回的一个是Some类型的变量，我们可以简单的认为Some是一个容器，匹配成功则返回数字，匹配不成功就返回None 12345678910scala&gt; var numPatten = "[0-9]+".rnumPatten: scala.util.matching.Regex = [0-9]+scala&gt; val str = "not number"str: String = not numberscala&gt; val match1 = numPatten.findFirstIn(str)match1: Option[String] = Nonescala&gt; （2）.findAllIn返回的是一个迭代器，可使用foreach方法进行遍历，也可以将迭代器转为数组,例如: 123456789101112scala&gt; var matchArr = numPatten.findAllIn(str).toArraymatchArr: Array[String] = Array(123, 321)scala&gt; for ( a &lt;- matchArr) println(a)123321scala&gt; matchArr.foreach(println)123321scala&gt; 0x41 字符串替换常规的替换手法字符串调用方法.replaceAll 1234567scala&gt; val str = "123 is not 321"str: String = 123 is not 321scala&gt; str.replaceAll("[1-9]+","sss")res12: String = sss is not sssscala&gt; 也可以使用正则表达式作为对象调用方法 .replaceAllIn及.replaceFirstIn 123456789scala&gt; var numPatten = "[0-9]+".rscala&gt; val str = "123 is not 321"str: String = 123 is not 321scala&gt; numPatten.replaceAllIn(str,"sss")res13: String = sss is not sssscala&gt; 0x42 字符串抽取正则表达式中使用() 表示 匹配的组，与其他（Perl、Python表达式相同） 例如： 12345678910111213scala&gt; import scala.util.matching.Regeximport scala.util.matching.Regexscala&gt; val numberPattern: Regex = "[0-9]".rnumberPattern: scala.util.matching.Regex = [0-9]scala&gt; numberPattern.findFirstMatchIn("awesomepassword") match &#123; | case Some(_) =&gt; println("Password OK") | case None =&gt; println("Password must contain a number") | &#125;Password must contain a numberscala&gt; 123456789101112131415161718192021222324scala&gt;import scala.util.matching.Regexscala&gt;val keyValPattern: Regex = "([0-9a-zA-Z-#() ]+): ([0-9a-zA-Z-#() ]+)".rscala&gt;val input: String = """background-color: #A03300; |background-image: url(img/header100.png); |background-position: top center; |background-repeat: repeat-x; |background-size: 2160px 108px; |margin: 0; |height: 108px; |width: 100%;""".stripMarginscala&gt;for (patternMatch &lt;- keyValPattern.findAllMatchIn(input)) | println(s"key: $&#123;patternMatch.group(1)&#125; value: $&#123;patternMatch.group(2)&#125;")key: background-color value: #A03300key: background-image value: url(imgkey: background-position value: top centerkey: background-repeat value: repeat-xkey: background-size value: 2160px 108pxkey: margin value: 0key: height value: 108pxkey: width value: 100scala&gt; 更多更详细的内容请参考Scala之旅-正则表达式模式和提取器对象（EXTRACTOR OBJECTS） 0x4E 正则表达式Scala 的正则表达式继承了 Java 的语法规则，Java 则大部分使用了 Perl 语言的规则。 下表我们给出了常用的一些正则表达式规则： 表达式 匹配规则 ^ 匹配输入字符串开始的位置。 $ 匹配输入字符串结尾的位置。 . 匹配除”\r\n”之外的任何单个字符。 […] 字符集。匹配包含的任一字符。例如，”[abc]”匹配”plain”中的”a”。 ... 反向字符集。匹配未包含的任何字符。例如，”abc“匹配”plain”中”p”，”l”，”i”，”n”。 \\A 匹配输入字符串开始的位置（无多行支持） \\z 字符串结尾(类似$，但不受处理多行选项的影响) \\Z 字符串结尾或行尾(不受处理多行选项的影响) re* 重复零次或更多次 re+ 重复一次或更多次 re? 重复零次或一次 re{ n} 重复n次 re{ n,} re{ n, m} 重复n到m次 a\ b 匹配 a 或者 b (re) 匹配 re,并捕获文本到自动命名的组里 (?: re) 匹配 re,不捕获匹配的文本，也不给此分组分配组号 (?&gt; re) 贪婪子表达式 \\w 匹配字母或数字或下划线或汉字 \\W 匹配任意不是字母，数字，下划线，汉字的字符 \\s 匹配任意的空白符,相等于 [\t\n\r\f] \\S 匹配任意不是空白符的字符 \\d 匹配数字，类似 [0-9] \\D 匹配任意非数字的字符 \\G 当前搜索的开头 \\n 换行符 \\b 通常是单词分界位置，但如果在字符类里使用代表退格 \\B 匹配不是单词开头或结束的位置 \\t 制表符 \\Q 开始引号：\Q(a+b)*3\E 可匹配文本 “(a+b)*3”。 \\E 结束引号：\Q(a+b)*3\E 可匹配文本 “(a+b)*3”。 0x4F 正则表达式实例 实例 描述 . 匹配除”\r\n”之外的任何单个字符。 [Rr]uby 匹配 “Ruby” 或 “ruby” rub[ye] 匹配 “ruby” 或 “rube” [aeiou] 匹配小写字母 ：aeiou [0-9] 匹配任何数字，类似 [0123456789] [a-z] 匹配任何 ASCII 小写字母 [A-Z] 匹配任何 ASCII 大写字母 [a-zA-Z0-9] 匹配数字，大小写字母 aeiou 匹配除了 aeiou 其他字符 0-9 匹配除了数字的其他字符 \\d 匹配数字，类似: [0-9] \\D 匹配非数字，类似: 0-9 \\s 匹配空格，类似: [ \t\r\n\f] \\S 匹配非空格，类似: \t\r\n\f \\w 匹配字母，数字，下划线，类似: [A-Za-z0-9_] \\W 匹配非字母，数字，下划线，类似: A-Za-z0-9_ ruby? 匹配 “rub” 或 “ruby”: y 是可选的 ruby* 匹配 “rub” 加上 0 个或多个的 y。 ruby+ 匹配 “rub” 加上 1 个或多个的 y。 \\d{3} 刚好匹配 3 个数字。 \\d{3,} 匹配 3 个或多个数字。 \\d{3,5} 匹配 3 个、4 个或 5 个数字。 \\D\\d+ 无分组： + 重复 \d (\\D\\d)+/ 分组： + 重复 \D\d 对 ([Rr]uby(, )?)+ 匹配 “Ruby”、”Ruby, ruby, ruby”，等等 0x5 自定义方法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Java中通常我们是建立类似StringUtils这种的通用类库，想要在String类中增加一些自定义的方法要怎么做？Java中是无法办到的，在Scala 2.1.0中可以定义隐式转换的类，在这个类中定义自己的方法来实现期望的功能。 123456789scala&gt; implicit class StringImprovement(s:String) &#123; | def increment = s.map(c =&gt; (c.toByte + 1).toChar) | &#125;defined class StringImprovementscala&gt; "abc".incrementres16: String = bcdscala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以任务String调用的方法都不奇怪，可能某处引用的包中定义了某个隐式转换的类及自定义方法。在Scala2.1.0之前的版本，若想要实现这样的功能，需要定义一个隐式转换的方法： 12345678910111213scala&gt; class StringImprovement(s:String) &#123; | def increment = s.map(c =&gt; (c.toByte + 1).toChar) | &#125;defined class StringImprovementscala&gt; implicit def stringToString(s:String) = new StringImprovement(s)warning: there was one feature warning; re-run with -feature for detailsstringToString: (s: String)StringImprovementscala&gt; "abc".incrementres0: String = bcdscala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如你所见，在Scala中可以创建一个隐式转换的类，将他们引入需要的范围中去，而不需要进行继承，定义一个MyXxxx的新类。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面简要描述下其工作原理： 编译器找到”abc”的字符串常量 编译器发现要在String调用increment方法 因为String类中没有increment方法，它开始在当前范围内搜索一个接受String作为参数的隐式转换。 如此编译器会找到StringImprovement类，在这个类中找到了increment方法]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起来学Scala(2)-基本类型]]></title>
    <url>%2F2019%2F01%2F09%2Fscala-programing-basictype%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文中已经介绍了Scala语言的入门概要，本文将介绍Scala的基本语法，Scala中有变量、对象、类、方法，在面向对象方面Scala与Java很像，在代码编写风格与Python又很像，除此之外，大家也会碰到一些未曾接触的语法，例如 _ . 等，本文将逐步介绍这些概念。 0x0 基本类型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们了解Scala中有哪些基本类型： 数据类型 描述 Byte 8位有符号补码整数。数值区间为 -128 到 127 Short 16位有符号补码整数。数值区间为 -32768 到 32767 Int 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 Float 32 位, IEEE 754 标准的单精度浮点数 Double 64 位 IEEE 754 标准的双精度浮点数 Char 16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF String 字符序列 Boolean true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null 或空引用 Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型。 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 (摘自Scala基本类型) 下图展示了基本类型之间的拓扑关系图，Scala中一切皆对象： 12val str:String = "Hello Scala"var i:Int = 10 上述实例中声明了变量str类型是String型，变量值为”Hello Scala”当为变量分配初始值时，Scala编译器可以根据分配给它的值来推断变量的类型。这被称为变量类型推断。 因此，可以编写这样的变量声明 12val str = "Hello Scala"var i = 10 0x1 声明变量0x10 声明关键字: val,var&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val 用于定义常量，相当于Java中使用final定义一样，val 是value的缩写&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var 用于定义变量，val 是variable的缩写&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Scala语言中，使用val定义的变量，是不能够重新赋值的，而var定义的变量是可以重新进行赋值 12345678scala&gt; val str = "Hello World"str: String = Hello Worldscala&gt; str = "Hello Scala"&lt;console&gt;:12: error: reassignment to val str = "Hello Scala" ^scala&gt; 1234567scala&gt; var str = "Hello World"str: String = Hello Worldscala&gt; str = "Hello Scala"str: String = Hello Scalascala&gt; 0x11 多个赋值&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scala支持多个赋值。如果代码块或方法返回一个元组(Tuple - 保持不同类型的对象的集合)，则可以将元组分配给一个val变量。 注：我们将在随后的章节学习元组。 12val (myVar1: Int, myVar2: String) = Pair(40, "Foo")Scala 类型推断得到正确的类型 - 1val (myVar1, myVar2) = Pair(40, "Foo") 0x2 变量的作用域&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scala中变量同其他语言类似，有不同的作用域，具体取决于它们被使用的位置。它们可以作为字段存在，作为方法参数和局部变量存在。以下是每种类型范围的详细信息。 0x20 字段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;声明为对象中的字段信息，那么根据需要可使用val或者var，对象中的字段可以被对象中其他方法访问，配合访问修饰符，可以设置外部变量或者对象访问 0x21 方法参数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;方法或函数入参都是常量，所以定义函数时都是使用val关键字定义，方法的参数是不可变的。 0x22 局部变量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;局部变量是在方法中声明的变量。局部变量只能从方法内部访问，但如果从方法返回，则您创建的对象可能会转义该方法。局部变量可以是可变的和不可变的类型，可以使用var或val定义。 0xF 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文主要介绍了Scala中基本数据类型及变量的声明方法，根据需要选择合适的关键字进行变量定义。var 修饰的对象引用可以改变，val修饰的则不可改变，但对象的状态却是可以改变的，即表示对于一个对象，通过 val 初始化对象后，该对象是不能够改变的，但是如果对象中有var声明的字段，那么是可以修改对象字段值，实现对象状态的改变。 123456789101112131415161718192021class A(n: Int) &#123; var value = n&#125;class B(n: Int) &#123; val value = new A(n)&#125;object Test &#123; def main(args: Array[String]) &#123; val x = new B(5) x = new B(6) // 错误，因为 x 为 val 修饰的，引用不可改变 x.value = new A(6) // 错误，因为 x.value 为 val 修饰的，引用不可改变 x.value.value = 6 // 正确，x.value.value 为var 修饰的，可以重新赋值 &#125;&#125;// --------------------- // 作者：GuaKin_Huang // 来源：CSDN // 原文：https://blog.csdn.net/a1234h/article/details/77962536 // 版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的一些SQL语句]]></title>
    <url>%2F2019%2F01%2F09%2Fuseful-sql%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际工作过程中，经常用的几个SQL，常常因为某些关键字忘记，从而经常需要找工具书或者之前做的工程，本文将自己工作中常常容易忘记的SQL整理出来，方便直接定位。 0x0 建表DDL(指定分隔符、分区)12345678910create table dim.dim_sms_rate_standard( contry_code string comment '国家码' ,contry_en_name string comment '国家英文名' ,contry_brief_name string comment '国家简码' ,sms_price float comment '短信单价')row format delimited fields terminated by '\t'partition(dt string comment '日期'); 0x1 设置压缩格式12set hive.exec.compress.output=true;set mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; 0x2 查看外表分区路径1desc formatted dim.dim_sms_rate_standard partition(dt='2019-01-08'); 0x3 动态分区1234set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict; SET hive.exec.max.dynamic.partitions=100000;SET hive.exec.max.dynamic.partitions.pernode=100000; 0xF 未完待续]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x2 技术平台</category>
        <category>1x21 Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起来学Scala(1)-入门概要]]></title>
    <url>%2F2019%2F01%2F07%2Fscala-programing-brief%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为什么要学Scala？Scala是大数据分析平台Spark、Flink 官方支持的语言，在学习Spark和Flink之前，需要学好Scala基础。Scala同样是一门当前热门的语言，Kafka、Spark均由Scala开发，由此学习Scala是学习这些平台比不可少的步骤。 0x0 Scala基本定义Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。Scala 运行在Java虚拟机上，并兼容现有的Java程序。Scala 源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库，由而Java同样可以调用Scala的类库。Scala语言 使用 JDK &amp; Scala SDK 0x1 Scala的运行方式开发Scala程序之前需要安装Scala 0x10 脚本运行Scala也是一种脚本式语言，可以将Scala程序写入一个脚本，直接执行 如下是第一个 Scala程序:HelloWorld.scala 12345object HelloWorld&#123; def main(args:Array[String])&#123; println("Hello World") &#125;&#125; 使用命令: 12~$:scala HelloWorld.scalaHello World 0x11 编译执行12345~$:scalac HelloWorld.scala# 将会得到如下两个 class文件，即为 Java字节码HewlloWorld$.classHewlloWorld.class 执行方式与执行javaclass类同 12~$:scala HelloWorldHello World 0xF 总结Scala 的语法相对Python而言是比较晦涩，后续尽量以简单清晰的实例，从而达到更好的效果。]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk的基本使用方法]]></title>
    <url>%2F2019%2F01%2F06%2Fawk-basic%2F</url>
    <content type="text"><![CDATA[0x0 简介&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。 0x1 使用方法1awk '&#123;pattern + action&#125;' &#123;filenames&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。awk 有如下三种使用方式 0x10 命令行方式awk [-F field-separatir] 'commands' input-file 1awk [-F field-separatir] 'commands' input-file 此种方式为最常用的一种方式，-F 是将文件按指定分隔符进行切割。 0x11 shell脚本方式将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。相当于shell脚本首行的：#!/bin/sh可以换成：#!/bin/awk 0x12 将所有的awk命令插入一个单独文件，然后调用：1awk -f awk-script-file input-file(s) 其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。 0x2 入门实例0x20 分隔符例如：a.txt 121 2 31 2 3 awk 默认支持分隔符，例如 空格，制表符，如下两种方式等价 12awk -F&apos; &apos; &apos;&#123;print $1&#125;&apos; a.txtawk &apos;&#123;print $1&#125;&apos; a.txt $0变量是指整条记录。$1表示当前行的第一个域,$2表示当前行的第二个域,……以此类推。 例如： b.txt 121,2,33,4,5 123awk -F&apos;,&apos; &apos;&#123;print $1&#125;&apos;13 0x21 BEGIN &amp; END123456789cat /etc/passwd \|awk -F ':' 'BEGIN &#123;print "name,shell"&#125; &#123;print $1","$7&#125; END &#123;print "blue,/bin/nosh"&#125;'name,shellroot,/bin/bashdaemon,/bin/shbin,/bin/shsys,/bin/sh....blue,/bin/nosh BEGIN 是在处理数据行之前进行的操作 END 则是在数据行处理之后进行的操作 例如：c.txt 求第一列的总和 12341,2,33,4,54,5,67,8,9 命令如下： 1awk -F',' 'BEGIN &#123;sum=0&#125; sum+=$1 END &#123;print "sum = "sum&#125;' BEGIN 声明一个sum 初始化值为0 的变量，END 将变量输出 0x3 awk内置变量1234567891011ARGC 命令行参数个数ARGV 命令行参数排列ENVIRON 支持队列中系统环境变量的使用FILENAME awk浏览的文件名FNR 浏览文件的记录数FS 设置输入域分隔符，等价于命令行 -F选项NF 浏览记录的域的个数NR 已读的记录数OFS 输出域分隔符ORS 输出记录分隔符RS 控制记录分隔符 0x4 awk 编程awk 可在内部实现丰富的编程 条件语句 awk中的条件语句是从C语言中借鉴来的，见如下声明方式： 12345678910111213141516171819if (expression) &#123; statement; statement; ... ...&#125;if (expression) &#123; statement;&#125; else &#123; statement2;&#125;if (expression) &#123; statement1;&#125; else if (expression1) &#123; statement2;&#125; else &#123; statement3;&#125; 例如：统计某个文件夹下的文件占用的字节数,过滤4096大小的文件(一般都是文件夹): 12ls -l |awk 'BEGIN &#123;size=0;print "[start]size is ", size&#125; &#123;if($5!=4096)&#123;size=size+$5;&#125;&#125; END&#123;print "[end]size is ", size/1024/1024,"M"&#125;' [end]size is 8.22339 M 循环语句 awk中的循环语句同样借鉴于C语言，支持while、do/while、for、break、continue，这些关键字的语义和C语言中的语义完全相同。 数组 因为awk中数组的下标可以是数字和字母，数组的下标通常被称为关键字(key)。值和关键字都存储在内部的一张针对key/value应用hash的表格里。由于hash不是顺序存储，因此在显示数组内容时会发现，它们并不是按照你预料的顺序显示出来的。数组和变量一样，都是在使用时自动创建的，awk也同样会自动判断其存储的是数字还是字符串。一般而言，awk中的数组用来从记录中收集信息，可以用于计算总和、统计单词以及跟踪模板被匹配的次数等等。 12345678awk -F ':' 'BEGIN &#123;count=0;&#125; &#123;name[count] = $1;count++;&#125;; END&#123;for (i = 0; i &lt; NR; i++) print i, name[i]&#125;' /etc/passwd0 root1 daemon2 bin3 sys4 sync5 games...... awk编程的内容极多，这里只罗列简单常用的用法，更多请参考：gawk]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x00 一起来学Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库工程师学习路线]]></title>
    <url>%2F2019%2F01%2F05%2Fedw-need-learning%2F</url>
    <content type="text"><![CDATA[刚看了木东居士关于《聊一聊如何面向简历学习》，有一些感触，每每思考到职业发展总是有一种恐慌，因为我们很多时候都忘了梦想，甚至根本没有梦想，也就谈不上规划，今天是思考和整理下自己的学习路线。写下自己的想法及规划。 你是否担心互联网寒冬中首先淘汰的那波人就有自己？你是否感觉到自己缺乏核心竞争力？是否感觉已经很久没有学习成长？ &nbsp; &nbsp; &nbsp; &nbsp;一个数据仓库工程师或者数据开发人员，需要掌握什么技能？数据仓库应该分为四类：建模方法论，实施方法论、数据管理方法论、数据应用，所以我们将木东居士的内容整理了一下，如下： 主题 内容 时间 数据建模 常用数据仓库模型原理总结 1 周 数据建模 维度建模原理 1 周 数据建模 以淘宝的场景为例设计一套数据模型 1 周 数据建模 时间维表设计 1 周 数据管理 元数据管理 1 周 数据管理 数据质量监控设计 1 周 数据管理 数据血缘分析 1 周 数据管理 作业监控设计 1 周 数据应用 OLap分析原理+Kylin实践 3 周 数据应用 数据分层设计 1 周 大数据平台 Hive（基本用法、高阶函数用法、Hive优化） 2 周 大数据平台 Hive 执行过程原理 1 周 大数据平台 Hadoop 常用命令 1 周 大数据平台 HBase 基本用法，原理 2 周 大数据平台 Spark 常用命令 1 周 大数据平台 Flink 基本用法，原理 2 周 大数据平台 Kafka 基本用法，原理 2 周 脚本语言 Python 2 周 脚本语言 Scala 2 周 论文阅读 Google 的 Goods 论文阅读（数据管理） 1 周 在日常工作中，并不是所有技术都有在使用，往往没有使用的技术或方法论，则容易被我们忘记，所以最好的方式，就是写博客或者写PPT，以自己理解的话术表达出来，相信这样的模式会比单纯的记忆一些概念要好很多。 感谢@木东居士的分享]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x1 数据仓库</category>
        <category>1x12 其他</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk的高级应用-多目录输出]]></title>
    <url>%2F2018%2F12%2F31%2Fawk-application%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;awk是Shell中三剑客之一，功能十分强大，本文分享的通过一份管道输入，实现多目录输出。原理：awk内部可实现编程，在awk内部将输入流进行预处理后，利用 print ${content} &gt;&gt; ${path}的方式将数据进行分流。 下面一个实例： 通过HDFS 标准输入流，在awk中根据数据日期时间分流至不同的日期文件夹中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#!/usr/bin/env bash#***********************************************************************#脚本功能: 追补历史推送数据#创建日期: 2016-05-16#修改纪录： 修改人 修改日期 修改描述#----+-----------------+--------------------+--------------------------+# 1 deletee 2017-10-12 创建#***********************************************************************# 环境变量设置#-------------------------------------------------------# JOB_HOME：作业目录# HDFS_PATH :HDFS目录#-------------------------------------------------------# LocalJOB_HOME=$(cd "$(dirname "$0")"; pwd)JOB_DATA_PATH=$&#123;JOB_HOME&#125;/dataPROCESSED_FILE_PATH=$&#123;JOB_HOME&#125;/proSECRETS_CONF_PATH=$&#123;JOB_HOME&#125;/confmkdir -p $&#123;JOB_HOME&#125;/&#123;data,pro,conf&#125;# HDFSHADOOP_HOME=/home/$&#123;USER&#125;/software/hadoopHDFS_PATH=/hdfs/*/some_log/#---------------------------# 函数名:genTargetFileModel# 功能:生成目标文件模式,遍历HDFS目录，生成日、时、分# 参数: $1 日期，格式 yyyy-MM-dd# 返回:无#---------------------------function genTargetFileModel() &#123; # 遍历日期 ETL_DT=$1 CUR_HDFS_PATH=`echo $&#123;HDFS_PATH&#125;|sed "s:*:$&#123;ETL_DT&#125;:"` for (( i = 0; i &lt; 24; i++ )); do for (( j = 0; j &lt; 12; j++ )); do hour=`printf "%02d" $i` minutes=$((j*5)) minutes=`printf "%02d" $minutes` mkdir -p $&#123;JOB_DATA_PATH&#125;/$&#123;ETL_DT&#125;/$&#123;hour&#125;/$&#123;minutes&#125; done done for path in `$&#123;HADOOP_HOME&#125;/bin/hadoop fs -ls $&#123;CUR_HDFS_PATH&#125; \ |awk '&#123;print $8&#125;'`;do echo "$&#123;ETL_DT&#125;:$&#123;path&#125;" &gt;&gt; $&#123;PROCESSED_FILE_PATH&#125;/pro_$&#123;ETL_DT&#125;.txt $&#123;HADOOP_HOME&#125;/bin/hadoop fs -text $&#123;path&#125; |awk '&#123; split($0,a," "); datestr=substr(a[1],11,10); hourstr=substr(a[2],1,2); minutestr=substr(a[2],4,2); minutestr=int(minutestr/5)*5; if(minutestr&lt;10)&#123; minutestr=("0"minutestr) &#125; gsub("\"","",a[3]) split(a[3],b,","); gsub("&#123;|&#125;","",b[3]) gsub("fields:HOSTNAME:","",b[3]) gsub("\\\\t","\t",b[1]) gsub(";","\t",b[1]) lens=split(b[1],tA,"\t") if(lens==12)&#123; re=(b[1]"\t\t\t\t\t\t\t\t\t\t\t\t"b[3]) &#125; if(lens==15)&#123; re=(b[1]"\t\t\t\t\t\t\t\t\t"b[3]) &#125; path=(datestr"/"hourstr"/"minutestr"/log_"datestr"_"hourstr"_"minutestr".txt") print re &gt;&gt; path &#125;' done unset ETL_DT&#125;#---------------------------# 函数名:genTargetFileModel# 功能:生成目标文件模式,遍历HDFS目录，生成日、时、分# 参数: $1 日期，格式 yyyy-MM-dd# 返回:无#---------------------------function sendByRysnc() &#123; ETL_DT=$1 nohup /usr/bin/rsync -avz \ --bwlimit=30720 \ --port=873 \ --progress \ --password-file=$&#123;SECRETS_CONF_PATH&#125;/secret.conf $&#123;JOB_DATA_PATH&#125;/$&#123;ETL_DT&#125; $&#123;user&#125;@$&#123;ip&#125;::$&#123;module&#125;/$&#123;path&#125;/ &gt;&gt; rysnc_send_$(date +%Y%m%d).log 2&gt;&amp;1 &amp; unset ETL_DT&#125;function mainStart() &#123; ETL_DT=$1 genTargetFileModel $&#123;ETL_DT&#125; sendByRysnc $&#123;ETL_DT&#125;&#125;mainStart 2017-09-29]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x0 开发语言</category>
        <category>1x00 一起来学Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(5)-维度设计]]></title>
    <url>%2F2018%2F12%2F29%2Fdim-design%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;都说维度设计是维度建模的灵魂，在维度建模中，我们将度量成为「事实」，将环境描述为「维度」，但是如何设计一个好的维度模型，我们需要遵循一定的技巧和方法。 如何标识维度？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常我们都会问「谁、何时、何地、为何、如何」来描述一个事件过程，同样维度可以理解为一个事件或者业务过程的角度，比如商品购买购买人、购买事件、购买产品、购买使用的设备、购买使用的IP等均为维度范畴 围绕业务流程来构建维度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;考虑到需求总是变换多端，但是有一点不变：需求总是围绕着业务流程，那么当我们设计维度的时候，就需要充分生成丰富的维度及属性 维度的基本设计方法？（1）根据业务流程确定主维度 ​ 即在业务过程中找出关键维度作为主维度，在企业级数据仓库中，必需维度的统一口径，以商品为例，有且仅有一个维度定义 （2）确定相关维度 （3）确定维度属性 尽可能生成丰富的维度属性 尽可能多地给出包括一些富有意义的文字性描述​ 维度建模中较多维度以ID形态存在，例如：商品ID，但是单一的ID在业务描述非常欠缺，通常ID的含义描述也随ID作为维度设计中的一部分 区分数值型属性和事实​ 在一些业务数据中，一些数值型字段是作为维度还是作为度量（事实） 尽量沉淀出通用的维度属性​ 有些维度属性值获取是经过了复杂的逻辑处理，或者是通过解析某个字段得到，或是多张表关联得到，当设计出负责的维度时，若不保留原生维度，将难以覆盖业务的变化​ 并且属性值格式做到统一，比如某模型A时间格式为 yyyy-MM-dd HH:mm:ss，而模型B中的时间格式 则是 Unix Timestamp，同样也将造成口径不一致 理解关键字：维度设计需要严格遵循开发规范，对字段命名、含义、口径、属性值格式的规范，需严格遵循。 常见维度设计方法 （1）缓慢变化或快速变化 ​ 维度缓慢变化 修改维度属性值 不保留历史操作，适用于无需关心历史维度值的情形。 插入一行 保留历史操作，由于多了新记录，使得维度关联时，会产生冗余记录，增加额外学习成本 插入一列 预留一列作为上一次维度值的保留，适用于只需要查看上一次的维度值情形 快照维度 即将每天的变更存储为一个快照，在维度表较小时，可以忽略存储 极限存储 通过拉链表的形式，记录维度的缓慢变化，但是注意，维度表中有频繁变化的字段。 ​ 维度快速变化 ​ 此类问题又称微型维度，主要是为了解决快变超大维度（rapidly changing monster dimension）,顾名思义，在某些维度表中，大量维度进行缓慢变化甚至没有变化，但是有少量的维度频繁的发生变化，此种情形下，若使用缓慢变化的方式进行处理的话，将消耗大量资源。由此解决办法是： ​ 将维度属性值频率变化比较高的字段提取出来，建立一个单独的维度表，只需维度这一张快速变化的维度表即可。 （2）维度的层次结构 ​ 也是通常所说的 规范化和反规范化 ​ 在维度设计的过程中，我们经常遇到维度的层次问题，比如：商品通常会有一级分类、二级分类、三级分类此种情形，对于这样的维度处理方式，通常有两种做法： 星系模型 将维度层次问题扁平化，将层次的维度项进行扁平处理，例如有如下维度：sku_id，category_lv1, category_lv2, category_lv3 … 雪花模型 将层级维度提炼出来单独作为一张新的维度表 上文已经阐述过，两种模型的利弊，在新互联网的大数据平台基础上，存储也变的非常廉价，通常是选用星系模型解决维度层次的问题 （3）维度一致性 In data warehousing, a conformed dimension is a dimension that has the same meaning to every fact with which it relates. Conformed dimensions allow facts and measures to be categorized and described in the same way across multiple facts and/or data marts, ensuring consistent reporting across the enterprise. ​ 维度一致性是如此解释的，在数据仓库中，一致的维度是与其相关的每个事实具有相同含义的维度。 一致的维度允许在多个事实和/或数据集市中以相同的方式对事实和度量进行分类和描述，从而确保整个企业的一致报告。 ​ 通常在数据仓库开发过程中如何保证维度的一致性呢？ 维度的定义需要遵循命名、取值等规范 维度需通过元数据进行管理 采用维度建模，意味着建仓过程是自下而上的，各个数据集市各自开发，那么如何保证企业级数据仓库维度一致性，则需要统一的元数据进行管理 维度建模的数据仓库中，有一个概念叫Conformed Dimension，中文一般翻译为“一致性维度”。一致性维度是Kimball的多维体系结构（MD）中的三个关键性概念之一，另两个是总线架构（Bus Architecture）和一致性事实（Conformed Fact）。 在多维体系结构中，没有物理上的数据仓库，由物理上的数据集市组合成逻辑上的数据仓库。而且数据集市的建立是可以逐步完成的，最终组合在一起，成为一个数据仓库。如果分步建立数据集市的过程出现了问题，数据集市就会变成孤立的集市，不能组合成数据仓库，而一致性维度的提出正式为了解决这个问题。 一致性维度的范围是总线架构中的维度，即可能会在多个数据集市中都存在的维度，这个范围的选取需要架构师来决定。一致性维度的内容和普通维度并没有本质上区别，都是经过数据清洗和整合后的结果。 一致性维度建立的地点是多维体系结构的后台（Back Room），即数据准备区。在多维体系结构的数据仓库项目组内需要有专门的维度设计师，他的职责就是建立维度和维护维度的一致性。在后台建立好的维度同步复制到各个数据集市。这样所有数据集市的这部分维度都是完全相同的。建立新的数据集市时，需要在后台进行一致性维度处理，根据情况来决定是否新增和修改一致性维度，然后同步复制到各个数据集市。这是不同数据集市维度保持一致的要点。 在同一个集市内，一致性维度的意思是两个维度如果有关系，要么就是完全一样的，要么就是一个维度在数学意义上是另一个维度的子集。例如，如果建立月维度话，月维度的各种描述必须与日期维度中的完全一致，最常用的做法就是在日期维度上建立视图生成月维度。这样月维度就可以是日期维度的子集，在后续钻取等操作时可以保持一致。如果维度表中的数据量较大，出于效率的考虑，应该建立物化视图或者实际的物理表。 这样，维度保持一致后，事实就可以保存在各个数据集市中。虽然在物理上是独立的，但在逻辑上由一致性维度使所有的数据集市是联系在一起，随时可以进行交叉探察等操作，也就组成了数据仓库。 （4）维度整合和拆分 ​ 因为数据仓库的数据源自各个系统，有从移动端，web端，PC端，每个段的数据结构差异较大，即便是同一端，可能因为业务拆分，字段的属性值也不完全一致。 ​ 例如：A系统 某字段值用 1/0表示 token是否有效，B系统则使用F/T表示是否有效 ​ 那么，在此种情形下，需要进行维度整合。维度整合需要遵循如下规范： 命名规范统一 字段类型统一 属性值编码和含义统一 ​ 另外有一种场景，因为系统差异太大，而无法进行维度整合，此时需要进行维度的拆分 ​ 例如：加油站主要的商品是则是油，加油站内同样也有零售店，售卖一些日常百货，油 有 92，95，98等维度属性，日常百货的通常 涉及的 进销存，单价等 ​ 两种商品在维度差异过大，通常的做法，加油站的主营业务是售卖油品，从而建立主要的商品维度表，另外建立一个零售商品维度记录表 ​ 理解关键词：各个业务差异独特性较大的业务各自建立独立的两个维度表 （5）杂项维度的处理方法 退化维度​ 所谓退化维度（degenerate dimension），是指在实施表中那些看起来像是一个事实表的一个维度关键字，但实际上没有对应的维度字段。退化维度一般都是事务的编号，如购物小票编号，发票编号 行为维度​ 行为维度是基于过去维度成员的行为进行分组或者过滤事实的办法。行为维度即将事实转化为维度，以确保获得更多的分析能力​ 例如：购买次数超过30次，30次至100次，超过100次 作为维度值 角色维度​ 角色维度通常是一个业务活动有多个角色参与，例如：办理银行业务，有客户经理，审批人等参与两个及以上角色，而这些角色均属于员工维度表，对于此种情形，没有必要根据角色建立多个维度表，而是可以通过建立视图的方式达到目的。 多值维度​ 多值维度一般会出现多对多的关系中，例如：购买房产，会有夫妻两人共同持有，一次下单，多个子订单 ​ 有如下三种方法： ​ 1）降低事实表的粒度 每一个事实都标注最小粒度，例如：前台业务与商业智能关注交易子订单，每一个子订单一个事实，只会有一个商品与之对应，很多时候，事实表的粒度是不能降低的，当强行降低之后，那么订单事实则发生改变，对于统计每日订单量计算，则复杂度变高，增加学习和计算成本 ​ 2）采用多字段 例如：在买房合同中，标注第一买受人，第二买受人，可顺位增加冗余的字段位 ​ 3）使用常用的桥接表 例如：订单中，确定父订单为粒度，建立父订单与子订单的桥接表，另建立子订单与商品的维度关系表，桥接表包含事实表关联的分组KEY，以及作为买受方的维度表外键ID，有多个买受方，则有在相同的KEY下有多条记录，桥接表需要更多的计算，也可能会造成双重计算，例如，买受人1籍贯为山东，买受人2籍贯为浙江，那么当分别统计外地山东的购房数，和浙江的购房数，则产生了多重计算。双重计算不一定是错误，对于一些业务需求是合理的，但对于另一些业务需求，则需要规避。 ​ 理解关键字：当前大数据平台支持复杂的数据结构，将多个买受人可以作为数据组结构存入一个字段中，使用key:value的形式，当然此种方法需增加一定的计算成本，属于非规范化操作。 杂项维度（junk dimension）​ 杂项维度就是一种包含的数据具有很少可能值的维度​ 当定义好各种维度后，会发现一些小范围维度取离散值或者标志位的字段，但是这样的维度又很难退化存储在事实表中，可能会造成事实表过大，若果单独建立维度表进行关联，通过外键关联，会出现维度过多的情况，如果将这些字段删除，则业务方不同意。 这时，我们通常的解决方案就是建立杂项维度，将这些字段建立到一个维度表中，在事实表中只需保存一个外键。几个字段的不同取值组成一条记录，生成代理键，存入维度表，并将该代理键保存入相应的事实表字段。建议不要直接使用所有的组合生成完整的杂项维度表，在抽取时遇到新的组合时生成相应记录即可。杂项维度的ETL过程比一般的维度略为复杂。 几个字段的不同取值组成一条记录，生成代理键，存入维度表： 即将几个杂项维度的组合建立杂项维度表，生成一个代理键，植入事实表中 微型维度 理解关键字2： ​ 第一：避免维度过度增长 某些维度值变化过高，如果维度表使用了拉链极限存储，那边过度增长或者变化的维度，将使得极限存储效率差 ​ 第二：避免耦合度过高 例如：卖家的主营项目，加工逻辑异常复杂，如果融合进现有的卖家维表中，那么过多的业务耦合将会导致卖家维度难以维护，应适当做维度拆分]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x1 数据仓库</category>
        <category>1x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(4)-维度建模]]></title>
    <url>%2F2018%2F12%2F28%2Fdim-modeling%2F</url>
    <content type="text"><![CDATA[1、什么是维度建模？理解关键字：维度建模的出发点是实现快速的数据分析与决策，维度建模通常面向业务人员、分析人员使用，相对ER建模来说会更加开放，更容易理解 维度建模是从业务过程中提炼而来，典型维度建表代表星形建模和雪花建模 2、维度建模一般过程（摘自《大数据之路》 &amp; 《离线和实时大数据之战》）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kimball四步建模流程适合上述数据仓库系统建设流程中模型设计环节，重点解决数据粒度、维度设计和事实表设计问题。 （1）选择业务过程 ​ 业务过程描述的是企业的业务活动，例如电商系统用户下单，账号系统用户注册了一个账号 （2）选择粒度 ​ 那么什么是粒度呢？粒度是描述的是活动细节的每一个子项，例如，用户注册一个账号，包含了用户名、时间、IP、终端类型、手机号码，帐号类型等 （3）确定维度 ​ 确定维度之前，需要先明确什么是维度？维度是表示业务活动在不同角度的表现方式，例如：注册账号的活动中，从时间、IP、终端类型等角度进行描述 ​ 比如：用户名、注册时间、注册IP、注册终端、注册手机号码、注册账号类型（手机账号、邮箱账号、三方账号） （4）确定事实 ​ 确定维度之后，事实则变的清晰，比如：我们需要计算某终端、某天的注册量 那么问题来了？ 1、如何确定粒度？ ​ 最细粒度和聚合粒度之争？ 到底是选择最细粒度，还是聚合粒度？ 2、如何确定维度？ ​ 标识维度解决的是业务人员如何描述来自业务过程的数据，维度用来表示“谁、什么、何时、何处、为何、如何”的问题。以竞价广告检索流程而言就是客户通过什么渠道、什么样的客户端（OS、IP）、检索了什么样的内容、请求最终有谁受理等。 3、如何确定事实？ ​ 标识事实其实是在确定业务过程的度量指标，指标何来？哪些指标必须保留，那些指标必须删除，待定指标如何处理？必须综合考虑业务用户需求和现实数据的实际情况。事实表的设计完全依赖于物理活动，不受可能产生的最终报表的影响，报表只是事实表设计的参考视角。 ​ 指标必需要考虑业务的需求和数据的实际情况 3、雪花模型 与 星形模型理解关键字：时间与空间交换 （1）雪花模型是充分展开维度，当出现维度层次时，例如：某商品分大类、种类、小类，使用雪花模型建模时，通常将维度表中记录小类的ID，额外设计一张类别的维度表 雪花模型去除了数据冗余，节省了部分存储，但也带来了一定的不变，使得业务人员分析查询时需要关联多张维度表。 （2）星系模型则是允许一定的冗余，将维度层次扁平化，通过牺牲一小部分空间换取查询时快捷 PS:某些情况下可以需要用到雪花模型]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x1 数据仓库</category>
        <category>1x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
        <tag>维度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(3)-ER建模]]></title>
    <url>%2F2018%2F12%2F27%2Fer-modeling%2F</url>
    <content type="text"><![CDATA[1、什么是ER建模？理解关键字：ER模型又称实体-关系模型，遵循3NF建模，采用ER进行数据仓库建模需要从整个企业的角度理清各业务之间的关系，建模的出发点是基于企业数据的整合，建设EDW需要建模人员对企业整体业务有精深的把控 例如：Teradata的 FS-LDM模型，将金融业务分为10大主题，通常是对整体行业发展的沉淀，将成熟的模型做适当的调整即可快速落地实施 2、ER建模的三个阶段理解关键字：ER模型是自上而下的建模方式，由此建模的需要先从企业的整体框架进行高度抽象 （1）高层模型​ 一个高度抽象的模型，描述主要的主题及主体间的关系，用于描述企业的总体情况（2）中层模型​ 在高层模型的基础上，细化主题域的实体、关系等数据项（3）物理模型​ 在中层模型的基础上，通常会根据平台的特性进行物理属性设计，例如：分表、分区、分区索引等设计理解关键字2：先通过整合整个企业的业务关系，划分相应主题域及对应的主题之间的关系，再细化每个主题域中的内容，最后是根据平台的特点进行物理设计]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x1 数据仓库</category>
        <category>1x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
        <tag>ER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(2)-数据建模]]></title>
    <url>%2F2018%2F12%2F26%2Fdata-modeling%2F</url>
    <content type="text"><![CDATA[1、为什么需要数据建模？理解关键字：数据建模就是数据组织和存储的方法数据模型是数据组织和存储方法，正如我们希望图书馆的书分门别类的放置。数据模型强调从业务、数据存取和使用角度合理规划数据 那么为什么需要数据建模呢？我们可以从建模有哪些好处理解 （1）性能 良好的模型能帮助我们快速查询所需要的数据，减少不必要的数据查询I/O（2）成本 良好的数据模型能够极大地减少不必要的数据存储，也能实现计算结果的复用，极大地降低大数据系统中的存储和计算成本（3）效率 良好的数据模型能够极大地提高用户体验，用户可通过业务沉淀后的模型尽可能减少不必要的查询，从而提高效率（4）质量 良好的数据模型将遵循一定的数据规范（例如：口径约束，字段约束等）从而减少数据计算的失误，保障数据质量 2、OLTP与OLAP的区别理解关键字： OLTP 是讲究事务，在OLAP中事务不是所关注的，主要是批量的读写，致力于联机分析 一个是讲究快速响应，一个是讲究大吞吐OLTP通常使用ER模型，采用三范式消除数据中的冗余，而OLAP采用的模型则比较丰富 3、有哪些建模方法论？（1）ER建模（2）维度建模（3）Data vault建模（4）Anchor建模]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x1 数据仓库</category>
        <category>1x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive SQL优化]]></title>
    <url>%2F2018%2F12%2F25%2Fsql-optimization%2F</url>
    <content type="text"><![CDATA[0x00 group by 引起的倾斜1|优化措施12set hive.map.aggr = trueset hive.groupby.skewindata = true 2|原理​ 作业将生成两个MapReduce Job ​ 第一个MapReduce Job中 Map的输出结果会随机的分布到Reduce中，每个Reduce做部分聚合操作并输出结果，这样处理的结果 相同的group by key会分布到不同的reduce中 ​ 第二个MapReduce Job中 再根据 相同group by key分布到reduce中 0x01 count distinct 优化1|例子1select count(distinct ssn) from table 2|说明​ 由于如上SQL，Hive会将Map阶段的输出全部分布到一个Reduce中，很容易引起性能问题 3|正确的操作方法12345select count(1) from ( select ssn from table group by ssn) t 12-- 原理-- 利用group by 先进行数据去重，然后再进行计数，计数是不可避免的，目的是为了大量减少重复计数 0x02 大表join小表1|方法​ 一般都是采用 map join的方法，默认Hive是开启 mapjoin，其Hive设置参数如下： 1set hive.auto.convert.join = true; ​ 或者是使用Hint的方式进行小表指定 2|案例12345678select /*+mapjoin(t1)*/ t0.ssn ,t1.namefrom big_table0 t0inner join small_table1 t1on t0.ssn = t1.ssn;-- /*+mapjoin(t1)*/ 即 map join hint，如果需要mapjoin多个表，则格式为 /*+mapjoin(t1,t2,t3)*/ 0x03 大表join大表(待补充)​ 1|问题场景 ​ 2|解决方法1 ​ 3|解决方法2 0x04 优化案例1|业务背景：​ 筛选出 当日 注册产品、注册IP为维度下超过30个帐号的 帐号清单 2|常规做法​ 先筛选出满足 注册产品、注册IP 数量超过30个的 注册产品及注册IP的维度清单，然后再关联源表进行筛选出帐号 1234567891011121314151617181920212223242526272829303132333435363738394041create table tmp.tmp_algorithm1 asSELECT t1.ssn, 1 AS rubbish_rateFROM ( SELECT dt, ssn, reg_ip, reg_product FROM dw.dw_user_static_reg_info_dd WHERE dt = '2018-11-22' AND ( split(ssn, '@') [1] IN ('126.com', 'yeah.net', 'vip.126.com', 'vip.163.com') OR split(ssn, '@') [1] IS NULL ) ) t1 JOIN ( SELECT dt, reg_ip, reg_product, count(*) AS cnt FROM dw.dw_user_static_reg_info_dd WHERE dt = '2018-11-22' AND ( split(ssn, '@') [1] IN ('126.com', 'yeah.net', 'vip.126.com', 'vip.163.com') OR split(ssn, '@') [1] IS NULL ) GROUP BY dt,reg_ip, reg_product HAVING cnt &gt;= 30 ) t2 ON t1.dt=t2.dt and t1.reg_ip = t2.reg_ip AND t1.reg_product = t2.reg_productGROUP BY t1.ssn; 3|优化做法​ 使用窗口函数进行维度内计数，如下通过窗口函数计算在 维度 注册产品、注册IP下的数量，通过子查询进行筛选出目标帐号清单 12345678910111213141516create table tmp.tmp_algorithm2 asselect ssn,1 as rubbish_ratefrom(select ssn ,reg_ip ,reg_product ,count(1) over(partition by reg_ip,reg_product) as cnt from dw.dw_user_static_reg_info_ddwhere dt = '2018-11-22' and ( split(ssn, '@') [1] in ('126.com', 'yeah.net', 'vip.126.com', 'vip.163.com') or split(ssn, '@') [1] is null )) twhere cnt &gt;= 30; 4|优化思路​ 由于HQL 是转换为MapReduce的形态进行数据计算，常规做法中会启用大量的MapReduce，并重复的抽取了源表数据，浪费了计算，优化做法是实现一份数据提取，复杂内存计算，从而减少使用计算资源]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x2 技术平台</category>
        <category>1x21 Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>SQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(1)-数仓概论]]></title>
    <url>%2F2018%2F12%2F20%2Fedw-brief-summary%2F</url>
    <content type="text"><![CDATA[通常来讲，大家都知道数据仓库的官方概念数据仓库概念创始人W.H.Inmon在《建立数据仓库》一书中对数据仓库的定义是：数据仓库就是面向主题的、集成的、相对稳定的、随时间不断变化（不同时间）的数据集合，用以支持经营管理中的决策制定过程、数据仓库中的数据面向主题，与传统数据库面向应用相对应。 0x1 数据仓库有哪些点？能看到那些面? 很多数据开发人员，无法将数据仓库的点连成线，更别说由线织成面？这里我们进行一些头脑风暴，看看数据仓库都有哪些内容？ 总的来说，数据仓库可以整理如下4块内容：（1）数据建模方法论（2）实施方法论（最佳实践）（3）数据管理（4）数据应用 每一块内容拿出来讲，都是无边界的，要想成为这一领域的专家，需要有数据仓库的全局观并且有某一领域的最佳实践。数仓之路，任重而道远！！！]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x1 数据仓库</category>
        <category>1x11 数据建模</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用fuse_dfs快速建立hdfs云存储]]></title>
    <url>%2F2018%2F08%2F16%2Ffuse-hdfs%2F</url>
    <content type="text"><![CDATA[一、介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将HDFS挂载到本地，允许用户像访问本地文件系统一样访问远程文件系统，用户可像读写本地文件一样读写HDFS上的文件，大大简化了HDFS使用，方法较为常用两种方法是：利用fuse_dfs或者利用NFSv3，因NFSv3需对Hadoop集群端进行操作，修改代价太大，本文介绍以fuse_dfs，只需在客户端进行配置即可挂载hdfs。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户空间文件系统（Filesystem in Userspace，简称FUSE）是一个面向类Unix计算机操作系统的软件接口，它使无特权的用户能够无需编辑内核代码而创建自己的文件系统。目前Linux通过内核模块对此进行支持。一些文件系统如ZFS、glusterfs和lustre使用FUSE实现。Linux用于支持用户空间文件系统的内核模块名叫FUSE，FUSE一词有时特指Linux下的用户空间文件系统。通过使用Hadoop的Fuse-DFS分类模块，任意一个Hadoop文件系统(不过一般为HDFS)都可以作为一个标准文件系统进行挂载。Hadooop源码中自带了fuse-dfs模块，利用fuse_dfs将hdfs当一个网盘挂载到本地。 二、应用场景&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同步：实现用户对个人计算指定目录下的特定类型的文件自动与HDFS同步，实时监测用户文件变化情况。不需要用户干预就能及时自动备份文件。当用户新建、修改或删除文件（目录）时自动更新同步，保障用户重要文件的安全。当用户移动办公时，可直接从HDFS访问，省略了移动存储介质的拷贝过程，提高用户工作效率。更换新计算机时，用户登录后原计算机中的个人文件将自动从HDFS存储中同步到新计算机中。例如：适用数据仓库需要ETL进行Hadoop文件进行ftp或者rsync推送的场景。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网盘：用户可将不常用的文件存储在HDFS存储中，从而节约本地存储空间，提高文件的物理安全性，也便于资源共享。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;共享：用户可以把网盘或同步盘的任意文件（目录）分享给群组或其他用户。只需要在云存储中存储一份文件，被分享的人不需要下载存储即可在线浏览，实现资源共享。若源文件被修改，所有分享人立即会得到更新文件。 三、部署实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前网络上关于利用fuse挂载hdfs较多基于较早版本的hadoop(0.x，1.x) ，并大多都基于无kerberos认证，本文以北京亦庄Hadoop 2.5.2为例，介绍通过fuse挂载hdfs到本地的方法。设定源码目录为$HADOOP_SRC_HOME，那么Hadoop 2.x 的 fuse_dfs 源码位于1$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么怎么编译fuse_dfs？fuse_dfs是Hadoop自带模块，编译Hadoop时，增加编译参数require.fuse=true，即编译命令如下1mvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar 编译环境123456789Linux: 3.16.0-0.bpo.4-amd64 (Debian7)JDK: 1.7.0_80 64bitApache Hadoop: 2.5.2 Ant: 1.8.2GCC: 4.7.2 (系统默认)Protocbuf 2.5.0Maven 3.0.4Cmake 2.8.9Make 3.8.1 安装fuse等依赖1sudo apt-get -y install maven build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev libfuse-dev 编译fuse_dfs(Hadoop)123456789101112# 下载mkdir package &amp;&amp; cd packagewget http://archive.apache.org/dist/hadoop/core/hadoop-2.5.2/hadoop-2.5.2-src.tar.gz#解压tar -zxvf hadoop-2.5.2-src.tar.gzln -s hadoop-2.5.2-src hadoop-srcecho "export HADOOP_SRC_HOME=$HOME/package/hadoop-src/" &gt;&gt; .bashrc &amp;&amp; source .bashrc# 编译cd hadoop-2.5.2-srcmvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar PS: 编译Hadoop之前需提前安装好fuse，否则fuse_dfs无法编译成功123456# 编译后的fuse_dfs位于：$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs/fuse_dfs# 完整的Hadoop包则位于：$HADOOP_SRC_HOME/hadoop-dist/target/hadoop-2.5.2.tar.gz# 一个用于挂载hdfs的脚本：$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh 部署Hadoop（略）配置、fuse_dfs_wrapper.shHDFS挂载主要依赖源码的两个模块，一个是hadoop-hdfs-project，另一个是hadoop-client，可将依赖的jar包单独提炼出来。 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env bashexport HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/local/share/hadoop&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport HADOOP_PREFIX=$HOME/package/hadoop-srcexport JAVA_HOME=$HOME/opt/jdkexport FUSEDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/main/native/fuse-dfs"export LIBHDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/target/usr/local/lib"if [ "$OS_ARCH" = "" ]; then export OS_ARCH=amd64fiif [ "$JAVA_HOME" = "" ]; then export JAVA_HOME=/usr/local/javafiif [ "$LD_LIBRARY_PATH" = "" ]; thenexport LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/usr/local/libfiJARS=`find "$HADOOP_PREFIX/hadoop-hdfs-project" -name "*.jar" | xargs`for jar in $JARS; do CLASSPATH=$jar:$CLASSPATHdoneJARS=`find "$HADOOP_PREFIX/hadoop-client" -name "*.jar" | xargs`for jar in $JARS; do CLASSPATH=$jar:$CLASSPATHdoneexport CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATHexport PATH=$FUSEDFS_PATH:$PATHexport LD_LIBRARY_PATH=$LIBHDFS_PATH:$JAVA_HOME/jre/lib/$OS_ARCH/server:$LD_LIBRARY_PATHfuse_dfs $@ 利用命令 1234567891011121314 sudo bash ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://$&#123;nn&#125;:$&#123;port&#125; /mnt/hdfsINFO /home/hzxijingjing/package/hadoop-2.5.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c:164 Adding FUSE arg /mnt/hdfs# 挂载信息 df -hFilesystem Size Used Avail Use% Mounted onrootfs 50G 8.2G 39G 18% /udev 10M 0 10M 0% /devtmpfs 1.6G 216K 1.6G 1% /run/dev/vda1 50G 8.2G 39G 18% /tmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 3.2G 0 3.2G 0% /run/shm/dev/vdb1 99G 75M 94G 1% /datafuse_dfs 49G 0 49G 0% /mnt/hdfs # HDFS挂载成功 ​ 至此为止，已实现无简单认证的Hadoop挂载，然而对于配置了kerberos认证的Hadoop，实施挂载会出现如下情况：123cd /mnt &amp;&amp; ls -lhls: cannot access hdfs: Transport endpoint is not connectedd????????? ? ? ? ? ? hdfs 后置参数 -d，进入debug模式， 查看运行信息 1sudo /bin/sh ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://$&#123;nn&#125;:$&#123;port&#125; /mnt/dfs -d 1234567891011121314151617181920212223242526272829............ # 省略一些无用日志 FUSE library version: 2.9.0nullpath_ok: 0nopath: 0utime_omit_ok: 0unique: 1, opcode: INIT (26), nodeid: 0, insize: 56, pid: 0INIT: 7.23flags=0x0003fffbmax_readahead=0x00020000INFO /home/hzxijingjing/package/hadoop-2.5.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c:98 Mounting with options: [ protected=(NULL), nn_uri=hdfs://hadoop1232.hz.163.org:8020, nn_port=0, debug=0, read_only=0, initchecks=0, no_permissions=0, usetrash=1, entry_timeout=60, attribute_timeout=60, rdbuffer_size=10485760, direct_io=0 ]............ # 省略一些无用日志 fuseConnectInit: initialized with timer period 5, expiry period 300 INIT: 7.18 flags=0x00000039 max_readahead=0x00020000 max_write=0x00020000 max_background=0 congestion_threshold=0 unique: 1, success, outsize: 40unique: 2, opcode: STATFS (17), nodeid: 1, insize: 40, pid: 13769statfs / unique: 2, success, outsize: 96unique: 3, opcode: GETATTR (3), nodeid: 1, insize: 56, pid: 13815getattr /fuseNewConnect: failed to find Kerberos ticket cache file '/tmp/krb5cc_0'. Did you remember to kinit for UID 0?fuseConnect(usrname=root): fuseNewConnect failed with error code -13fuseConnectAsThreadUid: failed to open a libhdfs connection! error -13. unique: 3, error: -5 (Input/output error), outsize: 16unique: 4, opcode: STATFS (17), nodeid: 1, insize: 40, pid: 13826 从上可以看出，大概是无法通过kerberos的cache文件/tmp/krb5cc_0认证，早就意识到基于kerberos的hadoop是块难啃的骨头，用过kerberos认证的同学都知道，kerberos认证依赖本地Linux命令 kinit 实现认证，认证之后通常会有一个有效期，即在一段时间内，无需再次认证。 然而利用fuse_dfs挂载kerberos认证的HDFS，无论是度娘还是谷爸在这方面资料也是极少，无奈只能看下fuse_dfs的源码。 fuse-dfs实现​ fuse_dfs是由C语言开发，代码目录位于$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/（上文有提到）​ 如下是fuse_dfs连接hdfs及操作相关源码文件 12./fuse-dfs/fuse_connect.c./libhdfs/hdfs.c vim fuse_connect.c利用libhdfs建立连接123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111 /**函数主体中一段代码表示如何通过libhdfs连接*//** * Create a new libhdfs connection. * * @param usrname Username to use for the new connection * @param ctx FUSE context to use for the new connection * @param out (out param) the new libhdfs connection * * @return 0 on success; error code otherwise */static int fuseNewConnect(const char *usrname, struct fuse_context *ctx, struct hdfsConn **out) /** * Find out what type of authentication the system administrator * has configured. * * @return the type of authentication, or AUTH_CONF_UNKNOWN on error. */ //获取Hadoop认证方式 static enum authConf discoverAuthConf(void)&#123; int ret; char *val = NULL; enum authConf authConf; ret = hdfsConfGetStr(HADOOP_SECURITY_AUTHENTICATION, &amp;val); if (ret) authConf = AUTH_CONF_UNKNOWN; else if (!val) authConf = AUTH_CONF_OTHER; else if (!strcmp(val, "kerberos")) authConf = AUTH_CONF_KERBEROS; else authConf = AUTH_CONF_OTHER; free(val); return authConf;&#125;// 获取到Hadoop的认证方式之后，首先找kerberos认证后的cache文件if (gHdfsAuthConf == AUTH_CONF_KERBEROS) &#123; findKerbTicketCachePath(ctx, kpath, sizeof(kpath)); if (stat(kpath, &amp;st) &lt; 0) &#123; fprintf(stderr, "fuseNewConnect: failed to find Kerberos ticket cache " "file '%s'. Did you remember to kinit for UID %d?\n", kpath, ctx-&gt;uid); ret = -EACCES; goto error; &#125;// 查找kerberos认证后的cache文件，从发现cache文件位于：/tmp/krb5cc_%d/** * Find the Kerberos ticket cache path. * * This function finds the Kerberos ticket cache path from the thread ID and * user ID of the process making the request. * * Normally, the ticket cache path is in a well-known location in /tmp. * However, it's possible that the calling process could set the KRB5CCNAME * environment variable, indicating that its Kerberos ticket cache is at a * non-default location. We try to handle this possibility by reading the * process' environment here. This will be allowed if we have root * capabilities, or if our UID is the same as the remote process' UID. * * Note that we don't check to see if the cache file actually exists or not. * We're just trying to find out where it would be if it did exist. * * @param path (out param) the path to the ticket cache file * @param pathLen length of the path buffer */static void findKerbTicketCachePath(struct fuse_context *ctx, char *path, size_t pathLen)&#123; FILE *fp = NULL; static const char * const KRB5CCNAME = "\0KRB5CCNAME="; int c = '\0', pathIdx = 0, keyIdx = 0; size_t KRB5CCNAME_LEN = strlen(KRB5CCNAME + 1) + 1; // /proc/&lt;tid&gt;/environ contains the remote process' environment. It is // exposed to us as a series of KEY=VALUE pairs, separated by NULL bytes. snprintf(path, pathLen, "/proc/%d/environ", ctx-&gt;pid); fp = fopen(path, "r"); if (!fp) goto done; while (1) &#123; if (c == EOF) goto done; if (keyIdx == KRB5CCNAME_LEN) &#123; if (pathIdx &gt;= pathLen - 1) goto done; if (c == '\0') goto done; path[pathIdx++] = c; &#125; else if (KRB5CCNAME[keyIdx++] != c) &#123; keyIdx = 0; &#125; c = fgetc(fp); &#125;done: if (fp) fclose(fp); if (pathIdx == 0) &#123; snprintf(path, pathLen, "/tmp/krb5cc_%d", ctx-&gt;uid); // cache文件的目录 &#125; else &#123; path[pathIdx] = '\0'; &#125;&#125; ​ 由上代码可以看出，函数findKerbTicketCachePath从上下文ctx中获取uid从而获取/tmp/krb5cc_${uid}，那么如何挂载hdfs时，识别到kerberos的cache文件呢？ vim hdfs.c123456789101112131415161718192021struct hdfsBuilder &#123; int forceNewInstance; const char *nn; tPort port; const char *kerbTicketCachePath; const char *userName;&#125;; #define KERBEROS_TICKET_CACHE_PATH "hadoop.security.kerberos.ticket.cache.path"if (bld-&gt;kerbTicketCachePath) &#123; jthr = hadoopConfSetStr(env, jConfiguration, KERBEROS_TICKET_CACHE_PATH, bld-&gt;kerbTicketCachePath); if (jthr) &#123; ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL, "hdfsBuilderConnect(%s)", hdfsBuilderToStr(bld, buf, sizeof(buf))); goto done; &#125;&#125; 最终定位到：KERBEROS_TICKET_CACHE_PATH “hadoop.security.kerberos.ticket.cache.path” 在core-site.xml添加如下属性： 12345&lt;property&gt; &lt;name&gt;hadoop.security.kerberos.ticket.cache.path&lt;/name&gt; &lt;value&gt;/tmp/krb5cc_$&#123;uid&#125;&lt;/value&gt; &lt;description&gt;Path to the Kerberos ticket cache. &lt;/description&gt;&lt;/property&gt; ${uid}是什么？uid=7765 1234567kinit urs.keytab ursTicket cache: FILE:/tmp/krb5cc_7765Default principal: urs@HADOOP.HZ.NETEASE.COMValid starting Expires Service principal10/05/2018 10:52 10/05/2018 20:52 krbtgt/HADOOP.HZ.NETEASE.COM@HADOOP.HZ.NETEASE.COM renew until 11/05/2018 10:52 最后重新挂载，操作手法同上。 大功告成!]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x2 技术平台</category>
        <category>1x20 Hadoop</category>
      </categories>
      <tags>
        <tag>fuse</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用LaTeX写数学公式]]></title>
    <url>%2F2018%2F08%2F16%2Flatex-math-formula%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;最近一直学习 线性代数，通过latex来写数学公式，替代了使用截图的方式来写博客，当然直接书写一个数学公式也是有一定难度，由此我们需要不断的积累，下文将简单介绍使用LaTeX书写矩阵，更多的数学公式可以参考LaTeX官方文档。 LaTeX 写简单Matrix语法 使用\$\$begin{matrix}…\end{matrix}$$来生成矩阵，其中… 表示的是LaTeX 的矩阵命令，矩阵命令中每一行以 \ 结束，矩阵的元素之间用&amp;来分隔开。 例如: 写一个简单矩阵 1234567$$ \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125; \tag&#123;1&#125;$$ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{matrix} \tag{1} 带括号的Matrix 感觉(1)中的矩阵不是很美观，可以给矩阵加上括号，加括号的方式有很多，大致可分为两种：使用\left … \right 或者把公式命令中的matrix 改成 pmatrix、bmatrix、Bmatrix、vmatrix、Vmatrix等。 1234567$$ \begin&#123;pmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;pmatrix&#125; \tag&#123;2&#125;$$ \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} \tag{2}1234567$$ \begin&#123;bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;bmatrix&#125; \tag&#123;3&#125;$$ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \tag{3}1234567$$ \begin&#123;Bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;Bmatrix&#125; \tag&#123;4&#125;$$ \begin{Bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{Bmatrix} \tag{4}1234567$$ \begin&#123;vmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;vmatrix&#125; \tag&#123;5&#125;$$ \begin{vmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{vmatrix} \tag{5}1234567$$ \begin&#123;Vmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;Vmatrix&#125; \tag&#123;6&#125;$$ \begin{Vmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{Vmatrix} \tag{6}增广矩阵 12345678$$ \left[ \begin&#123;array&#125;&#123;cc|c&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end&#123;array&#125;\right] \tag&#123;7&#125;$$ \left[ \begin{array}{cc|c} 1 & 2 & 3 \\ 4 & 5 & 6 \end{array} \right] \tag{7}组合矩阵 12345678$$ \left[ \begin&#123;pmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end&#123;pmatrix&#125;\right] \tag&#123;7&#125;$$ \left[ \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix} \right] \tag{8}带省略符号的Matrix\ddots 表示： ⋱ 123456789$$ \left[ \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; \ddots &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125;\right] \tag&#123;9&#125;$$ \left[ \begin{matrix} 1 & 2 & 3 \\ 4 & \ddots & 6 \\ 7 & 8 & 9 \end{matrix} \right] \tag{9}\vdots 表示： ⋮ 12345678$$ \left[ \begin&#123;matrix&#125; 1 &amp; \vdots &amp; 3 \\ 4 &amp; \vdots &amp; 6 \end&#123;matrix&#125;\right] \tag&#123;10&#125;$$ \left[ \begin{matrix} 1 & \vdots & 3 \\ 4 & \vdots & 6 \end{matrix} \right] \tag{10}\cdots 表示： ⋯ 12345678$$ \left[ \begin&#123;matrix&#125; 1 &amp; \cdots &amp; 3 \\ 4 &amp; \cdots &amp; 6 \end&#123;matrix&#125;\right] \tag&#123;11&#125;$$ \left[ \begin{matrix} 1 & \cdots & 3 \\ 4 & \cdots & 6 \end{matrix} \right] \tag{11}更多LaTeX语法请点击这里 e=mc^2 x^y = (1+e^x)^-2xy^w lim_{1\to+\infty}P(|\frac{1}{n}\sum_i^nX_i-\mu|]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x4 办公技能</category>
        <category>1x40 MarkDown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库ETL之数据交换]]></title>
    <url>%2F2017%2F06%2F05%2Fedw-data-exchange%2F</url>
    <content type="text"><![CDATA[​ 从1990年数据仓库之父比尔·恩门（Bill Inmon）提出数据仓库的概念，ETL作为数据仓库的核心组件，在传统的数据仓库中是服务于数据采集，数据处理，大数据时代来临，对ETL的理解也由【抽取、转换、加载】升级到【交换】这个层面。如果你也考虑建设企业级数据仓库可以作为参考。 0x0 ETL之定位业内多数知名数据仓库解决方案提供方利用公司的自我研发的ETL工具及数据平台从生产系统接入数据源，在库内进行数据转换工作，与其他三方厂家（乙方）一样，自顾一亩三分地，完成了端对端的数据支撑，然而对于甲方而言，各个厂家的端的数据采集无疑是对生产系统的增加多倍压力，另外厂家之间的数据交换也是常态。 建设数据仓库的模式有由上而下以及由下而上两种模式，对于大企业而言，各个部门的运营模式、业务方向均有千差万别，部门大多建设自己的数据仓库，公司从各个部门建设的数据仓库之上建立总的数据仓库。在我们公司，部门之间数据ETL，一方面因业务模式，另一方面各部门数据平台产品多样化，ETL大多采用ftp、rsync这种文件数据传输方式，日志采集则使用DataStream或flume等。我们通常面临企业之间的数据交互，政府部门的数据推送，即便是一个部门内部数据的交换也是常态，数据共享有利于大数据生态发展，离线平台需要生产系统的数据进行计算，生产系统也需要离线平台的数据支撑。数据交换作为ETL一个升级模式成为企业级数据仓库建设的必然。 0x1 交换平台的产生大多大数据从业人士也认识到数据交换，很多厂家也认识到了ETL模式的与时俱进，阿里的数据工场，亚信的云化交换平台，华为的云化交换平台以及我们的猛犸大数据平台应运而生，大数据交换平台服务于企业（部门、系统）数据交换，数据交换对于用户而言都是透明的，不仅满足传统的ETL功能，同时需要满足动态扩展。大数据形势下，互联网数据仓库较之传统的数据仓库平台有如下特点： 为适应数据仓库的发展，那么数据交换平台应该具备哪些能力呢？小编认为可从如下几个方面进行阐述，数据采集能力，数据存储能力，数据管理能力，数据计算能力，平台开放能力，作业调度能力。 (1) 数据采集能力交换平台应支持对表（关系表，k-v表）、文件（结构化、非结构化）、消息、数据流等多种数据形式的支持，同时需支持多样化的采集方式：实时增量流式（flume、DataStream等），消息队列（nsq、Kafaka）等主流技术。由于技术繁杂，种类繁多，选取适合自己业务模式的工具形成组件化利于跨平台部署，这是数据采集应考虑的。（PS：实际上对于日志流式收集的形式通常是效率高，高响应，但也处于不稳定的，易丢失数据等现状。） (2) 数据存储能力互联网的数据【大】是毋庸置疑的事实，传统RDBMS的核心设计思想基本上是30年前形成的，在数据仓库领域有相当权威的公司有Teradata、IBM、Oracle、SAP等这里公司均拥有自身数据存储系统以及软硬一体化的大数据解决方案，支持动态扩展的能力，Teradata、Aster Data、GreenPlum都是MPP存储架构，支持PB级别存储（前两者属于商业数据库，GreenPlum开源），这些均是关系型数据库，我们最熟悉的Hadoop是属于SMP架构，支持非结构化、半结构化、结构化数据存储，数据存储平台的选型根据企业的业务模式选定。 (3) 平台管理能力数据管理：数据的交换忌讳数据冗余，浙江移动大数据中心使用亚信云化交换平台出现了这样的一个场景，厂家A需要生产系统中数据表T1部分字段,，厂家B需要生产系统中数据表T1的所有字段，另有特殊情景长家C需要表中的字段与厂家A有交叉字段现象，生产系统为给三个厂家同时供数，造成了大量数据冗余，空间有效利用率60%不到。 多租户管理：数据交换本身的一个概念就是交换，意味着多租户，平台是开放的，面向多个部门甚至多个企业，租户可以灵活创建自己的作业体系，提出相应的数据需求，进行自主管理和配置，跨系统或者跨部门的数据请求开放给用户，开启有效的需求管理体系。 作业管理：ETL平台所需维护的细节繁多，如果交互的接口管理一塌糊涂，比如繁多的FTP、散乱的Shell脚本、crontab定时任务搞晕了运维人员，一旦人员迭代，付出的管理成本很大，建立一套完善的元数据管理体系是保障系统正常运作必需的工具。 (4) 数据计算能力 互联网的数据“大”是不争的事实，现在分析一下数据处理技术面临的挑战，近几年见过形形色色的数据仓库，大多使用的自下而上的方式建设，生态数仓中所使用的平台产品多样性，然而传统行业电信也好，银行也好，互联网产品数仓也罢，鲜有PB级别的数仓，面对的用户量几千万，对应的数据是几百TB，数据量少怎么样的建设方案都是无所谓的，当面对10亿级别的用户，PB级别的数据时，如何满足数据计算？如何适应多种数据平台的接入？ 数据交换平台需具备强大的数据计算能力，支持常见的分布式计算模型，以实现数据的预处理（格式转换【例如lzo转换成HFile】，空值处理、数据压缩等）以及常见的计算函数（窗口函数，中位数计算、时序计算、图计算等）。 (5) 平台开放能力能够对外输出数据，这是企业级的交换平台所应具备的基本能力，数据开放并不意味着谁都可以随意获取数据，放弃数据安全，数据开放表示数据共享，满足企业的各项业务需求，根据部门的具体需求，申请已开放的数据，数据安全保障。 数据交换平台所应支持的不仅是传统的数据开放，更应具有元数据开放、计算能力的开放，丰富的API提供，既然无法满足所有的个性化需求，那就应有开放的API为其他系统无缝衔接，单纯的数据导出已无法满足业务的发展。 (6) 作业调度能力调度功能是从ETL工具形成初便拥有的，ETL调度工具发展也从最初的crontab衍生到现在的调度平台，业内企业级的调度工具例如国外的Informatica公司的Informatica、Teradata的 ETL Automation、微软的 SSIS、Control-M，国内的有自主研发阿里的Zeus，亚信的 AI-CLOUD-ETL，华为研发的云-ETL，网易的nschedule、猛犸等，开源的调度工具有Azkaban，Kettle等。 调度模式是各色各样，有以工作流调度单位，有以作业为调度单位，所支持的作业形式也是多种，脚本形式(shell、python、Perl，ruby)，SQL、Java、MapReduce以及其他组件形式。调度策略也是丰富多样，有时间触发、时间触发、接口通知、手工执行。 实现任务的统一调度，支持动态扩展的作业类型，丰富的调度模式是数据交换平台应具备的。以上所列举的6项能力提出一些常见的体现，构建数据交换平台所考虑的细节非常多。 0x3 交换平台建设最佳实践很多数据平台提供的是纯粹的数据服务，较少提供Api或元数据服务以满足其他平台的嵌入，业内打造成PaaS级的ETL交换平台屈指可数，技术因素是一方面，业务需求是否需要打造这样一个平台也是其中一个原因，对业务需求的理解，是否具有持续的维护及更新能力又是另一方面。还有一句话是这么说的，规则与灵活怎么取舍？为便于维护可能将主流的需求满足，对于一些个性化的需求则被抛弃，很多公司技术也很强，但打造这样一个ETL交换平台更多的考虑是从用户需求中来，到实践去来，纯粹的为实现功能，大谈性能也未必是一个好的解决方案，业务始终为王。下面设想一个交换平台架构图供参考： 稍作拓展：]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x1 数据仓库</category>
        <category>1x10 数仓概论</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用HFile BulkLoad实现HBase海量数据加载]]></title>
    <url>%2F2016%2F07%2F01%2Fhbase-bulk-load%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据仓库开发过程中，我们可能将HBase作为我们数据即服务的持久化存储介质，由此大量的数据模型从数据仓库计算后写入至HBase。通常我们使用HBase提供的API方法，实现了接口调用，但对于海量的数据，接口的调用引起HBase cpu、内存占用过高，影响正常业务使用。于是我们着手研究HFile BulkLoad的方式进行离线数据加载。 0x0 HBase的存储文件格式：HFile&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HBase实际的存储文件功能是由HFile类实现的，它被专门创建以达到一个目的：有效地存储HBase的数据。它们基于Hadoop的TFile类，并模仿-Google的BigTable架构使用的SSTable格式。曾在HBase中使用过的Hadoop的MapFile类被证明性能不够好。 0x1 BulkLoad原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 Bulk Load 方式由于利用了 HBase 的数据信息是按照特定格式存储在 HDFS 里的这一特性，直接在 HDFS 中生成持久化的 HFile 数据格式文件，然后完成巨量数据快速入库的操作，配合 MapReduce 完成这样的操作，不占用 Region 资源，不会产生巨量的写入 I/O，所以需要较少的 CPU 和网络资源。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bulk Load 的实现原理是通过一个 MapReduce Job 来实现的，通过 Job 直接生成一个 HBase 的内部 HFile 格式文件，用来形成一个特殊的 HBase 数据表，然后直接将数据文件加载到运行的集群中。使用 Bulk Load 功能最简单的方式就是使用ImportTsv 工具，ImportTsv 是 HBase 的一个内置工具，目的是从 TSV 文件直接加载内容至 HBase。它通过运行一个 MapReduce Job, 将数据从 TSV 文件中直接写入 HBase 的表或者写入一个 HBase 的自有格式数据文件。 0x2 结合MapReduce生成HFile0x20 作业的配置1234567891011121314151617181920/** * Job Configure */ Job job = Job.getInstance(conf,"HFileProducerETL"); job.setJarByClass(HFileProducerETL2.class); TableMapReduceUtil.addDependencyJars(job); job.setMapperClass(BulkLoadMapper.class); job.setReducerClass(KeyValueSortReducer.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setOutputValueClass(KeyValue.class); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); HTable htable =new HTable(conf, tablename); HFileOutputFormat2.configureIncrementalLoad(job,htable,htable); return job.waitForCompletion(false) ? 0 : 1; 0x21 Map​ key 类：ImmutableBytesWritable​ value 类：KeyValue 1234567891011121314151617181920212223242526272829303132333435363738394041@Override public void map(LongWritable key, Text value, Context context) &#123; try &#123; String line = value.toString(); String[] all_column_values = line.split(context.getConfiguration().get(FILE_DELIMITER_STR),-1); String rowKeyString = all_column_values[rowKeyIndex]; byte[] rowKey=Bytes.toBytes(rowKeyString); // rowKey 赋值 ImmutableBytesWritable rowKeyWritable=new ImmutableBytesWritable(rowKey); if (all_column_name.length!=all_column_values.length)&#123; return; &#125; for (int i = 0; i &lt; all_column_name.length; i++) &#123; if (export_column_index[i] == EXPORT_FLAG &amp;&amp; !all_column_values[i].trim().equals(HBaseConstant.HiveNULL) &amp;&amp; !all_column_values[i].trim().equals(HBaseConstant.HBaseNull) &amp;&amp; !all_column_values[i].trim().equals(HBaseConstant.HEmpty) )&#123; KeyValue kv ; // 如果是 HFILE_CLEAR_FLAG 则 清空 if (all_column_values[i].equals(HBaseConstant.HFILE_CLEAR_FLAG))&#123; all_column_values[i]=HBaseConstant.HEmpty; &#125; if (timestampIndex == TIMESTAMP_COLUMN_DEFAULT_INDEX)&#123; kv = new KeyValue(rowKey, CF, Bytes.toBytes(all_column_name[i]),Bytes.toBytes(all_column_values[i])); &#125; else&#123; Long timestamp_value = Long.parseLong(all_column_values[timestampIndex]); kv = new KeyValue(rowKey, CF, Bytes.toBytes(all_column_name[i]),timestamp_value,Bytes.toBytes(all_column_values[i])); &#125; context.write(rowKeyWritable,kv); &#125; &#125; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 0x22 BulkLoad​ 生成HFile之后，load相对就比较简单了，代码如下： 123456Configuration conf = DefConfiguration.GetConfiguration(hbaseEnv);conf.setInt("hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily",5000);HTable htable =new HTable(conf, tableName);LoadIncrementalHFiles loader = new LoadIncrementalHFiles(conf);loader.doBulkLoad(hFilePath, htable);]]></content>
      <categories>
        <category>1x 技术综合篇</category>
        <category>1x2 技术平台</category>
        <category>1x22 HBase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
</search>
