<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一起来学Scala(4)-循环]]></title>
    <url>%2F2019%2F01%2F22%2Fscala-programing-loop%2F</url>
    <content type="text"><![CDATA[&ensp;&ensp;&ensp;&ensp;&ensp;今天学习的内容是循环，循环语句允许我们多次执行一个语句或语句组，下面是大多数编程语言中循环语句的流程图： &ensp;&ensp;&ensp;&ensp;&ensp;Scala中的循环与Java中循环表现不一样，主要体现在写法及卫语句支持。下面看一段伪代码： 1234567for(line &lt;- source.getLines)&#123; for (char &lt;- line)&#123; if char.isLetter&#123; // your code &#125; &#125;&#125; Scala可以这样简写… 12345for&#123;line &lt;- source.getLines)&#123; char &lt;- line if char.isLetter // your code&#125; Scala可使得循环的写法非常简洁。 0x1 循环类型0x10 for循环表达式: 123for(x &lt;- Range )&#123; statement(x);&#125; 举个例子「🌰」例1： 1234567891011scala&gt; var r = Range(1,4,1)r: scala.collection.immutable.Range = Range(1, 2 ,3)scala&gt; for( x&lt;- r)&#123; | println(x) | &#125;123scala&gt; 遍历一个Map，例2 1234val names = Map("fname" -&gt; "Robert","lname"-&gt;"Goren")for((k,v)&lt;-names)&#123; println(s"key:$k,value:$v")&#125; 多种循环： 123for(x &lt;- Range; y&lt;- Range )&#123; statement(x,y);&#125; 0x11 while循环表达式如下: 123while(condition)&#123; statement(x);&#125; 例子： 123456789101112scala&gt; var i = 0i: Int = 0scala&gt; while(i&lt;10)&#123; | println(i) | i += 1 | &#125;0123scala&gt; 多重循环条件: 1234while(condition1)&#123; while(condition2) statement(x);&#125; 0x12 do…while循环while 与 do…while循环类似 表达式如下： 123do&#123; statement(x);&#125;while(condition) 0x2 循环中的卫语句&ensp;&ensp;&ensp;&ensp;&ensp;循环中的卫语句即在for循环中嵌入if语句，我们在业务开发中，在遍历过程中需要根据if 条件进行不同逻辑处理，比如过滤掉某些项。通常的做法是： 123456for(i &lt;- 1 to 10)&#123; if (i % 2 == 0) &#123; println(i) &#125;&#125; 在Scala中，我们可以这样写: 123for(i &lt;- 1 to 10 if i % 2 == 0)&#123; println(i)&#125; 也可以这样写 1234for&#123; i &lt;- 1 to 10 if i % 2 == 0&#125; println(i) 表达式语法： 123456for&#123; loop condition1 condition2 ...&#125; statement(); 0x3 yield语法&ensp;&ensp;&ensp;&ensp;&ensp;通过yield语法生成新的集合（PS：通常原集合是什么类型，生成的集合就是什么类型），假设有一个数组，数组值都是小写的字符串，将其进行首字母大写处理： 123456789val arr = Array("delete","scala","zt")val arr_yield = for (i&lt;-arr) yield i.capitalizescala&gt; arr_yield.foreach(println)DeleteScalaZtscala&gt; 0x4 break &amp; continue&ensp;&ensp;&ensp;&ensp;&ensp;与Java和其他语言不通，在Scala中break 一种方法，而不是一个关键字，在Scala中需用用到 breakable及break两个方法，由break抛出异常，breakable进行异常捕获，由此实现break &amp; continue功能，下面看一个例子： 例1：break实现 12345breakable&#123; for(i&lt;- 1 to 10)&#123; if i % 2 == 0 break # 将跳出循环 实现break &#125;&#125; 例2：continue实现 12345for(i&lt;- 1 to 10)&#123; breakable&#123; if i % 2 == 0 break # 跳出当前，继续下一遍历 &#125;&#125; 在scala.util.control.breaks类中，有非常清晰的定义： 123456789101112131415161718192021 /** * A block from which one can exit with a `break`. The `break` may be * executed further down in the call stack provided that it is called on the * exact same instance of `Breaks`. */ def breakable(op: =&gt; Unit) &#123; try &#123; op &#125; catch &#123; case ex: BreakControl =&gt; if (ex ne breakException) throw ex &#125; &#125;/** * Break from dynamically closest enclosing breakable block using this exact * `Breaks` instance. * * @note This might be different than the statically closest enclosing block! */ def break(): Nothing = &#123; throw breakException &#125; 由break函数抛出异常，由breakable捕获异常 0xF 总结&ensp;&ensp;&ensp;&ensp;&ensp;循环结构与其他语言大同小异，Scala中更加简洁的表达方式是其一大亮点，学习Scala可能需要多适应Scala语言的表达方式才能理解，当然是用常规的方式进行表达也不会有问题。]]></content>
      <categories>
        <category>0x0 开发语言</category>
        <category>0x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Hive窗口函数]]></title>
    <url>%2F2019%2F01%2F20%2Fhow-to-use-window-func%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;窗口函数常常用于我们业务中的复杂计算，本文介绍Hive中几个常用的窗口函数，并用案例讲述如何使用。那么我们常用的窗口函数有哪些？ 函数名 备注 row_number 分组内根据排列条件从1开始的排序，没有相同的序号 rank 分组内根据排列条件从1开始的排序，排名相等会在名次中留下空位，例如：1，2，2，4 dense_rank 分组内根据排列条件从1开始的排序，排名相等会在名次中不会留下空位，例如：1，2，2，3 lag 分组内根据排列条件取前第n个值 lead 分组内根据排列条件取后第n个值，与 lag相反 first_value 分组内根据排列条件去分组第一个值 last_value 分组内根据排列条件去分组最后一个值 min 分组内取最小值 max 分组内取最大值 sum 分组内取分组值的和 … … PS: 所有的窗口函数都将是生成的一个新列，并不会对原有数据列造成影响 下面将一一这些函数的使用。 0x0 row_numberrow_number 可以说是最常用的一个窗口函数。 举个栗子「🌰」：我们有一组数据，是一个班级一学期4次数学月考的成绩表 score 学生ID(sid) 第几次月考（exam_time） 成绩(score) s001 1 83 s001 2 87 s001 3 84 s001 4 89 s002 1 90 s002 2 75 s002 3 89 s002 4 82 那么Question：请问每个学生4次月考成绩中最好的一次是哪一次月考？ 此时，max的手法是不能够使用了，如果解决上述问题？实际上是将根据每个学生进行分组，即每个学生的4次月考为一组，那么计算这个分组成绩中的最大值即可 12345678910111213select sid ,exam_time ,scorefrom ( select sid ,exam_time ,score ,row_number() over(partition by sid order by score desc) as r_idx from score)t where t.r_idx = 1 说明： 我们称 row_number 为窗口函数，over为从句，窗口函数与over配合使用 1、使用PARTITION BY语句，使用一个或者多个原始数据类型的列进行分组2、使用ORDER BY语句，使用一个或者多个数据类型的排序列 反过来over 中若不指定 partition by 的数据列，那么整个表作为一个默认分组，同理不指定 order by 那么默认当前顺序，大家先只需了解这两点，熟练之后可以学习第三点。 3、使用窗口规范，窗口规范支持以下格式： (ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING)(ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING 详细参考文档：Hive官方参考文档]]></content>
      <categories>
        <category>0x2 技术平台</category>
        <category>0x21 Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ftp上传目录的方法]]></title>
    <url>%2F2019%2F01%2F16%2Fftp-upload-folder%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;众所周知，使用ftp命令是不能够直接上传文件夹的，然后工作中有很多需要上传目录的场景，通常大家想到的就是先创建文件夹，然后再通过遍历的方式上传文件，其中比较繁琐的是，ftp里不支持，多层级目录直接创建，1deletee:$ mkdir -p $&#123;dir&#125;/$&#123;subdir&#125; 这样的方式，只能逐级创建，将造成很多工作量。下面将分享下如何快速有效的实现上传ftp文件夹 0x0 生成创建多级文件夹命令1234567891011export mkdir_cmds=`find $&#123;upload_folder&#125; -type d -printf '%P\n' \ |awk '&#123; \ split($0,a,"/"); \ for (i in a) &#123; \ cmds ="mkdir "; \ for (j=1;j&lt;=i;j++) &#123; \ cmds = cmds"/"a[j] \ &#125; print cmds \ &#125; \ &#125;' \ |sort|uniq` 效果如下： 1234567891011121314151617181920212223242526272829deletee@pig_house:~/logon_log/bin$ ls -lhtotal 48K-rwxr-xr-x 1 delete pig_house 324 Jan 9 14:59 clearftpfile.sh-rwxr-xr-x 1 delete pig_house 2.2K Jan 11 12:19 contact_send.sh-rwxr-xr-x 1 delete pig_house 5.7K Jan 9 14:59 handle_file_repair.sh-rwxr-xr-x 1 delete pig_house 7.9K Jan 10 10:29 handle_file.shdrwxr-xr-x 3 delete pig_house 4.0K Jan 14 15:59 logon-rwxr-xr-x 1 delete pig_house 2.2K Jan 11 12:18 mail_send.sh-rwxr-xr-x 1 delete pig_house 2.2K Jan 11 12:18 news_send.shdrwxr-xr-x 3 delete pig_house 4.0K Jan 11 12:19 reg-rwxr-xr-x 1 delete pig_house 1.4K Jan 9 14:59 repair_login.sh-rwxr-xr-x 1 delete pig_house 2.8K Jan 16 14:50 send_news.shdeletee@pig_house:~/logon_log/bin$ upload_folder="bin"deletee@pig_house:~/logon_log/bin$ find $&#123;upload_folder&#125; -type d -printf '%P\n' \&gt; |awk '&#123; \&gt; split($0,a,"/"); \&gt; for (i in a) &#123; \&gt; cmds ="mkdir /$&#123;upload_folder&#125;"; \&gt; for (j=1;j&lt;=i;j++) &#123; \&gt; cmds = cmds"/"a[j] \&gt; &#125; print cmds \&gt; &#125; \&gt; &#125;' \&gt; |sort|uniqmkdir /bin/logonmkdir /bin/logon/bakmkdir /bin/regmkdir /bin/reg/bak 0x1 生成put文件命令&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原理与创建文件夹命令类似1export put_cmds=`find $&#123;upload_folder&#125; -type f -printf 'put %p $&#123;ftp_target_folder&#125;/%P\n'` 123456789deletee@pig_house:upload_path='~/logon_log/bin'deletee@pig_house:upload_folder='bin'deletee@pig_house:ftp_target_path='/ftp'deletee@pig_house:find $&#123;upload_path&#125; -type f -printf 'put %p $&#123;ftp_target_path&#125;/%P\n'put ./bin/logon/bak/police-logon.jar.bak /ftp/bin/logon/bak/police-logon.jar.bakput ./bin/logon/bak/run.sh /ftp/bin/logon/bak/run.shput ./bin/logon/bak/stop.sh /ftp/bin/logon/bak/stop.sh... 0x2 一个栗子123456789101112131415161718192021222324252627282930313233#!/bin/bashupload_path=$1 # 待上传目录ftp_target_path=$2 # ftp目标目录upload_folder=`basename $&#123;upload_path&#125;`export mkdir_cmds=`find $&#123;upload_path&#125; -type d -printf '%P\n' \ |awk '&#123; \ split($0,a,"/"); \ for (i in a) &#123; \ cmds ="mkdir $&#123;upload_folder&#125;"; \ for (j=1;j&lt;=i;j++) &#123; \ cmds = cmds"/"a[j] \ &#125; print cmds \ &#125; \ &#125;' \ |sort|uniq`export put_cmds=`find $&#123;upload_path&#125; -type f -printf 'put %p $&#123;ftp_target_folder&#125;/%P\n'`ftp_host=ftp_port=ftp_username=ftp_password=ftp -i -n &lt;&lt; EOFopen $ftp_host $ftp_portuser $ftp_username $ftp_passwordbi$&#123;mkdir_cmds&#125;$&#123;put_cmds&#125;byeEOF 0XF 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用生成ftp命令的方式，避免多层级及不定层级目录的创建ftp目录的繁琐。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大家觉得怎么样？或是有更好的方法！]]></content>
      <categories>
        <category>0x0 开发语言</category>
        <category>0x00 一起来学Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起来学Scala(3)-字符串]]></title>
    <url>%2F2019%2F01%2F10%2Fscala-programing-string%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文中已经介绍了Scala语言中的基本类型，本文着重介绍下字符串的用法，字符串在我们日常开发中经常用到，字符串在Scala中的类型String，那么Scala中的String和Java中的String有什么关系呢？ 1234scala&gt; "Scala".getClass.getNameres0: String = java.lang.Stringscala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本以为Scala是怎样实现了String，上述代码中告诉我们Scala中的String竟就是Java中的String。我们接下更深层次的学习String。 0x0 基本用法0x00 长度 .length1234567scala&gt; val str = "Scala"str: String = Scalascala&gt; str.lengthres3: Int = 5scala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过 .length的方式可获取变量str的长度 0x01 遍历第一种方式: 通过遍历字符串的每一个字符 12345678scala&gt; for(c &lt;- str) &#123; println(c) &#125;Scalascala&gt; 第二种方式: 通过调用.foreach方法，入参 println函数实现 12345678scala&gt; str.foreach(println)Scalascala&gt; 第三种方式: 通过调用.map方法，入参 println函数实现 12345678scala&gt; str.map(println)Scalascala&gt; 0x02 过滤 .filter1234scala&gt; str.filter(_ != 'a')res10: String = Sclscala&gt; 0x03 Bytes数组 .getBytes1234scala&gt; str.getBytesres11: Array[Byte] = Array(83, 99, 97, 108, 97)scala&gt; 0x0F 小结 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;String有很多的方法，例如 .drop(n) 删除前面n个字符,.take(n)取前n个字符，.captitalize 将字符串转为大写(与 .toUpperCase 功能等同)等等。 0x1 字符串相等性&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如何比较两个字符串是否相等 12345678910scala&gt; val str1 = "Scala"str1: String = Scalascala&gt; val str2 = "S" + "cala"str2: String = Scalascala&gt; str1 == str2res12: Boolean = truescala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如上，两个字符串内容是相等的,通过== 可判断两个字符串内容是否相等，这与Java中使用equal的方法比较两个对象不同。在Java中需要先判断调用的对象是否为null，即在AnyRef类使用时，Scala也会判断调用是否是null，其他情况下是不需要判断变量是否为null。实际上在Scala开发过程中，不推荐定义null， 0x2 创建多行字符串&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一点与Python类似，可以通过三个双引号来定义，例如： 1234567scala&gt; val multi_line_str = """Hello, | I love Scala"""multi_line_str: String =Hello, I love Scalascala&gt; 显示第二行是一连串的空格，我们可以使用管道的方式，与Python类似 123456789scala&gt; val multi_line_str = """Hello, | I love Scala | """multi_line_str: String ="Hello,I love Scala"scala&gt; 或者 123456789scala&gt; val multi_line_str = """Hello, | I love Scala | """.stripMarginmulti_line_str: String ="Hello,I love Scala"scala&gt; 如果不喜欢使用管道 |可以使用其他符号表示，例如: 123456789scala&gt; val multi_line_str = """Hello, @ I love Scala @ """.stripMargin('@')multi_line_str: String ="Hello,I love Scala"scala&gt; 0x3 变量代换&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 类似于Python一样，使用变量的值代换进另一个字符串 0x30 s是一个方法1234567scala&gt; val name = "deleee"name: String = deleeescala&gt; val setence = s"My name is $name"setence: String = My name is deleeescala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;整形变量可以做计算，例如： 1234567scala&gt; var age = 2age: Int = 2scala&gt; val setence1 = s"My age is $&#123;age + 1&#125;"setence1: String = My age is 3scala&gt; 0x31 字符串差值(printf)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 即我们常在C语言或者Python中的数值格式化，需要在字符串前使用f标识 1234scala&gt; val setence2 = f"My age is $&#123;age + 1&#125;%.2f"setence2: String = My age is 3.00scala&gt; %.2f表示将数值进行小数点后2位精度 格式化符号 描述 %c 字符 %d 整数 与 %i 相同 %e 指数浮点型 %f 浮点型 %i 整数 %o 八进制 %s 字符串 %u 无符号整形 %x 十六进制 %% 或者 \% 输出一个百分号 0x4 正则表达式0x40 创建正则表达式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在字符串末尾使用.r即表示创建的为一个Regex对象，随后可以用.findFirstIn匹配第一个，.findAllIn来匹配所有的。 例如: 1234567891011121314scala&gt; var numPatten = "[0-9]+".rnumPatten: scala.util.matching.Regex = [0-9]+scala&gt; val str = "123 is not 321"str: String = 123 is not 321scala&gt; val match1 = numPatten.findFirstIn(str)match1: Option[String] = Some(123)scala&gt; numPatten.findAllIn(str).foreach(println)123321scala&gt; 或者： 1234567scala&gt; import scala.util.matching.Regeximport scala.util.matching.Regexscala&gt; val pattern = new Regex("(S|s)cala")pattern: scala.util.matching.Regex = (S|s)calascala&gt; PS:关注下返回类型 （1）.findFirstIn 返回的一个是Some类型的变量，我们可以简单的认为Some是一个容器，匹配成功则返回数字，匹配不成功就返回None 12345678910scala&gt; var numPatten = "[0-9]+".rnumPatten: scala.util.matching.Regex = [0-9]+scala&gt; val str = "not number"str: String = not numberscala&gt; val match1 = numPatten.findFirstIn(str)match1: Option[String] = Nonescala&gt; （2）.findAllIn返回的是一个迭代器，可使用foreach方法进行遍历，也可以将迭代器转为数组,例如: 123456789101112scala&gt; var matchArr = numPatten.findAllIn(str).toArraymatchArr: Array[String] = Array(123, 321)scala&gt; for ( a &lt;- matchArr) println(a)123321scala&gt; matchArr.foreach(println)123321scala&gt; 0x41 字符串替换常规的替换手法字符串调用方法.replaceAll 1234567scala&gt; val str = "123 is not 321"str: String = 123 is not 321scala&gt; str.replaceAll("[1-9]+","sss")res12: String = sss is not sssscala&gt; 也可以使用正则表达式作为对象调用方法 .replaceAllIn及.replaceFirstIn 123456789scala&gt; var numPatten = "[0-9]+".rscala&gt; val str = "123 is not 321"str: String = 123 is not 321scala&gt; numPatten.replaceAllIn(str,"sss")res13: String = sss is not sssscala&gt; 0x42 字符串抽取正则表达式中使用() 表示 匹配的组，与其他（Perl、Python表达式相同） 例如： 12345678910111213scala&gt; import scala.util.matching.Regeximport scala.util.matching.Regexscala&gt; val numberPattern: Regex = "[0-9]".rnumberPattern: scala.util.matching.Regex = [0-9]scala&gt; numberPattern.findFirstMatchIn("awesomepassword") match &#123; | case Some(_) =&gt; println("Password OK") | case None =&gt; println("Password must contain a number") | &#125;Password must contain a numberscala&gt; 123456789101112131415161718192021222324scala&gt;import scala.util.matching.Regexscala&gt;val keyValPattern: Regex = "([0-9a-zA-Z-#() ]+): ([0-9a-zA-Z-#() ]+)".rscala&gt;val input: String = """background-color: #A03300; |background-image: url(img/header100.png); |background-position: top center; |background-repeat: repeat-x; |background-size: 2160px 108px; |margin: 0; |height: 108px; |width: 100%;""".stripMarginscala&gt;for (patternMatch &lt;- keyValPattern.findAllMatchIn(input)) | println(s"key: $&#123;patternMatch.group(1)&#125; value: $&#123;patternMatch.group(2)&#125;")key: background-color value: #A03300key: background-image value: url(imgkey: background-position value: top centerkey: background-repeat value: repeat-xkey: background-size value: 2160px 108pxkey: margin value: 0key: height value: 108pxkey: width value: 100scala&gt; 更多更详细的内容请参考Scala之旅-正则表达式模式和提取器对象（EXTRACTOR OBJECTS） 0x4E 正则表达式Scala 的正则表达式继承了 Java 的语法规则，Java 则大部分使用了 Perl 语言的规则。 下表我们给出了常用的一些正则表达式规则： 表达式 匹配规则 ^ 匹配输入字符串开始的位置。 $ 匹配输入字符串结尾的位置。 . 匹配除”\r\n”之外的任何单个字符。 […] 字符集。匹配包含的任一字符。例如，”[abc]”匹配”plain”中的”a”。 ... 反向字符集。匹配未包含的任何字符。例如，”abc“匹配”plain”中”p”，”l”，”i”，”n”。 \\A 匹配输入字符串开始的位置（无多行支持） \\z 字符串结尾(类似$，但不受处理多行选项的影响) \\Z 字符串结尾或行尾(不受处理多行选项的影响) re* 重复零次或更多次 re+ 重复一次或更多次 re? 重复零次或一次 re{ n} 重复n次 re{ n,} re{ n, m} 重复n到m次 a\ b 匹配 a 或者 b (re) 匹配 re,并捕获文本到自动命名的组里 (?: re) 匹配 re,不捕获匹配的文本，也不给此分组分配组号 (?&gt; re) 贪婪子表达式 \\w 匹配字母或数字或下划线或汉字 \\W 匹配任意不是字母，数字，下划线，汉字的字符 \\s 匹配任意的空白符,相等于 [\t\n\r\f] \\S 匹配任意不是空白符的字符 \\d 匹配数字，类似 [0-9] \\D 匹配任意非数字的字符 \\G 当前搜索的开头 \\n 换行符 \\b 通常是单词分界位置，但如果在字符类里使用代表退格 \\B 匹配不是单词开头或结束的位置 \\t 制表符 \\Q 开始引号：\Q(a+b)*3\E 可匹配文本 “(a+b)*3”。 \\E 结束引号：\Q(a+b)*3\E 可匹配文本 “(a+b)*3”。 0x4F 正则表达式实例 实例 描述 . 匹配除”\r\n”之外的任何单个字符。 [Rr]uby 匹配 “Ruby” 或 “ruby” rub[ye] 匹配 “ruby” 或 “rube” [aeiou] 匹配小写字母 ：aeiou [0-9] 匹配任何数字，类似 [0123456789] [a-z] 匹配任何 ASCII 小写字母 [A-Z] 匹配任何 ASCII 大写字母 [a-zA-Z0-9] 匹配数字，大小写字母 aeiou 匹配除了 aeiou 其他字符 0-9 匹配除了数字的其他字符 \\d 匹配数字，类似: [0-9] \\D 匹配非数字，类似: 0-9 \\s 匹配空格，类似: [ \t\r\n\f] \\S 匹配非空格，类似: \t\r\n\f \\w 匹配字母，数字，下划线，类似: [A-Za-z0-9_] \\W 匹配非字母，数字，下划线，类似: A-Za-z0-9_ ruby? 匹配 “rub” 或 “ruby”: y 是可选的 ruby* 匹配 “rub” 加上 0 个或多个的 y。 ruby+ 匹配 “rub” 加上 1 个或多个的 y。 \\d{3} 刚好匹配 3 个数字。 \\d{3,} 匹配 3 个或多个数字。 \\d{3,5} 匹配 3 个、4 个或 5 个数字。 \\D\\d+ 无分组： + 重复 \d (\\D\\d)+/ 分组： + 重复 \D\d 对 ([Rr]uby(, )?)+ 匹配 “Ruby”、”Ruby, ruby, ruby”，等等 0x5 自定义方法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Java中通常我们是建立类似StringUtils这种的通用类库，想要在String类中增加一些自定义的方法要怎么做？Java中是无法办到的，在Scala 2.1.0中可以定义隐式转换的类，在这个类中定义自己的方法来实现期望的功能。 123456789scala&gt; implicit class StringImprovement(s:String) &#123; | def increment = s.map(c =&gt; (c.toByte + 1).toChar) | &#125;defined class StringImprovementscala&gt; "abc".incrementres16: String = bcdscala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以任务String调用的方法都不奇怪，可能某处引用的包中定义了某个隐式转换的类及自定义方法。在Scala2.1.0之前的版本，若想要实现这样的功能，需要定义一个隐式转换的方法： 12345678910111213scala&gt; class StringImprovement(s:String) &#123; | def increment = s.map(c =&gt; (c.toByte + 1).toChar) | &#125;defined class StringImprovementscala&gt; implicit def stringToString(s:String) = new StringImprovement(s)warning: there was one feature warning; re-run with -feature for detailsstringToString: (s: String)StringImprovementscala&gt; "abc".incrementres0: String = bcdscala&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如你所见，在Scala中可以创建一个隐式转换的类，将他们引入需要的范围中去，而不需要进行继承，定义一个MyXxxx的新类。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面简要描述下其工作原理： 编译器找到”abc”的字符串常量 编译器发现要在String调用increment方法 因为String类中没有increment方法，它开始在当前范围内搜索一个接受String作为参数的隐式转换。 如此编译器会找到StringImprovement类，在这个类中找到了increment方法]]></content>
      <categories>
        <category>0x0 开发语言</category>
        <category>0x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起来学Scala(2)-基本类型]]></title>
    <url>%2F2019%2F01%2F09%2Fscala-programing-basictype%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文中已经介绍了Scala语言的入门概要，本文将介绍Scala的基本语法，Scala中有变量、对象、类、方法，在面向对象方面Scala与Java很像，在代码编写风格与Python又很像，除此之外，大家也会碰到一些未曾接触的语法，例如 _ . 等，本文将逐步介绍这些概念。 0x0 基本类型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们了解Scala中有哪些基本类型： 数据类型 描述 Byte 8位有符号补码整数。数值区间为 -128 到 127 Short 16位有符号补码整数。数值区间为 -32768 到 32767 Int 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 Float 32 位, IEEE 754 标准的单精度浮点数 Double 64 位 IEEE 754 标准的双精度浮点数 Char 16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF String 字符序列 Boolean true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null 或空引用 Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型。 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 (摘自Scala基本类型) 下图展示了基本类型之间的拓扑关系图，Scala中一切皆对象： 12val str:String = "Hello Scala"var i:Int = 10 上述实例中声明了变量str类型是String型，变量值为”Hello Scala”当为变量分配初始值时，Scala编译器可以根据分配给它的值来推断变量的类型。这被称为变量类型推断。 因此，可以编写这样的变量声明 12val str = "Hello Scala"var i = 10 0x1 声明变量0x10 声明关键字: val,var&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val 用于定义常量，相当于Java中使用final定义一样，val 是value的缩写&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;var 用于定义变量，val 是variable的缩写&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Scala语言中，使用val定义的变量，是不能够重新赋值的，而var定义的变量是可以重新进行赋值 12345678scala&gt; val str = "Hello World"str: String = Hello Worldscala&gt; str = "Hello Scala"&lt;console&gt;:12: error: reassignment to val str = "Hello Scala" ^scala&gt; 1234567scala&gt; var str = "Hello World"str: String = Hello Worldscala&gt; str = "Hello Scala"str: String = Hello Scalascala&gt; 0x11 多个赋值&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scala支持多个赋值。如果代码块或方法返回一个元组(Tuple - 保持不同类型的对象的集合)，则可以将元组分配给一个val变量。 注：我们将在随后的章节学习元组。 12val (myVar1: Int, myVar2: String) = Pair(40, "Foo")Scala 类型推断得到正确的类型 - 1val (myVar1, myVar2) = Pair(40, "Foo") 0x2 变量的作用域&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scala中变量同其他语言类似，有不同的作用域，具体取决于它们被使用的位置。它们可以作为字段存在，作为方法参数和局部变量存在。以下是每种类型范围的详细信息。 0x20 字段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;声明为对象中的字段信息，那么根据需要可使用val或者var，对象中的字段可以被对象中其他方法访问，配合访问修饰符，可以设置外部变量或者对象访问 0x21 方法参数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;方法或函数入参都是常量，所以定义函数时都是使用val关键字定义，方法的参数是不可变的。 0x22 局部变量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;局部变量是在方法中声明的变量。局部变量只能从方法内部访问，但如果从方法返回，则您创建的对象可能会转义该方法。局部变量可以是可变的和不可变的类型，可以使用var或val定义。 0xF 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文主要介绍了Scala中基本数据类型及变量的声明方法，根据需要选择合适的关键字进行变量定义。var 修饰的对象引用可以改变，val修饰的则不可改变，但对象的状态却是可以改变的，即表示对于一个对象，通过 val 初始化对象后，该对象是不能够改变的，但是如果对象中有var声明的字段，那么是可以修改对象字段值，实现对象状态的改变。 123456789101112131415161718192021class A(n: Int) &#123; var value = n&#125;class B(n: Int) &#123; val value = new A(n)&#125;object Test &#123; def main(args: Array[String]) &#123; val x = new B(5) x = new B(6) // 错误，因为 x 为 val 修饰的，引用不可改变 x.value = new A(6) // 错误，因为 x.value 为 val 修饰的，引用不可改变 x.value.value = 6 // 正确，x.value.value 为var 修饰的，可以重新赋值 &#125;&#125;// --------------------- // 作者：GuaKin_Huang // 来源：CSDN // 原文：https://blog.csdn.net/a1234h/article/details/77962536 // 版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>0x0 开发语言</category>
        <category>0x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的一些SQL语句]]></title>
    <url>%2F2019%2F01%2F09%2Fuseful-sql%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际工作过程中，经常用的几个SQL，常常因为某些关键字忘记，从而经常需要找工具书或者之前做的工程，本文将自己工作中常常容易忘记的SQL整理出来，方便直接定位。 0x0 建表DDL(指定分隔符、分区)12345678910create table dim.dim_sms_rate_standard( contry_code string comment '国家码' ,contry_en_name string comment '国家英文名' ,contry_brief_name string comment '国家简码' ,sms_price float comment '短信单价')row format delimited fields terminated by '\t'partition(dt string comment '日期'); 0x1 设置压缩格式12set hive.exec.compress.output=true;set mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec; 0x2 查看外表分区路径1desc formatted dim.dim_sms_rate_standard partition(dt='2019-01-08'); 0x3 动态分区1234set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict; SET hive.exec.max.dynamic.partitions=100000;SET hive.exec.max.dynamic.partitions.pernode=100000; 0xF 未完待续]]></content>
      <categories>
        <category>0x2 技术平台</category>
        <category>0x21 Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起来学Scala(1)-入门概要]]></title>
    <url>%2F2019%2F01%2F07%2Fscala-programing-brief%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为什么要学Scala？Scala是大数据分析平台Spark、Flink 官方支持的语言，在学习Spark和Flink之前，需要学好Scala基础。Scala同样是一门当前热门的语言，Kafka、Spark均由Scala开发，由此学习Scala是学习这些平台比不可少的步骤。 0x0 Scala基本定义Scala 是一门多范式（multi-paradigm）的编程语言，设计初衷是要集成面向对象编程和函数式编程的各种特性。Scala 运行在Java虚拟机上，并兼容现有的Java程序。Scala 源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库，由而Java同样可以调用Scala的类库。Scala语言 使用 JDK &amp; Scala SDK 0x1 Scala的运行方式开发Scala程序之前需要安装Scala 0x10 脚本运行Scala也是一种脚本式语言，可以将Scala程序写入一个脚本，直接执行 如下是第一个 Scala程序:HelloWorld.scala 12345object HelloWorld&#123; def main(args:Array[String])&#123; println("Hello World") &#125;&#125; 使用命令: 12~$:scala HelloWorld.scalaHello World 0x11 编译执行12345~$:scalac HelloWorld.scala# 将会得到如下两个 class文件，即为 Java字节码HewlloWorld$.classHewlloWorld.class 执行方式与执行javaclass类同 12~$:scala HelloWorldHello World 0xF 总结Scala 的语法相对Python而言是比较晦涩，后续尽量以简单清晰的实例，从而达到更好的效果。]]></content>
      <categories>
        <category>0x0 开发语言</category>
        <category>0x01 一起来学Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk的基本使用方法]]></title>
    <url>%2F2019%2F01%2F06%2Fawk-basic%2F</url>
    <content type="text"><![CDATA[0x0 简介&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。 awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。 0x1 使用方法1awk '&#123;pattern + action&#125;' &#123;filenames&#125; 通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。 awk 有如下三种使用方式 0x10 命令行方式awk [-F field-separatir] 'commands' input-file 1awk [-F field-separatir] 'commands' input-file 此种方式为最常用的一种方式，-F 是将文件按指定分隔符进行切割。 0x11 shell脚本方式将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。相当于shell脚本首行的：#!/bin/sh可以换成：#!/bin/awk 0x12 将所有的awk命令插入一个单独文件，然后调用：1awk -f awk-script-file input-file(s) 其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。 0x2 入门实例0x20 分隔符例如：a.txt 121 2 31 2 3 awk 默认支持分隔符，例如 空格，制表符，如下两种方式等价 12awk -F&apos; &apos; &apos;&#123;print $1&#125;&apos; a.txtawk &apos;&#123;print $1&#125;&apos; a.txt $0变量是指整条记录。$1表示当前行的第一个域,$2表示当前行的第二个域,……以此类推。 例如： b.txt 121,2,33,4,5 123awk -F&apos;,&apos; &apos;&#123;print $1&#125;&apos;13 0x21 BEGIN &amp; END123456789cat /etc/passwd \|awk -F ':' 'BEGIN &#123;print "name,shell"&#125; &#123;print $1","$7&#125; END &#123;print "blue,/bin/nosh"&#125;'name,shellroot,/bin/bashdaemon,/bin/shbin,/bin/shsys,/bin/sh....blue,/bin/nosh BEGIN 是在处理数据行之前进行的操作 END 则是在数据行处理之后进行的操作 例如：c.txt 求第一列的总和 12341,2,33,4,54,5,67,8,9 命令如下： 1awk -F',' 'BEGIN &#123;sum=0&#125; sum+=$1 END &#123;print "sum = "sum&#125;' BEGIN 声明一个sum 初始化值为0 的变量，END 将变量输出 0x3 awk内置变量1234567891011ARGC 命令行参数个数ARGV 命令行参数排列ENVIRON 支持队列中系统环境变量的使用FILENAME awk浏览的文件名FNR 浏览文件的记录数FS 设置输入域分隔符，等价于命令行 -F选项NF 浏览记录的域的个数NR 已读的记录数OFS 输出域分隔符ORS 输出记录分隔符RS 控制记录分隔符 0x4 awk 编程awk 可在内部实现丰富的编程 条件语句 awk中的条件语句是从C语言中借鉴来的，见如下声明方式： 12345678910111213141516171819if (expression) &#123; statement; statement; ... ...&#125;if (expression) &#123; statement;&#125; else &#123; statement2;&#125;if (expression) &#123; statement1;&#125; else if (expression1) &#123; statement2;&#125; else &#123; statement3;&#125; 例如：统计某个文件夹下的文件占用的字节数,过滤4096大小的文件(一般都是文件夹): 12ls -l |awk 'BEGIN &#123;size=0;print "[start]size is ", size&#125; &#123;if($5!=4096)&#123;size=size+$5;&#125;&#125; END&#123;print "[end]size is ", size/1024/1024,"M"&#125;' [end]size is 8.22339 M 循环语句 awk中的循环语句同样借鉴于C语言，支持while、do/while、for、break、continue，这些关键字的语义和C语言中的语义完全相同。 数组 因为awk中数组的下标可以是数字和字母，数组的下标通常被称为关键字(key)。值和关键字都存储在内部的一张针对key/value应用hash的表格里。由于hash不是顺序存储，因此在显示数组内容时会发现，它们并不是按照你预料的顺序显示出来的。数组和变量一样，都是在使用时自动创建的，awk也同样会自动判断其存储的是数字还是字符串。一般而言，awk中的数组用来从记录中收集信息，可以用于计算总和、统计单词以及跟踪模板被匹配的次数等等。 12345678awk -F &apos;:&apos; &apos;BEGIN &#123;count=0;&#125; &#123;name[count] = $1;count++;&#125;; END&#123;for (i = 0; i &lt; NR; i++) print i, name[i]&#125;&apos; /etc/passwd0 root1 daemon2 bin3 sys4 sync5 games...... awk编程的内容极多，这里只罗列简单常用的用法，更多请参考 http://www.gnu.org/software/gawk/manual/gawk.html]]></content>
      <categories>
        <category>0x0 开发语言</category>
        <category>0x00 一起来学Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库工程师学习路线]]></title>
    <url>%2F2019%2F01%2F05%2Fedw-need-learning%2F</url>
    <content type="text"><![CDATA[刚看了木东居士关于《聊一聊如何面向简历学习》，有一些感触，每每思考到职业发展总是有一种恐慌，因为我们很多时候都忘了梦想，甚至根本没有梦想，也就谈不上规划，今天是思考和整理下自己的学习路线。写下自己的想法及规划。 你是否担心互联网寒冬中首先淘汰的那波人就有自己？你是否感觉到自己缺乏核心竞争力？是否感觉已经很久没有学习成长？ &nbsp; &nbsp; &nbsp; &nbsp;一个数据仓库工程师或者数据开发人员，需要掌握什么技能？数据仓库应该分为四类：建模方法论，实施方法论、数据管理方法论、数据应用，所以我们将木东居士的内容整理了一下，如下： 主题 内容 时间 数据建模 常用数据仓库模型原理总结 1 周 数据建模 维度建模原理 1 周 数据建模 以淘宝的场景为例设计一套数据模型 1 周 数据建模 时间维表设计 1 周 数据管理 元数据管理 1 周 数据管理 数据质量监控设计 1 周 数据管理 数据血缘分析 1 周 数据管理 作业监控设计 1 周 数据应用 OLap分析原理+Kylin实践 3 周 数据应用 数据分层设计 1 周 大数据平台 Hive（基本用法、高阶函数用法、Hive优化） 2 周 大数据平台 Hive 执行过程原理 1 周 大数据平台 Hadoop 常用命令 1 周 大数据平台 HBase 基本用法，原理 2 周 大数据平台 Spark 常用命令 1 周 大数据平台 Flink 基本用法，原理 2 周 大数据平台 Kafka 基本用法，原理 2 周 脚本语言 Python 2 周 脚本语言 Scala 2 周 论文阅读 Google 的 Goods 论文阅读（数据管理） 1 周 在日常工作中，并不是所有技术都有在使用，往往没有使用的技术或方法论，则容易被我们忘记，所以最好的方式，就是写博客或者写PPT，以自己理解的话术表达出来，相信这样的模式会比单纯的记忆一些概念要好很多。 感谢@木东居士的分享]]></content>
      <categories>
        <category>0x1 数据仓库</category>
        <category>0x1F 其他</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk的高级应用-多目录输出]]></title>
    <url>%2F2018%2F12%2F31%2Fawk-application%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;awk是Shell中三剑客之一，功能十分强大，本文分享的通过一份管道输入，实现多目录输出。原理：awk内部可实现编程，在awk内部将输入流进行预处理后，利用 print ${content} &gt;&gt; ${path}的方式将数据进行分流。 下面一个实例： 通过HDFS 标准输入流，在awk中根据数据日期时间分流至不同的日期文件夹中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#!/usr/bin/env bash#***********************************************************************#脚本功能: 追补历史推送数据#创建日期: 2016-05-16#修改纪录： 修改人 修改日期 修改描述#----+-----------------+--------------------+--------------------------+# 1 deletee 2017-10-12 创建#***********************************************************************# 环境变量设置#-------------------------------------------------------# JOB_HOME：作业目录# HDFS_PATH :HDFS目录#-------------------------------------------------------# LocalJOB_HOME=$(cd "$(dirname "$0")"; pwd)JOB_DATA_PATH=$&#123;JOB_HOME&#125;/dataPROCESSED_FILE_PATH=$&#123;JOB_HOME&#125;/proSECRETS_CONF_PATH=$&#123;JOB_HOME&#125;/confmkdir -p $&#123;JOB_HOME&#125;/&#123;data,pro,conf&#125;# HDFSHADOOP_HOME=/home/$&#123;USER&#125;/software/hadoopHDFS_PATH=/hdfs/*/some_log/#---------------------------# 函数名:genTargetFileModel# 功能:生成目标文件模式,遍历HDFS目录，生成日、时、分# 参数: $1 日期，格式 yyyy-MM-dd# 返回:无#---------------------------function genTargetFileModel() &#123; # 遍历日期 ETL_DT=$1 CUR_HDFS_PATH=`echo $&#123;HDFS_PATH&#125;|sed "s:*:$&#123;ETL_DT&#125;:"` for (( i = 0; i &lt; 24; i++ )); do for (( j = 0; j &lt; 12; j++ )); do hour=`printf "%02d" $i` minutes=$((j*5)) minutes=`printf "%02d" $minutes` mkdir -p $&#123;JOB_DATA_PATH&#125;/$&#123;ETL_DT&#125;/$&#123;hour&#125;/$&#123;minutes&#125; done done for path in `$&#123;HADOOP_HOME&#125;/bin/hadoop fs -ls $&#123;CUR_HDFS_PATH&#125; \ |awk '&#123;print $8&#125;'`;do echo "$&#123;ETL_DT&#125;:$&#123;path&#125;" &gt;&gt; $&#123;PROCESSED_FILE_PATH&#125;/pro_$&#123;ETL_DT&#125;.txt $&#123;HADOOP_HOME&#125;/bin/hadoop fs -text $&#123;path&#125; |awk '&#123; split($0,a," "); datestr=substr(a[1],11,10); hourstr=substr(a[2],1,2); minutestr=substr(a[2],4,2); minutestr=int(minutestr/5)*5; if(minutestr&lt;10)&#123; minutestr=("0"minutestr) &#125; gsub("\"","",a[3]) split(a[3],b,","); gsub("&#123;|&#125;","",b[3]) gsub("fields:HOSTNAME:","",b[3]) gsub("\\\\t","\t",b[1]) gsub(";","\t",b[1]) lens=split(b[1],tA,"\t") if(lens==12)&#123; re=(b[1]"\t\t\t\t\t\t\t\t\t\t\t\t"b[3]) &#125; if(lens==15)&#123; re=(b[1]"\t\t\t\t\t\t\t\t\t"b[3]) &#125; path=(datestr"/"hourstr"/"minutestr"/log_"datestr"_"hourstr"_"minutestr".txt") print re &gt;&gt; path &#125;' done unset ETL_DT&#125;#---------------------------# 函数名:genTargetFileModel# 功能:生成目标文件模式,遍历HDFS目录，生成日、时、分# 参数: $1 日期，格式 yyyy-MM-dd# 返回:无#---------------------------function sendByRysnc() &#123; ETL_DT=$1 nohup /usr/bin/rsync -avz \ --bwlimit=30720 \ --port=873 \ --progress \ --password-file=$&#123;SECRETS_CONF_PATH&#125;/secret.conf $&#123;JOB_DATA_PATH&#125;/$&#123;ETL_DT&#125; $&#123;user&#125;@$&#123;ip&#125;::$&#123;module&#125;/$&#123;path&#125;/ &gt;&gt; rysnc_send_$(date +%Y%m%d).log 2&gt;&amp;1 &amp; unset ETL_DT&#125;function mainStart() &#123; ETL_DT=$1 genTargetFileModel $&#123;ETL_DT&#125; sendByRysnc $&#123;ETL_DT&#125;&#125;mainStart 2017-09-29]]></content>
      <categories>
        <category>0x0 开发语言</category>
        <category>0x00 一起来学Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(5)-维度设计]]></title>
    <url>%2F2018%2F12%2F29%2Fdim-design%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;都说维度设计是维度建模的灵魂，在维度建模中，我们将度量成为「事实」，将环境描述为「维度」，但是如何设计一个好的维度模型，我们需要遵循一定的技巧和方法。 如何标识维度？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常我们都会问「谁、何时、何地、为何、如何」来描述一个事件过程，同样维度可以理解为一个事件或者业务过程的角度，比如商品购买购买人、购买事件、购买产品、购买使用的设备、购买使用的IP等均为维度范畴 围绕业务流程来构建维度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;考虑到需求总是变换多端，但是有一点不变：需求总是围绕着业务流程，那么当我们设计维度的时候，就需要充分生成丰富的维度及属性 维度的基本设计方法？（1）根据业务流程确定主维度 ​ 即在业务过程中找出关键维度作为主维度，在企业级数据仓库中，必需维度的统一口径，以商品为例，有且仅有一个维度定义 （2）确定相关维度 （3）确定维度属性 尽可能生成丰富的维度属性 尽可能多地给出包括一些富有意义的文字性描述​ 维度建模中较多维度以ID形态存在，例如：商品ID，但是单一的ID在业务描述非常欠缺，通常ID的含义描述也随ID作为维度设计中的一部分 区分数值型属性和事实​ 在一些业务数据中，一些数值型字段是作为维度还是作为度量（事实） 尽量沉淀出通用的维度属性​ 有些维度属性值获取是经过了复杂的逻辑处理，或者是通过解析某个字段得到，或是多张表关联得到，当设计出负责的维度时，若不保留原生维度，将难以覆盖业务的变化​ 并且属性值格式做到统一，比如某模型A时间格式为 yyyy-MM-dd HH:mm:ss，而模型B中的时间格式 则是 Unix Timestamp，同样也将造成口径不一致 理解关键字：维度设计需要严格遵循开发规范，对字段命名、含义、口径、属性值格式的规范，需严格遵循。 常见维度设计方法 （1）缓慢变化或快速变化 ​ 维度缓慢变化 修改维度属性值 不保留历史操作，适用于无需关心历史维度值的情形。 插入一行 保留历史操作，由于多了新记录，使得维度关联时，会产生冗余记录，增加额外学习成本 插入一列 预留一列作为上一次维度值的保留，适用于只需要查看上一次的维度值情形 快照维度 即将每天的变更存储为一个快照，在维度表较小时，可以忽略存储 极限存储 通过拉链表的形式，记录维度的缓慢变化，但是注意，维度表中有频繁变化的字段。 ​ 维度快速变化 ​ 此类问题又称微型维度，主要是为了解决快变超大维度（rapidly changing monster dimension）,顾名思义，在某些维度表中，大量维度进行缓慢变化甚至没有变化，但是有少量的维度频繁的发生变化，此种情形下，若使用缓慢变化的方式进行处理的话，将消耗大量资源。由此解决办法是： ​ 将维度属性值频率变化比较高的字段提取出来，建立一个单独的维度表，只需维度这一张快速变化的维度表即可。 （2）维度的层次结构 ​ 也是通常所说的 规范化和反规范化 ​ 在维度设计的过程中，我们经常遇到维度的层次问题，比如：商品通常会有一级分类、二级分类、三级分类此种情形，对于这样的维度处理方式，通常有两种做法： 星系模型 将维度层次问题扁平化，将层次的维度项进行扁平处理，例如有如下维度：sku_id，category_lv1, category_lv2, category_lv3 … 雪花模型 将层级维度提炼出来单独作为一张新的维度表 上文已经阐述过，两种模型的利弊，在新互联网的大数据平台基础上，存储也变的非常廉价，通常是选用星系模型解决维度层次的问题 （3）维度一致性 In data warehousing, a conformed dimension is a dimension that has the same meaning to every fact with which it relates. Conformed dimensions allow facts and measures to be categorized and described in the same way across multiple facts and/or data marts, ensuring consistent reporting across the enterprise. ​ 维度一致性是如此解释的，在数据仓库中，一致的维度是与其相关的每个事实具有相同含义的维度。 一致的维度允许在多个事实和/或数据集市中以相同的方式对事实和度量进行分类和描述，从而确保整个企业的一致报告。 ​ 通常在数据仓库开发过程中如何保证维度的一致性呢？ 维度的定义需要遵循命名、取值等规范 维度需通过元数据进行管理 采用维度建模，意味着建仓过程是自下而上的，各个数据集市各自开发，那么如何保证企业级数据仓库维度一致性，则需要统一的元数据进行管理 维度建模的数据仓库中，有一个概念叫Conformed Dimension，中文一般翻译为“一致性维度”。一致性维度是Kimball的多维体系结构（MD）中的三个关键性概念之一，另两个是总线架构（Bus Architecture）和一致性事实（Conformed Fact）。 在多维体系结构中，没有物理上的数据仓库，由物理上的数据集市组合成逻辑上的数据仓库。而且数据集市的建立是可以逐步完成的，最终组合在一起，成为一个数据仓库。如果分步建立数据集市的过程出现了问题，数据集市就会变成孤立的集市，不能组合成数据仓库，而一致性维度的提出正式为了解决这个问题。 一致性维度的范围是总线架构中的维度，即可能会在多个数据集市中都存在的维度，这个范围的选取需要架构师来决定。一致性维度的内容和普通维度并没有本质上区别，都是经过数据清洗和整合后的结果。 一致性维度建立的地点是多维体系结构的后台（Back Room），即数据准备区。在多维体系结构的数据仓库项目组内需要有专门的维度设计师，他的职责就是建立维度和维护维度的一致性。在后台建立好的维度同步复制到各个数据集市。这样所有数据集市的这部分维度都是完全相同的。建立新的数据集市时，需要在后台进行一致性维度处理，根据情况来决定是否新增和修改一致性维度，然后同步复制到各个数据集市。这是不同数据集市维度保持一致的要点。 在同一个集市内，一致性维度的意思是两个维度如果有关系，要么就是完全一样的，要么就是一个维度在数学意义上是另一个维度的子集。例如，如果建立月维度话，月维度的各种描述必须与日期维度中的完全一致，最常用的做法就是在日期维度上建立视图生成月维度。这样月维度就可以是日期维度的子集，在后续钻取等操作时可以保持一致。如果维度表中的数据量较大，出于效率的考虑，应该建立物化视图或者实际的物理表。 这样，维度保持一致后，事实就可以保存在各个数据集市中。虽然在物理上是独立的，但在逻辑上由一致性维度使所有的数据集市是联系在一起，随时可以进行交叉探察等操作，也就组成了数据仓库。 （4）维度整合和拆分 ​ 因为数据仓库的数据源自各个系统，有从移动端，web端，PC端，每个段的数据结构差异较大，即便是同一端，可能因为业务拆分，字段的属性值也不完全一致。 ​ 例如：A系统 某字段值用 1/0表示 token是否有效，B系统则使用F/T表示是否有效 ​ 那么，在此种情形下，需要进行维度整合。维度整合需要遵循如下规范： 命名规范统一 字段类型统一 属性值编码和含义统一 ​ 另外有一种场景，因为系统差异太大，而无法进行维度整合，此时需要进行维度的拆分 ​ 例如：加油站主要的商品是则是油，加油站内同样也有零售店，售卖一些日常百货，油 有 92，95，98等维度属性，日常百货的通常 涉及的 进销存，单价等 ​ 两种商品在维度差异过大，通常的做法，加油站的主营业务是售卖油品，从而建立主要的商品维度表，另外建立一个零售商品维度记录表 ​ 理解关键词：各个业务差异独特性较大的业务各自建立独立的两个维度表 （5）杂项维度的处理方法 退化维度​ 所谓退化维度（degenerate dimension），是指在实施表中那些看起来像是一个事实表的一个维度关键字，但实际上没有对应的维度字段。退化维度一般都是事务的编号，如购物小票编号，发票编号 行为维度​ 行为维度是基于过去维度成员的行为进行分组或者过滤事实的办法。行为维度即将事实转化为维度，以确保获得更多的分析能力​ 例如：购买次数超过30次，30次至100次，超过100次 作为维度值 角色维度​ 角色维度通常是一个业务活动有多个角色参与，例如：办理银行业务，有客户经理，审批人等参与两个及以上角色，而这些角色均属于员工维度表，对于此种情形，没有必要根据角色建立多个维度表，而是可以通过建立视图的方式达到目的。 多值维度​ 多值维度一般会出现多对多的关系中，例如：购买房产，会有夫妻两人共同持有，一次下单，多个子订单 ​ 有如下三种方法： ​ 1）降低事实表的粒度 每一个事实都标注最小粒度，例如：前台业务与商业智能关注交易子订单，每一个子订单一个事实，只会有一个商品与之对应，很多时候，事实表的粒度是不能降低的，当强行降低之后，那么订单事实则发生改变，对于统计每日订单量计算，则复杂度变高，增加学习和计算成本 ​ 2）采用多字段 例如：在买房合同中，标注第一买受人，第二买受人，可顺位增加冗余的字段位 ​ 3）使用常用的桥接表 例如：订单中，确定父订单为粒度，建立父订单与子订单的桥接表，另建立子订单与商品的维度关系表，桥接表包含事实表关联的分组KEY，以及作为买受方的维度表外键ID，有多个买受方，则有在相同的KEY下有多条记录，桥接表需要更多的计算，也可能会造成双重计算，例如，买受人1籍贯为山东，买受人2籍贯为浙江，那么当分别统计外地山东的购房数，和浙江的购房数，则产生了多重计算。双重计算不一定是错误，对于一些业务需求是合理的，但对于另一些业务需求，则需要规避。 ​ 理解关键字：当前大数据平台支持复杂的数据结构，将多个买受人可以作为数据组结构存入一个字段中，使用key:value的形式，当然此种方法需增加一定的计算成本，属于非规范化操作。 杂项维度（junk dimension）​ 杂项维度就是一种包含的数据具有很少可能值的维度​ 当定义好各种维度后，会发现一些小范围维度取离散值或者标志位的字段，但是这样的维度又很难退化存储在事实表中，可能会造成事实表过大，若果单独建立维度表进行关联，通过外键关联，会出现维度过多的情况，如果将这些字段删除，则业务方不同意。 这时，我们通常的解决方案就是建立杂项维度，将这些字段建立到一个维度表中，在事实表中只需保存一个外键。几个字段的不同取值组成一条记录，生成代理键，存入维度表，并将该代理键保存入相应的事实表字段。建议不要直接使用所有的组合生成完整的杂项维度表，在抽取时遇到新的组合时生成相应记录即可。杂项维度的ETL过程比一般的维度略为复杂。 几个字段的不同取值组成一条记录，生成代理键，存入维度表： 即将几个杂项维度的组合建立杂项维度表，生成一个代理键，植入事实表中 微型维度 理解关键字2： ​ 第一：避免维度过度增长 某些维度值变化过高，如果维度表使用了拉链极限存储，那边过度增长或者变化的维度，将使得极限存储效率差 ​ 第二：避免耦合度过高 例如：卖家的主营项目，加工逻辑异常复杂，如果融合进现有的卖家维表中，那么过多的业务耦合将会导致卖家维度难以维护，应适当做维度拆分]]></content>
      <categories>
        <category>0x1 数据仓库</category>
        <category>0x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(4)-维度建模]]></title>
    <url>%2F2018%2F12%2F28%2Fdim-modeling%2F</url>
    <content type="text"><![CDATA[1、什么是维度建模？理解关键字：维度建模的出发点是实现快速的数据分析与决策，维度建模通常面向业务人员、分析人员使用，相对ER建模来说会更加开放，更容易理解 维度建模是从业务过程中提炼而来，典型维度建表代表星形建模和雪花建模 2、维度建模一般过程（摘自《大数据之路》 &amp; 《离线和实时大数据之战》）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kimball四步建模流程适合上述数据仓库系统建设流程中模型设计环节，重点解决数据粒度、维度设计和事实表设计问题。 （1）选择业务过程 ​ 业务过程描述的是企业的业务活动，例如电商系统用户下单，账号系统用户注册了一个账号 （2）选择粒度 ​ 那么什么是粒度呢？粒度是描述的是活动细节的每一个子项，例如，用户注册一个账号，包含了用户名、时间、IP、终端类型、手机号码，帐号类型等 （3）确定维度 ​ 确定维度之前，需要先明确什么是维度？维度是表示业务活动在不同角度的表现方式，例如：注册账号的活动中，从时间、IP、终端类型等角度进行描述 ​ 比如：用户名、注册时间、注册IP、注册终端、注册手机号码、注册账号类型（手机账号、邮箱账号、三方账号） （4）确定事实 ​ 确定维度之后，事实则变的清晰，比如：我们需要计算某终端、某天的注册量 那么问题来了？ 1、如何确定粒度？ ​ 最细粒度和聚合粒度之争？ 到底是选择最细粒度，还是聚合粒度？ 2、如何确定维度？ ​ 标识维度解决的是业务人员如何描述来自业务过程的数据，维度用来表示“谁、什么、何时、何处、为何、如何”的问题。以竞价广告检索流程而言就是客户通过什么渠道、什么样的客户端（OS、IP）、检索了什么样的内容、请求最终有谁受理等。 3、如何确定事实？ ​ 标识事实其实是在确定业务过程的度量指标，指标何来？哪些指标必须保留，那些指标必须删除，待定指标如何处理？必须综合考虑业务用户需求和现实数据的实际情况。事实表的设计完全依赖于物理活动，不受可能产生的最终报表的影响，报表只是事实表设计的参考视角。 ​ 指标必需要考虑业务的需求和数据的实际情况 3、雪花模型 与 星形模型理解关键字：时间与空间交换 （1）雪花模型是充分展开维度，当出现维度层次时，例如：某商品分大类、种类、小类，使用雪花模型建模时，通常将维度表中记录小类的ID，额外设计一张类别的维度表 雪花模型去除了数据冗余，节省了部分存储，但也带来了一定的不变，使得业务人员分析查询时需要关联多张维度表。 （2）星系模型则是允许一定的冗余，将维度层次扁平化，通过牺牲一小部分空间换取查询时快捷 PS:某些情况下可以需要用到雪花模型]]></content>
      <categories>
        <category>0x1 数据仓库</category>
        <category>0x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
        <tag>维度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(3)-ER建模]]></title>
    <url>%2F2018%2F12%2F27%2Fer-modeling%2F</url>
    <content type="text"><![CDATA[1、什么是ER建模？理解关键字：ER模型又称实体-关系模型，遵循3NF建模，采用ER进行数据仓库建模需要从整个企业的角度理清各业务之间的关系，建模的出发点是基于企业数据的整合，建设EDW需要建模人员对企业整体业务有精深的把控 例如：Teradata的 FS-LDM模型，将金融业务分为10大主题，通常是对整体行业发展的沉淀，将成熟的模型做适当的调整即可快速落地实施 2、ER建模的三个阶段理解关键字：ER模型是自上而下的建模方式，由此建模的需要先从企业的整体框架进行高度抽象 （1）高层模型​ 一个高度抽象的模型，描述主要的主题及主体间的关系，用于描述企业的总体情况（2）中层模型​ 在高层模型的基础上，细化主题域的实体、关系等数据项（3）物理模型​ 在中层模型的基础上，通常会根据平台的特性进行物理属性设计，例如：分表、分区、分区索引等设计理解关键字2：先通过整合整个企业的业务关系，划分相应主题域及对应的主题之间的关系，再细化每个主题域中的内容，最后是根据平台的特点进行物理设计]]></content>
      <categories>
        <category>0x1 数据仓库</category>
        <category>0x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
        <tag>ER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(2)-数据建模]]></title>
    <url>%2F2018%2F12%2F26%2Fdata-modeling%2F</url>
    <content type="text"><![CDATA[1、为什么需要数据建模？理解关键字：数据建模就是数据组织和存储的方法数据模型是数据组织和存储方法，正如我们希望图书馆的书分门别类的放置。数据模型强调从业务、数据存取和使用角度合理规划数据 那么为什么需要数据建模呢？我们可以从建模有哪些好处理解 （1）性能 良好的模型能帮助我们快速查询所需要的数据，减少不必要的数据查询I/O（2）成本 良好的数据模型能够极大地减少不必要的数据存储，也能实现计算结果的复用，极大地降低大数据系统中的存储和计算成本（3）效率 良好的数据模型能够极大地提高用户体验，用户可通过业务沉淀后的模型尽可能减少不必要的查询，从而提高效率（4）质量 良好的数据模型将遵循一定的数据规范（例如：口径约束，字段约束等）从而减少数据计算的失误，保障数据质量 2、OLTP与OLAP的区别理解关键字： OLTP 是讲究事务，在OLAP中事务不是所关注的，主要是批量的读写，致力于联机分析 一个是讲究快速响应，一个是讲究大吞吐OLTP通常使用ER模型，采用三范式消除数据中的冗余，而OLAP采用的模型则比较丰富 3、有哪些建模方法论？（1）ER建模（2）维度建模（3）Data vault建模（4）Anchor建模]]></content>
      <categories>
        <category>0x1 数据仓库</category>
        <category>0x11 数据建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive SQL优化]]></title>
    <url>%2F2018%2F12%2F25%2Fsql-optimization%2F</url>
    <content type="text"><![CDATA[0x00 group by 引起的倾斜1|优化措施12set hive.map.aggr = trueset hive.groupby.skewindata = true 2|原理​ 作业将生成两个MapReduce Job ​ 第一个MapReduce Job中 Map的输出结果会随机的分布到Reduce中，每个Reduce做部分聚合操作并输出结果，这样处理的结果 相同的group by key会分布到不同的reduce中 ​ 第二个MapReduce Job中 再根据 相同group by key分布到reduce中 0x01 count distinct 优化1|例子1select count(distinct ssn) from table 2|说明​ 由于如上SQL，Hive会将Map阶段的输出全部分布到一个Reduce中，很容易引起性能问题 3|正确的操作方法12345select count(1) from ( select ssn from table group by ssn) t 12-- 原理-- 利用group by 先进行数据去重，然后再进行计数，计数是不可避免的，目的是为了大量减少重复计数 0x02 大表join小表1|方法​ 一般都是采用 map join的方法，默认Hive是开启 mapjoin，其Hive设置参数如下： 1set hive.auto.convert.join = true; ​ 或者是使用Hint的方式进行小表指定 2|案例12345678select /*+mapjoin(t1)*/ t0.ssn ,t1.namefrom big_table0 t0inner join small_table1 t1on t0.ssn = t1.ssn;-- /*+mapjoin(t1)*/ 即 map join hint，如果需要mapjoin多个表，则格式为 /*+mapjoin(t1,t2,t3)*/ 0x03 大表join大表(待补充)​ 1|问题场景 ​ 2|解决方法1 ​ 3|解决方法2 0x04 优化案例1|业务背景：​ 筛选出 当日 注册产品、注册IP为维度下超过30个帐号的 帐号清单 2|常规做法​ 先筛选出满足 注册产品、注册IP 数量超过30个的 注册产品及注册IP的维度清单，然后再关联源表进行筛选出帐号 1234567891011121314151617181920212223242526272829303132333435363738394041create table tmp.tmp_algorithm1 asSELECT t1.ssn, 1 AS rubbish_rateFROM ( SELECT dt, ssn, reg_ip, reg_product FROM dw.dw_user_static_reg_info_dd WHERE dt = '2018-11-22' AND ( split(ssn, '@') [1] IN ('126.com', 'yeah.net', 'vip.126.com', 'vip.163.com') OR split(ssn, '@') [1] IS NULL ) ) t1 JOIN ( SELECT dt, reg_ip, reg_product, count(*) AS cnt FROM dw.dw_user_static_reg_info_dd WHERE dt = '2018-11-22' AND ( split(ssn, '@') [1] IN ('126.com', 'yeah.net', 'vip.126.com', 'vip.163.com') OR split(ssn, '@') [1] IS NULL ) GROUP BY dt,reg_ip, reg_product HAVING cnt &gt;= 30 ) t2 ON t1.dt=t2.dt and t1.reg_ip = t2.reg_ip AND t1.reg_product = t2.reg_productGROUP BY t1.ssn; 3|优化做法​ 使用窗口函数进行维度内计数，如下通过窗口函数计算在 维度 注册产品、注册IP下的数量，通过子查询进行筛选出目标帐号清单 12345678910111213141516create table tmp.tmp_algorithm2 asselect ssn,1 as rubbish_ratefrom(select ssn ,reg_ip ,reg_product ,count(1) over(partition by reg_ip,reg_product) as cnt from dw.dw_user_static_reg_info_ddwhere dt = '2018-11-22' and ( split(ssn, '@') [1] in ('126.com', 'yeah.net', 'vip.126.com', 'vip.163.com') or split(ssn, '@') [1] is null )) twhere cnt &gt;= 30; 4|优化思路​ 由于HQL 是转换为MapReduce的形态进行数据计算，常规做法中会启用大量的MapReduce，并重复的抽取了源表数据，浪费了计算，优化做法是实现一份数据提取，复杂内存计算，从而减少使用计算资源]]></content>
      <categories>
        <category>0x2 技术平台</category>
        <category>0x21 Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>SQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库专题(1)-数仓概论]]></title>
    <url>%2F2018%2F12%2F20%2Fedw-brief-summary%2F</url>
    <content type="text"><![CDATA[通常来讲，大家都知道数据仓库的官方概念数据仓库概念创始人W.H.Inmon在《建立数据仓库》一书中对数据仓库的定义是：数据仓库就是面向主题的、集成的、相对稳定的、随时间不断变化（不同时间）的数据集合，用以支持经营管理中的决策制定过程、数据仓库中的数据面向主题，与传统数据库面向应用相对应。 0x1 数据仓库有哪些点？能看到那些面? 很多数据开发人员，无法将数据仓库的点连成线，更别说由线织成面？这里我们进行一些头脑风暴，看看数据仓库都有哪些内容？ 总的来说，数据仓库可以整理如下4块内容：（1）数据建模方法论（2）实施方法论（最佳实践）（3）数据管理（4）数据应用 每一块内容拿出来讲，都是无边界的，要想成为这一领域的专家，需要有数据仓库的全局观并且有某一领域的最佳实践。数仓之路，任重而道远！！！]]></content>
      <categories>
        <category>0x1 数据仓库</category>
        <category>0x11 数据建模</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用fuse_dfs快速建立hdfs云存储]]></title>
    <url>%2F2018%2F08%2F16%2Ffuse-hdfs%2F</url>
    <content type="text"><![CDATA[一、介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将HDFS挂载到本地，允许用户像访问本地文件系统一样访问远程文件系统，用户可像读写本地文件一样读写HDFS上的文件，大大简化了HDFS使用，方法较为常用两种方法是：利用fuse_dfs或者利用NFSv3，因NFSv3需对Hadoop集群端进行操作，修改代价太大，本文介绍以fuse_dfs，只需在客户端进行配置即可挂载hdfs。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户空间文件系统（Filesystem in Userspace，简称FUSE）是一个面向类Unix计算机操作系统的软件接口，它使无特权的用户能够无需编辑内核代码而创建自己的文件系统。目前Linux通过内核模块对此进行支持。一些文件系统如ZFS、glusterfs和lustre使用FUSE实现。Linux用于支持用户空间文件系统的内核模块名叫FUSE，FUSE一词有时特指Linux下的用户空间文件系统。通过使用Hadoop的Fuse-DFS分类模块，任意一个Hadoop文件系统(不过一般为HDFS)都可以作为一个标准文件系统进行挂载。Hadooop源码中自带了fuse-dfs模块，利用fuse_dfs将hdfs当一个网盘挂载到本地。 二、应用场景&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同步：实现用户对个人计算指定目录下的特定类型的文件自动与HDFS同步，实时监测用户文件变化情况。不需要用户干预就能及时自动备份文件。当用户新建、修改或删除文件（目录）时自动更新同步，保障用户重要文件的安全。当用户移动办公时，可直接从HDFS访问，省略了移动存储介质的拷贝过程，提高用户工作效率。更换新计算机时，用户登录后原计算机中的个人文件将自动从HDFS存储中同步到新计算机中。例如：适用数据仓库需要ETL进行Hadoop文件进行ftp或者rsync推送的场景。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网盘：用户可将不常用的文件存储在HDFS存储中，从而节约本地存储空间，提高文件的物理安全性，也便于资源共享。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;共享：用户可以把网盘或同步盘的任意文件（目录）分享给群组或其他用户。只需要在云存储中存储一份文件，被分享的人不需要下载存储即可在线浏览，实现资源共享。若源文件被修改，所有分享人立即会得到更新文件。 三、部署实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前网络上关于利用fuse挂载hdfs较多基于较早版本的hadoop(0.x，1.x) ，并大多都基于无kerberos认证，本文以北京亦庄Hadoop 2.5.2为例，介绍通过fuse挂载hdfs到本地的方法。设定源码目录为$HADOOP_SRC_HOME，那么Hadoop 2.x 的 fuse_dfs 源码位于1$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么怎么编译fuse_dfs？fuse_dfs是Hadoop自带模块，编译Hadoop时，增加编译参数require.fuse=true，即编译命令如下1mvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar 编译环境123456789Linux: 3.16.0-0.bpo.4-amd64 (Debian7)JDK: 1.7.0_80 64bitApache Hadoop: 2.5.2 Ant: 1.8.2GCC: 4.7.2 (系统默认)Protocbuf 2.5.0Maven 3.0.4Cmake 2.8.9Make 3.8.1 安装fuse等依赖1sudo apt-get -y install maven build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev libfuse-dev 编译fuse_dfs(Hadoop)123456789101112# 下载mkdir package &amp;&amp; cd packagewget http://archive.apache.org/dist/hadoop/core/hadoop-2.5.2/hadoop-2.5.2-src.tar.gz#解压tar -zxvf hadoop-2.5.2-src.tar.gzln -s hadoop-2.5.2-src hadoop-srcecho "export HADOOP_SRC_HOME=$HOME/package/hadoop-src/" &gt;&gt; .bashrc &amp;&amp; source .bashrc# 编译cd hadoop-2.5.2-srcmvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar PS: 编译Hadoop之前需提前安装好fuse，否则fuse_dfs无法编译成功123456# 编译后的fuse_dfs位于：$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs/fuse_dfs# 完整的Hadoop包则位于：$HADOOP_SRC_HOME/hadoop-dist/target/hadoop-2.5.2.tar.gz# 一个用于挂载hdfs的脚本：$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh 部署Hadoop（略）配置、fuse_dfs_wrapper.shHDFS挂载主要依赖源码的两个模块，一个是hadoop-hdfs-project，另一个是hadoop-client，可将依赖的jar包单独提炼出来。 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env bashexport HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/local/share/hadoop&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport HADOOP_PREFIX=$HOME/package/hadoop-srcexport JAVA_HOME=$HOME/opt/jdkexport FUSEDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/main/native/fuse-dfs"export LIBHDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs/target/native/target/usr/local/lib"if [ "$OS_ARCH" = "" ]; then export OS_ARCH=amd64fiif [ "$JAVA_HOME" = "" ]; then export JAVA_HOME=/usr/local/javafiif [ "$LD_LIBRARY_PATH" = "" ]; thenexport LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/usr/local/libfiJARS=`find "$HADOOP_PREFIX/hadoop-hdfs-project" -name "*.jar" | xargs`for jar in $JARS; do CLASSPATH=$jar:$CLASSPATHdoneJARS=`find "$HADOOP_PREFIX/hadoop-client" -name "*.jar" | xargs`for jar in $JARS; do CLASSPATH=$jar:$CLASSPATHdoneexport CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATHexport PATH=$FUSEDFS_PATH:$PATHexport LD_LIBRARY_PATH=$LIBHDFS_PATH:$JAVA_HOME/jre/lib/$OS_ARCH/server:$LD_LIBRARY_PATHfuse_dfs $@ 利用命令 1234567891011121314 sudo bash ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://$&#123;nn&#125;:$&#123;port&#125; /mnt/hdfsINFO /home/hzxijingjing/package/hadoop-2.5.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_options.c:164 Adding FUSE arg /mnt/hdfs# 挂载信息 df -hFilesystem Size Used Avail Use% Mounted onrootfs 50G 8.2G 39G 18% /udev 10M 0 10M 0% /devtmpfs 1.6G 216K 1.6G 1% /run/dev/vda1 50G 8.2G 39G 18% /tmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 3.2G 0 3.2G 0% /run/shm/dev/vdb1 99G 75M 94G 1% /datafuse_dfs 49G 0 49G 0% /mnt/hdfs # HDFS挂载成功 ​ 至此为止，已实现无简单认证的Hadoop挂载，然而对于配置了kerberos认证的Hadoop，实施挂载会出现如下情况：123cd /mnt &amp;&amp; ls -lhls: cannot access hdfs: Transport endpoint is not connectedd????????? ? ? ? ? ? hdfs 后置参数 -d，进入debug模式， 查看运行信息 1sudo /bin/sh ./fuse-dfs/fuse_dfs_wrapper.sh hdfs://$&#123;nn&#125;:$&#123;port&#125; /mnt/dfs -d 1234567891011121314151617181920212223242526272829............ # 省略一些无用日志 FUSE library version: 2.9.0nullpath_ok: 0nopath: 0utime_omit_ok: 0unique: 1, opcode: INIT (26), nodeid: 0, insize: 56, pid: 0INIT: 7.23flags=0x0003fffbmax_readahead=0x00020000INFO /home/hzxijingjing/package/hadoop-2.5.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c:98 Mounting with options: [ protected=(NULL), nn_uri=hdfs://hadoop1232.hz.163.org:8020, nn_port=0, debug=0, read_only=0, initchecks=0, no_permissions=0, usetrash=1, entry_timeout=60, attribute_timeout=60, rdbuffer_size=10485760, direct_io=0 ]............ # 省略一些无用日志 fuseConnectInit: initialized with timer period 5, expiry period 300 INIT: 7.18 flags=0x00000039 max_readahead=0x00020000 max_write=0x00020000 max_background=0 congestion_threshold=0 unique: 1, success, outsize: 40unique: 2, opcode: STATFS (17), nodeid: 1, insize: 40, pid: 13769statfs / unique: 2, success, outsize: 96unique: 3, opcode: GETATTR (3), nodeid: 1, insize: 56, pid: 13815getattr /fuseNewConnect: failed to find Kerberos ticket cache file '/tmp/krb5cc_0'. Did you remember to kinit for UID 0?fuseConnect(usrname=root): fuseNewConnect failed with error code -13fuseConnectAsThreadUid: failed to open a libhdfs connection! error -13. unique: 3, error: -5 (Input/output error), outsize: 16unique: 4, opcode: STATFS (17), nodeid: 1, insize: 40, pid: 13826 从上可以看出，大概是无法通过kerberos的cache文件/tmp/krb5cc_0认证，早就意识到基于kerberos的hadoop是块难啃的骨头，用过kerberos认证的同学都知道，kerberos认证依赖本地Linux命令 kinit 实现认证，认证之后通常会有一个有效期，即在一段时间内，无需再次认证。 然而利用fuse_dfs挂载kerberos认证的HDFS，无论是度娘还是谷爸在这方面资料也是极少，无奈只能看下fuse_dfs的源码。 fuse-dfs实现​ fuse_dfs是由C语言开发，代码目录位于$HADOOP_SRC_HOME/hadoop-hdfs-project/hadoop-hdfs/src/main/native/（上文有提到）​ 如下是fuse_dfs连接hdfs及操作相关源码文件 12./fuse-dfs/fuse_connect.c./libhdfs/hdfs.c vim fuse_connect.c利用libhdfs建立连接123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111 /**函数主体中一段代码表示如何通过libhdfs连接*//** * Create a new libhdfs connection. * * @param usrname Username to use for the new connection * @param ctx FUSE context to use for the new connection * @param out (out param) the new libhdfs connection * * @return 0 on success; error code otherwise */static int fuseNewConnect(const char *usrname, struct fuse_context *ctx, struct hdfsConn **out) /** * Find out what type of authentication the system administrator * has configured. * * @return the type of authentication, or AUTH_CONF_UNKNOWN on error. */ //获取Hadoop认证方式 static enum authConf discoverAuthConf(void)&#123; int ret; char *val = NULL; enum authConf authConf; ret = hdfsConfGetStr(HADOOP_SECURITY_AUTHENTICATION, &amp;val); if (ret) authConf = AUTH_CONF_UNKNOWN; else if (!val) authConf = AUTH_CONF_OTHER; else if (!strcmp(val, "kerberos")) authConf = AUTH_CONF_KERBEROS; else authConf = AUTH_CONF_OTHER; free(val); return authConf;&#125;// 获取到Hadoop的认证方式之后，首先找kerberos认证后的cache文件if (gHdfsAuthConf == AUTH_CONF_KERBEROS) &#123; findKerbTicketCachePath(ctx, kpath, sizeof(kpath)); if (stat(kpath, &amp;st) &lt; 0) &#123; fprintf(stderr, "fuseNewConnect: failed to find Kerberos ticket cache " "file '%s'. Did you remember to kinit for UID %d?\n", kpath, ctx-&gt;uid); ret = -EACCES; goto error; &#125;// 查找kerberos认证后的cache文件，从发现cache文件位于：/tmp/krb5cc_%d/** * Find the Kerberos ticket cache path. * * This function finds the Kerberos ticket cache path from the thread ID and * user ID of the process making the request. * * Normally, the ticket cache path is in a well-known location in /tmp. * However, it's possible that the calling process could set the KRB5CCNAME * environment variable, indicating that its Kerberos ticket cache is at a * non-default location. We try to handle this possibility by reading the * process' environment here. This will be allowed if we have root * capabilities, or if our UID is the same as the remote process' UID. * * Note that we don't check to see if the cache file actually exists or not. * We're just trying to find out where it would be if it did exist. * * @param path (out param) the path to the ticket cache file * @param pathLen length of the path buffer */static void findKerbTicketCachePath(struct fuse_context *ctx, char *path, size_t pathLen)&#123; FILE *fp = NULL; static const char * const KRB5CCNAME = "\0KRB5CCNAME="; int c = '\0', pathIdx = 0, keyIdx = 0; size_t KRB5CCNAME_LEN = strlen(KRB5CCNAME + 1) + 1; // /proc/&lt;tid&gt;/environ contains the remote process' environment. It is // exposed to us as a series of KEY=VALUE pairs, separated by NULL bytes. snprintf(path, pathLen, "/proc/%d/environ", ctx-&gt;pid); fp = fopen(path, "r"); if (!fp) goto done; while (1) &#123; if (c == EOF) goto done; if (keyIdx == KRB5CCNAME_LEN) &#123; if (pathIdx &gt;= pathLen - 1) goto done; if (c == '\0') goto done; path[pathIdx++] = c; &#125; else if (KRB5CCNAME[keyIdx++] != c) &#123; keyIdx = 0; &#125; c = fgetc(fp); &#125;done: if (fp) fclose(fp); if (pathIdx == 0) &#123; snprintf(path, pathLen, "/tmp/krb5cc_%d", ctx-&gt;uid); // cache文件的目录 &#125; else &#123; path[pathIdx] = '\0'; &#125;&#125; ​ 由上代码可以看出，函数findKerbTicketCachePath从上下文ctx中获取uid从而获取/tmp/krb5cc_${uid}，那么如何挂载hdfs时，识别到kerberos的cache文件呢？ vim hdfs.c123456789101112131415161718192021struct hdfsBuilder &#123; int forceNewInstance; const char *nn; tPort port; const char *kerbTicketCachePath; const char *userName;&#125;; #define KERBEROS_TICKET_CACHE_PATH "hadoop.security.kerberos.ticket.cache.path"if (bld-&gt;kerbTicketCachePath) &#123; jthr = hadoopConfSetStr(env, jConfiguration, KERBEROS_TICKET_CACHE_PATH, bld-&gt;kerbTicketCachePath); if (jthr) &#123; ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL, "hdfsBuilderConnect(%s)", hdfsBuilderToStr(bld, buf, sizeof(buf))); goto done; &#125;&#125; 最终定位到：KERBEROS_TICKET_CACHE_PATH “hadoop.security.kerberos.ticket.cache.path” 在core-site.xml添加如下属性： 12345&lt;property&gt; &lt;name&gt;hadoop.security.kerberos.ticket.cache.path&lt;/name&gt; &lt;value&gt;/tmp/krb5cc_$&#123;uid&#125;&lt;/value&gt; &lt;description&gt;Path to the Kerberos ticket cache. &lt;/description&gt;&lt;/property&gt; ${uid}是什么？uid=7765 1234567kinit urs.keytab ursTicket cache: FILE:/tmp/krb5cc_7765Default principal: urs@HADOOP.HZ.NETEASE.COMValid starting Expires Service principal10/05/2018 10:52 10/05/2018 20:52 krbtgt/HADOOP.HZ.NETEASE.COM@HADOOP.HZ.NETEASE.COM renew until 11/05/2018 10:52 最后重新挂载，操作手法同上。 大功告成!]]></content>
      <categories>
        <category>0x2 技术平台</category>
        <category>0x20 Hadoop</category>
      </categories>
      <tags>
        <tag>fuse</tag>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用LaTeX写数学公式]]></title>
    <url>%2F2018%2F08%2F16%2Flatex-math-formula%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;最近一直学习 线性代数，通过latex来写数学公式，替代了使用截图的方式来写博客，当然直接书写一个数学公式也是有一定难度，由此我们需要不断的积累，下文将简单介绍使用LaTeX书写矩阵，更多的数学公式可以参考LaTeX官方文档。 LaTeX 写简单Matrix语法 使用\$\$begin{matrix}…\end{matrix}$$来生成矩阵，其中… 表示的是LaTeX 的矩阵命令，矩阵命令中每一行以 \ 结束，矩阵的元素之间用&amp;来分隔开。 例如: 写一个简单矩阵 1234567$$ \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125; \tag&#123;1&#125;$$ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{matrix} \tag{1} 带括号的Matrix 感觉(1)中的矩阵不是很美观，可以给矩阵加上括号，加括号的方式有很多，大致可分为两种：使用\left … \right 或者把公式命令中的matrix 改成 pmatrix、bmatrix、Bmatrix、vmatrix、Vmatrix等。 1234567$$ \begin&#123;pmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;pmatrix&#125; \tag&#123;2&#125;$$ \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix} \tag{2}1234567$$ \begin&#123;bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;bmatrix&#125; \tag&#123;3&#125;$$ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \tag{3}1234567$$ \begin&#123;Bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;Bmatrix&#125; \tag&#123;4&#125;$$ \begin{Bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{Bmatrix} \tag{4}1234567$$ \begin&#123;vmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;vmatrix&#125; \tag&#123;5&#125;$$ \begin{vmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{vmatrix} \tag{5}1234567$$ \begin&#123;Vmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;Vmatrix&#125; \tag&#123;6&#125;$$ \begin{Vmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{Vmatrix} \tag{6}增广矩阵 12345678$$ \left[ \begin&#123;array&#125;&#123;cc|c&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end&#123;array&#125;\right] \tag&#123;7&#125;$$ \left[ \begin{array}{cc|c} 1 & 2 & 3 \\ 4 & 5 & 6 \end{array} \right] \tag{7}组合矩阵 12345678$$ \left[ \begin&#123;pmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end&#123;pmatrix&#125;\right] \tag&#123;7&#125;$$ \left[ \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix} \right] \tag{8}带省略符号的Matrix\ddots 表示： ⋱ 123456789$$ \left[ \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; \ddots &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125;\right] \tag&#123;9&#125;$$ \left[ \begin{matrix} 1 & 2 & 3 \\ 4 & \ddots & 6 \\ 7 & 8 & 9 \end{matrix} \right] \tag{9}\vdots 表示： ⋮ 12345678$$ \left[ \begin&#123;matrix&#125; 1 &amp; \vdots &amp; 3 \\ 4 &amp; \vdots &amp; 6 \end&#123;matrix&#125;\right] \tag&#123;10&#125;$$ \left[ \begin{matrix} 1 & \vdots & 3 \\ 4 & \vdots & 6 \end{matrix} \right] \tag{10}\cdots 表示： ⋯ 12345678$$ \left[ \begin&#123;matrix&#125; 1 &amp; \cdots &amp; 3 \\ 4 &amp; \cdots &amp; 6 \end&#123;matrix&#125;\right] \tag&#123;11&#125;$$ \left[ \begin{matrix} 1 & \cdots & 3 \\ 4 & \cdots & 6 \end{matrix} \right] \tag{11}更多LaTeX语法请点击这里 e=mc^2 x^y = (1+e^x)^-2xy^w lim_{1\to+\infty}P(|\frac{1}{n}\sum_i^nX_i-\mu|]]></content>
      <categories>
        <category>0xF 办公技能</category>
        <category>0xF0 MarkDown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库ETL之数据交换]]></title>
    <url>%2F2017%2F06%2F05%2Fedw-data-exchange%2F</url>
    <content type="text"><![CDATA[​ 从1990年数据仓库之父比尔·恩门（Bill Inmon）提出数据仓库的概念，ETL作为数据仓库的核心组件，在传统的数据仓库中是服务于数据采集，数据处理，大数据时代来临，对ETL的理解也由【抽取、转换、加载】升级到【交换】这个层面。如果你也考虑建设企业级数据仓库可以作为参考。 0x0 ETL之定位业内多数知名数据仓库解决方案提供方利用公司的自我研发的ETL工具及数据平台从生产系统接入数据源，在库内进行数据转换工作，与其他三方厂家（乙方）一样，自顾一亩三分地，完成了端对端的数据支撑，然而对于甲方而言，各个厂家的端的数据采集无疑是对生产系统的增加多倍压力，另外厂家之间的数据交换也是常态。 建设数据仓库的模式有由上而下以及由下而上两种模式，对于大企业而言，各个部门的运营模式、业务方向均有千差万别，部门大多建设自己的数据仓库，公司从各个部门建设的数据仓库之上建立总的数据仓库。在我们公司，部门之间数据ETL，一方面因业务模式，另一方面各部门数据平台产品多样化，ETL大多采用ftp、rsync这种文件数据传输方式，日志采集则使用DataStream或flume等。我们通常面临企业之间的数据交互，政府部门的数据推送，即便是一个部门内部数据的交换也是常态，数据共享有利于大数据生态发展，离线平台需要生产系统的数据进行计算，生产系统也需要离线平台的数据支撑。数据交换作为ETL一个升级模式成为企业级数据仓库建设的必然。 0x1 交换平台的产生大多大数据从业人士也认识到数据交换，很多厂家也认识到了ETL模式的与时俱进，阿里的数据工场，亚信的云化交换平台，华为的云化交换平台以及我们的猛犸大数据平台应运而生，大数据交换平台服务于企业（部门、系统）数据交换，数据交换对于用户而言都是透明的，不仅满足传统的ETL功能，同时需要满足动态扩展。大数据形势下，互联网数据仓库较之传统的数据仓库平台有如下特点： 为适应数据仓库的发展，那么数据交换平台应该具备哪些能力呢？小编认为可从如下几个方面进行阐述，数据采集能力，数据存储能力，数据管理能力，数据计算能力，平台开放能力，作业调度能力。 (1) 数据采集能力交换平台应支持对表（关系表，k-v表）、文件（结构化、非结构化）、消息、数据流等多种数据形式的支持，同时需支持多样化的采集方式：实时增量流式（flume、DataStream等），消息队列（nsq、Kafaka）等主流技术。由于技术繁杂，种类繁多，选取适合自己业务模式的工具形成组件化利于跨平台部署，这是数据采集应考虑的。（PS：实际上对于日志流式收集的形式通常是效率高，高响应，但也处于不稳定的，易丢失数据等现状。） (2) 数据存储能力互联网的数据【大】是毋庸置疑的事实，传统RDBMS的核心设计思想基本上是30年前形成的，在数据仓库领域有相当权威的公司有Teradata、IBM、Oracle、SAP等这里公司均拥有自身数据存储系统以及软硬一体化的大数据解决方案，支持动态扩展的能力，Teradata、Aster Data、GreenPlum都是MPP存储架构，支持PB级别存储（前两者属于商业数据库，GreenPlum开源），这些均是关系型数据库，我们最熟悉的Hadoop是属于SMP架构，支持非结构化、半结构化、结构化数据存储，数据存储平台的选型根据企业的业务模式选定。 (3) 平台管理能力数据管理：数据的交换忌讳数据冗余，浙江移动大数据中心使用亚信云化交换平台出现了这样的一个场景，厂家A需要生产系统中数据表T1部分字段,，厂家B需要生产系统中数据表T1的所有字段，另有特殊情景长家C需要表中的字段与厂家A有交叉字段现象，生产系统为给三个厂家同时供数，造成了大量数据冗余，空间有效利用率60%不到。 多租户管理：数据交换本身的一个概念就是交换，意味着多租户，平台是开放的，面向多个部门甚至多个企业，租户可以灵活创建自己的作业体系，提出相应的数据需求，进行自主管理和配置，跨系统或者跨部门的数据请求开放给用户，开启有效的需求管理体系。 作业管理：ETL平台所需维护的细节繁多，如果交互的接口管理一塌糊涂，比如繁多的FTP、散乱的Shell脚本、crontab定时任务搞晕了运维人员，一旦人员迭代，付出的管理成本很大，建立一套完善的元数据管理体系是保障系统正常运作必需的工具。 (4) 数据计算能力 互联网的数据“大”是不争的事实，现在分析一下数据处理技术面临的挑战，近几年见过形形色色的数据仓库，大多使用的自下而上的方式建设，生态数仓中所使用的平台产品多样性，然而传统行业电信也好，银行也好，互联网产品数仓也罢，鲜有PB级别的数仓，面对的用户量几千万，对应的数据是几百TB，数据量少怎么样的建设方案都是无所谓的，当面对10亿级别的用户，PB级别的数据时，如何满足数据计算？如何适应多种数据平台的接入？ 数据交换平台需具备强大的数据计算能力，支持常见的分布式计算模型，以实现数据的预处理（格式转换【例如lzo转换成HFile】，空值处理、数据压缩等）以及常见的计算函数（窗口函数，中位数计算、时序计算、图计算等）。 (5) 平台开放能力能够对外输出数据，这是企业级的交换平台所应具备的基本能力，数据开放并不意味着谁都可以随意获取数据，放弃数据安全，数据开放表示数据共享，满足企业的各项业务需求，根据部门的具体需求，申请已开放的数据，数据安全保障。 数据交换平台所应支持的不仅是传统的数据开放，更应具有元数据开放、计算能力的开放，丰富的API提供，既然无法满足所有的个性化需求，那就应有开放的API为其他系统无缝衔接，单纯的数据导出已无法满足业务的发展。 (6) 作业调度能力调度功能是从ETL工具形成初便拥有的，ETL调度工具发展也从最初的crontab衍生到现在的调度平台，业内企业级的调度工具例如国外的Informatica公司的Informatica、Teradata的 ETL Automation、微软的 SSIS、Control-M，国内的有自主研发阿里的Zeus，亚信的 AI-CLOUD-ETL，华为研发的云-ETL，网易的nschedule、猛犸等，开源的调度工具有Azkaban，Kettle等。 调度模式是各色各样，有以工作流调度单位，有以作业为调度单位，所支持的作业形式也是多种，脚本形式(shell、python、Perl，ruby)，SQL、Java、MapReduce以及其他组件形式。调度策略也是丰富多样，有时间触发、时间触发、接口通知、手工执行。 实现任务的统一调度，支持动态扩展的作业类型，丰富的调度模式是数据交换平台应具备的。以上所列举的6项能力提出一些常见的体现，构建数据交换平台所考虑的细节非常多。 0x3 交换平台建设最佳实践很多数据平台提供的是纯粹的数据服务，较少提供Api或元数据服务以满足其他平台的嵌入，业内打造成PaaS级的ETL交换平台屈指可数，技术因素是一方面，业务需求是否需要打造这样一个平台也是其中一个原因，对业务需求的理解，是否具有持续的维护及更新能力又是另一方面。还有一句话是这么说的，规则与灵活怎么取舍？为便于维护可能将主流的需求满足，对于一些个性化的需求则被抛弃，很多公司技术也很强，但打造这样一个ETL交换平台更多的考虑是从用户需求中来，到实践去来，纯粹的为实现功能，大谈性能也未必是一个好的解决方案，业务始终为王。下面设想一个交换平台架构图供参考： 稍作拓展：]]></content>
      <categories>
        <category>0x1 数据仓库</category>
        <category>0x10 数仓概论</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用HFile BulkLoad实现HBase海量数据加载]]></title>
    <url>%2F2016%2F07%2F01%2Fhbase-bulk-load%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据仓库开发过程中，我们可能将HBase作为我们数据即服务的持久化存储介质，由此大量的数据模型从数据仓库计算后写入至HBase。通常我们使用HBase提供的API方法，实现了接口调用，但对于海量的数据，接口的调用引起HBase cpu、内存占用过高，影响正常业务使用。于是我们着手研究HFile BulkLoad的方式进行离线数据加载。 0x0 HBase的存储文件格式：HFile&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HBase实际的存储文件功能是由HFile类实现的，它被专门创建以达到一个目的：有效地存储HBase的数据。它们基于Hadoop的TFile类，并模仿-Google的BigTable架构使用的SSTable格式。曾在HBase中使用过的Hadoop的MapFile类被证明性能不够好。 0x1 BulkLoad原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 Bulk Load 方式由于利用了 HBase 的数据信息是按照特定格式存储在 HDFS 里的这一特性，直接在 HDFS 中生成持久化的 HFile 数据格式文件，然后完成巨量数据快速入库的操作，配合 MapReduce 完成这样的操作，不占用 Region 资源，不会产生巨量的写入 I/O，所以需要较少的 CPU 和网络资源。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bulk Load 的实现原理是通过一个 MapReduce Job 来实现的，通过 Job 直接生成一个 HBase 的内部 HFile 格式文件，用来形成一个特殊的 HBase 数据表，然后直接将数据文件加载到运行的集群中。使用 Bulk Load 功能最简单的方式就是使用ImportTsv 工具，ImportTsv 是 HBase 的一个内置工具，目的是从 TSV 文件直接加载内容至 HBase。它通过运行一个 MapReduce Job, 将数据从 TSV 文件中直接写入 HBase 的表或者写入一个 HBase 的自有格式数据文件。 0x2 结合MapReduce生成HFile0x20 作业的配置1234567891011121314151617181920/** * Job Configure */ Job job = Job.getInstance(conf,"HFileProducerETL"); job.setJarByClass(HFileProducerETL2.class); TableMapReduceUtil.addDependencyJars(job); job.setMapperClass(BulkLoadMapper.class); job.setReducerClass(KeyValueSortReducer.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setOutputValueClass(KeyValue.class); if (fs.exists(outputPath)) &#123; fs.delete(outputPath, true); &#125; FileInputFormat.setInputPaths(job,inputPath); FileOutputFormat.setOutputPath(job,outputPath); HTable htable =new HTable(conf, tablename); HFileOutputFormat2.configureIncrementalLoad(job,htable,htable); return job.waitForCompletion(false) ? 0 : 1; 0x21 Map​ key 类：ImmutableBytesWritable​ value 类：KeyValue 1234567891011121314151617181920212223242526272829303132333435363738394041@Override public void map(LongWritable key, Text value, Context context) &#123; try &#123; String line = value.toString(); String[] all_column_values = line.split(context.getConfiguration().get(FILE_DELIMITER_STR),-1); String rowKeyString = all_column_values[rowKeyIndex]; byte[] rowKey=Bytes.toBytes(rowKeyString); // rowKey 赋值 ImmutableBytesWritable rowKeyWritable=new ImmutableBytesWritable(rowKey); if (all_column_name.length!=all_column_values.length)&#123; return; &#125; for (int i = 0; i &lt; all_column_name.length; i++) &#123; if (export_column_index[i] == EXPORT_FLAG &amp;&amp; !all_column_values[i].trim().equals(HBaseConstant.HiveNULL) &amp;&amp; !all_column_values[i].trim().equals(HBaseConstant.HBaseNull) &amp;&amp; !all_column_values[i].trim().equals(HBaseConstant.HEmpty) )&#123; KeyValue kv ; // 如果是 HFILE_CLEAR_FLAG 则 清空 if (all_column_values[i].equals(HBaseConstant.HFILE_CLEAR_FLAG))&#123; all_column_values[i]=HBaseConstant.HEmpty; &#125; if (timestampIndex == TIMESTAMP_COLUMN_DEFAULT_INDEX)&#123; kv = new KeyValue(rowKey, CF, Bytes.toBytes(all_column_name[i]),Bytes.toBytes(all_column_values[i])); &#125; else&#123; Long timestamp_value = Long.parseLong(all_column_values[timestampIndex]); kv = new KeyValue(rowKey, CF, Bytes.toBytes(all_column_name[i]),timestamp_value,Bytes.toBytes(all_column_values[i])); &#125; context.write(rowKeyWritable,kv); &#125; &#125; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 0x22 BulkLoad​ 生成HFile之后，load相对就比较简单了，代码如下： 123456Configuration conf = DefConfiguration.GetConfiguration(hbaseEnv);conf.setInt("hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily",5000);HTable htable =new HTable(conf, tableName);LoadIncrementalHFiles loader = new LoadIncrementalHFiles(conf);loader.doBulkLoad(hFilePath, htable);]]></content>
      <categories>
        <category>0x2 技术平台</category>
        <category>0x22 HBase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
</search>
